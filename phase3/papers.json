[{
  "id": "2981549002",
  "title": "Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes. | Paper | Microsoft Academic",
  "abstract": "Wide neural networks with random weights and biases are Gaussian processes, as originally observed by Neal (1995) and more recently by Lee et al. (2018) and Matthews et al. (2018) for deep fully-connected networks, as well as by Novak et al. (2019) and Garriga-Alonso et al. (2019) for deep convolutional networks. We show that this Neural Network-Gaussian Process correspondence surprisingly extends to all modern feedforward or recurrent neural networks composed of multilayer perceptron, RNNs (e.g. LSTMs, GRUs), (nD or graph) convolution, pooling, skip connection, attention, batch normalization, and/or layer normalization. More generally, we introduce a language for expressing neural network computations, and our result encompasses all such expressible neural networks. This work serves as a tutorial on the *tensor programs* technique formulated in Yang (2019) and elucidates the Gaussian Process results obtained there. We provide open-source implementations of the Gaussian Process kernels of simple RNN, GRU, transformer, and batchnorm+ReLU network at this http URL.",
  "date": " 2019 ",
  "authors": [
    "Greg Yang"
  ],
  "references": [
    "2194775991",
    "2963403868",
    "1836465849",
    "2964308564",
    "1677182931",
    "2963446712",
    "2157331557",
    "2310919327",
    "2064675550",
    "1533861849"
  ]
}, {
  "id": "3105081694",
  "title": "COVID-Net: a tailored deep convolutional neural network design for detection of COVID-19 cases from chest X-ray images. | Paper | Microsoft Academic",
  "abstract": "The Coronavirus Disease 2019 (COVID-19) pandemic continues to have a devastating effect on the health and well-being of the global population. A critical step in the fight against COVID-19 is effective screening of infected patients, with one of the key screening approaches being radiology examination using chest radiography. It was found in early studies that patients present abnormalities in chest radiography images that are characteristic of those infected with COVID-19. Motivated by this and inspired by the open source efforts of the research community, in this study we introduce COVID-Net, a deep convolutional neural network design tailored for the detection of COVID-19 cases from chest X-ray (CXR) images that is open source and available to the general public. To the best of the authors' knowledge, COVID-Net is one of the first open source network designs for COVID-19 detection from CXR images at the time of initial release. We also introduce COVIDx, an open access benchmark dataset that we generated comprising of 13,975 CXR images across 13,870 patient patient cases, with the largest number of publicly available COVID-19 positive cases to the best of the authors' knowledge. Furthermore, we investigate how COVID-Net makes predictions using an explainability method in an attempt to not only gain deeper insights into critical factors associated with COVID cases, which can aid clinicians in improved screening, but also audit COVID-Net in a responsible and transparent manner to validate that it is making decisions based on relevant information from the CXR images. By no means a production-ready solution, the hope is that the open access COVID-Net, along with the description on constructing the open source COVIDx dataset, will be leveraged and build upon by both researchers and citizen data scientists alike to accelerate the development of highly accurate yet practical deep learning solutions for detecting COVID-19 cases and accelerate treatment of those who need it the most.",
  "date": " 2020 ",
  "authors": [
    "Linda Wang",
    "Zhong Qiu Lin",
    "Alexander Wong"
  ],
  "references": [
    "2194775991",
    "3001118548",
    "2962835968",
    "3008827533",
    "2919115771",
    "2108598243",
    "2963446712",
    "3007497549",
    "3010604545",
    "3008985036"
  ]
}, {
  "id": "2950893734",
  "title": "Self-Attention Generative Adversarial Networks | Paper | Microsoft Academic",
  "abstract": "In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN achieves the state-of-the-art results, boosting the best published Inception score from 36.8 to 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.",
  "date": " 2018 ",
  "authors": [
    "Han Zhang",
    "Ian Goodfellow",
    "Dimitris Metaxas",
    "Augustus Odena"
  ],
  "references": [
    "2964121744",
    "2963403868",
    "2117539524",
    "2099471712",
    "2964308564",
    "2963073614",
    "2962793481",
    "2963684088",
    "2963373786",
    "2963470893"
  ]
}, {
  "id": "2194775991",
  "title": "Deep Residual Learning for Image Recognition | Paper | Microsoft Academic",
  "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers\u20148\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
  "date": " 2016 ",
  "authors": [
    "Kaiming He",
    "Xiangyu Zhang",
    "Shaoqing Ren",
    "Jian Sun"
  ],
  "references": [
    "2618530766",
    "2962835968",
    "2097117768",
    "639708223",
    "1836465849",
    "2102605133",
    "2117539524",
    "1903029394",
    "2155893237",
    "1536680647"
  ]
}, {
  "id": "2963403868",
  "title": "Attention is All You Need | Paper | Microsoft Academic",
  "abstract": "The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms. We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.",
  "date": " 2017 ",
  "authors": [
    "Ashish Vaswani",
    "Noam Shazeer",
    "Niki Parmar",
    "Jakob Uszkoreit",
    "Llion Jones",
    "Aidan N. Gomez",
    "Lukasz Kaiser",
    "Illia Polosukhin"
  ],
  "references": [
    "2963341956",
    "2965373594",
    "2970597249",
    "2963091558",
    "2923014074",
    "2996428491",
    "2911489562",
    "2964110616"
  ]
}, {
  "id": "1836465849",
  "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift | Paper | Microsoft Academic",
  "abstract": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.",
  "date": " 2015 ",
  "authors": [
    "Sergey Ioffe",
    "Christian Szegedy"
  ],
  "references": [
    "2097117768",
    "2117539524",
    "2095705004",
    "1677182931",
    "2146502635",
    "2310919327",
    "1665214252",
    "2168231600",
    "1533861849",
    "104184427"
  ]
}, {
  "id": "2964308564",
  "title": "Neural Machine Translation by Jointly Learning to Align and Translate | Paper | Microsoft Academic",
  "abstract": "Abstract: Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
  "date": " 2014 ",
  "authors": [
    "Dzmitry Bahdanau",
    "Kyunghyun Cho",
    "Yoshua Bengio"
  ],
  "references": [
    "2157331557",
    "2064675550",
    "2294059674",
    "2132339004",
    "6908809",
    "1753482797",
    "1810943226",
    "2964199361",
    "2153653739",
    "1815076433"
  ]
}, {
  "id": "1677182931",
  "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification | Paper | Microsoft Academic",
  "abstract": "Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66% [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1%, [26]) on this dataset.",
  "date": " 2015 ",
  "authors": [
    "Kaiming He",
    "Xiangyu Zhang",
    "Shaoqing Ren",
    "Jian Sun"
  ],
  "references": [
    "2618530766",
    "2962835968",
    "2097117768",
    "1836465849",
    "2102605133",
    "2117539524",
    "2095705004",
    "2108598243",
    "2155893237",
    "1536680647"
  ]
}, {
  "id": "2963446712",
  "title": "Densely Connected Convolutional Networks | Paper | Microsoft Academic",
  "abstract": "Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections&#x2014;one between each layer and its subsequent layer&#x2014;our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.",
  "date": " 2017 ",
  "authors": [
    "Gao Huang",
    "Zhuang Liu",
    "Laurens van der Maaten",
    "Kilian Q. Weinberger"
  ],
  "references": [
    "2194775991",
    "2618530766",
    "2097117768",
    "1836465849",
    "2117539524",
    "1903029394",
    "2095705004",
    "2108598243",
    "1677182931",
    "2183341477"
  ]
}, {
  "id": "2157331557",
  "title": "Learning Phrase Representations using RNN Encoder--Decoder for Statistical Machine Translation | Paper | Microsoft Academic",
  "abstract": "In this paper, we propose a novel neural network model called RNN Encoder\u2010 Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder\u2010Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
  "date": " 2014 ",
  "authors": [
    "Kyunghyun Cho",
    "Bart van Merrienboer",
    "Caglar Gulcehre",
    "Dzmitry Bahdanau",
    "Fethi Bougares",
    "Holger Schwenk",
    "Yoshua Bengio"
  ],
  "references": [
    "2618530766",
    "2153579005",
    "2064675550",
    "2294059674",
    "2147768505",
    "2132339004",
    "6908809",
    "1753482797",
    "2156387975",
    "2963504252"
  ]
}, {
  "id": "2310919327",
  "title": "Gradient-based learning applied to document recognition | Paper | Microsoft Academic",
  "abstract": "",
  "date": " 2000 ",
  "authors": [
    "Yann Lecun",
    "Leon Bottou",
    "Yoshua Bengio",
    "Patrick Haffner"
  ],
  "references": [
    "2147880316",
    "2156163116",
    "2963399829",
    "2964311892",
    "2157364932",
    "1510526001",
    "1944615693",
    "2158778629"
  ]
}, {
  "id": "2064675550",
  "title": "Long short-term memory | Paper | Microsoft Academic",
  "abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.",
  "date": " 1997 ",
  "authors": [
    "Sepp Hochreiter",
    "J\u00fcrgen Schmidhuber"
  ],
  "references": [
    "2107878631",
    "2128499899",
    "2007431958",
    "2123716044",
    "2143503258",
    "2154890045",
    "194249466",
    "2048060899",
    "2103452139",
    "1674799117"
  ]
}, {
  "id": "1533861849",
  "title": "Understanding the difficulty of training deep feedforward neural networks | Paper | Microsoft Academic",
  "abstract": "Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence. 1 Deep Neural Networks Deep learning methods aim at learning feature hierarchies with features from higher levels of the hierarchy formed by the composition of lower level features. They include Appearing in Proceedings of the 13 International Conference on Artificial Intelligence and Statistics (AISTATS) 2010, Chia Laguna Resort, Sardinia, Italy. Volume 9 of JMLR: WC Weston et al., 2008). Much attention has recently been devoted to them (see (Bengio, 2009) for a review), because of their theoretical appeal, inspiration from biology and human cognition, and because of empirical success in vision (Ranzato et al., 2007; Larochelle et al., 2007; Vincent et al., 2008) and natural language processing (NLP) (Collobert & Weston, 2008; Mnih & Hinton, 2009). Theoretical results reviewed and discussed by Bengio (2009), suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one may need deep architectures. Most of the recent experimental results with deep architecture are obtained with models that can be turned into deep supervised neural networks, but with initialization or training schemes different from the classical feedforward neural networks (Rumelhart et al., 1986). Why are these new algorithms working so much better than the standard random initialization and gradient-based optimization of a supervised training criterion? Part of the answer may be found in recent analyses of the effect of unsupervised pretraining (Erhan et al., 2009), showing that it acts as a regularizer that initializes the parameters in a \u201cbetter\u201d basin of attraction of the optimization procedure, corresponding to an apparent local minimum associated with better generalization. But earlier work (Bengio et al., 2007) had shown that even a purely supervised but greedy layer-wise procedure would give better results. So here instead of focusing on what unsupervised pre-training or semi-supervised criteria bring to deep architectures, we focus on analyzing what may be going wrong with good old (but deep) multilayer neural networks. Our analysis is driven by investigative experiments to monitor activations (watching for saturation of hidden units) and gradients, across layers and across training iterations. We also evaluate the effects on these of choices of activation function (with the idea that it might affect saturation) and initialization procedure (since unsupervised pretraining is a particular form of initialization and it has a drastic impact).",
  "date": " 2010 ",
  "authors": [
    "Xavier Glorot",
    "Yoshua Bengio"
  ],
  "references": [
    "2136922672",
    "3118608800",
    "2310919327",
    "2072128103",
    "2117130368",
    "2025768430",
    "2110798204",
    "1498436455",
    "1994197834",
    "2131462252"
  ]
}, {
  "id": "3001118548",
  "title": "Clinical features of patients infected with 2019 novel coronavirus in Wuhan, China | Paper | Microsoft Academic",
  "abstract": "A recent cluster of pneumonia cases in Wuhan, China, was caused by a novel betacoronavirus, the 2019 novel coronavirus (2019-nCoV). We report the epidemiological, clinical, laboratory, and radiological characteristics and treatment and clinical outcomes of these patients. All patients with suspected 2019-nCoV were admitted to a designated hospital in Wuhan. We prospectively collected and analysed data on patients with laboratory-confirmed 2019-nCoV infection by real-time RT-PCR and next-generation sequencing. Data were obtained with standardised data collection forms shared by the International Severe Acute Respiratory and Emerging Infection Consortium from electronic medical records. Researchers also directly communicated with patients or their families to ascertain epidemiological and symptom data. Outcomes were also compared between patients who had been admitted to the intensive care unit (ICU) and those who had not.",
  "date": " 2020 ",
  "authors": [
    "Chaolin Huang",
    "Yeming Wang",
    "Xingwang Li",
    "Lili Ren",
    "Jianping Zhao",
    "Yi Hu",
    "Li Zhang",
    "Guohui Fan",
    "Jiuyang Xu",
    "Xiaoying Gu",
    "Zhenshun Cheng",
    "Ting Yu",
    "Jiaan Xia",
    "Yuan Wei",
    "Wenjuan Wu",
    "Xuelei Xie",
    "Wen Yin",
    "Hui Li",
    "Min Liu",
    "Yan Xiao",
    "Hong Gao",
    "Li Guo",
    "Jungang Xie",
    "Guangfa Wang",
    "Rongmeng Jiang",
    "Zhancheng Gao",
    "Qi Jin",
    "Jianwei Wang",
    "Bin Cao"
  ],
  "references": [
    "2903899730",
    "2166867592",
    "3000413850",
    "2026274122",
    "2132260239",
    "2104548316",
    "2131262274",
    "2006434809",
    "2725497285",
    "3011483298"
  ]
}, {
  "id": "2962835968",
  "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition | Paper | Microsoft Academic",
  "abstract": "Abstract: In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.",
  "date": " 2014 ",
  "authors": [
    "Karen Simonyan",
    "Andrew Zisserman"
  ],
  "references": [
    "2194775991",
    "639708223",
    "1901129140",
    "1903029394",
    "1536680647",
    "3106250896"
  ]
}, {
  "id": "3008827533",
  "title": "Clinical Characteristics of Coronavirus Disease 2019 in China | Paper | Microsoft Academic",
  "abstract": "Abstract Background Since December 2019, when coronavirus disease 2019 (Covid-19) emerged in Wuhan city and rapidly spread throughout China, data have been needed on the clinical characteristics of...",
  "date": " 2020 ",
  "authors": [
    "Wei-Jie Guan",
    "Zheng-Yi Ni",
    "Yu Hu",
    "Wen-Hua Liang",
    "Chun-Quan Ou",
    "Jian-Xing He",
    "Lei Liu",
    "Hong Shan",
    "Chun-Liang Lei",
    "David S C Hui",
    "Bin Du",
    "Lan-Juan Li",
    "Guang Zeng",
    "Kwok-Yung Yuen",
    "Ru-Chong Chen",
    "Chun-Li Tang",
    "Tao Wang",
    "Ping-Yan Chen",
    "Jie Xiang",
    "Shi-Yue Li",
    "Jin-Lin Wang",
    "Zi-Jing Liang",
    "Yi-Xiang Peng",
    "Li Wei",
    "Yong Liu",
    "Ya-Hua Hu",
    "Peng Peng",
    "Jian-Ming Wang",
    "Ji-Yang Liu",
    "Zhong Chen",
    "Gang Li",
    "Zhi-Jian Zheng",
    "Shao-Qin Qiu",
    "Jie Luo",
    "Chang-Jiang Ye",
    "Shao-Yong Zhu",
    "Nan-Shan Zhong"
  ],
  "references": [
    "3001118548",
    "3001897055",
    "3005079553",
    "3003668884",
    "3002108456",
    "3002539152",
    "3004318991",
    "3003465021",
    "3004239190",
    "3003573988"
  ]
}, {
  "id": "2919115771",
  "title": "Deep learning | Paper | Microsoft Academic",
  "abstract": "Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.",
  "date": " 2015 ",
  "authors": [
    "Yann LeCun",
    "Yoshua Bengio",
    "Geoffrey Hinton"
  ],
  "references": [
    "2145339207",
    "2136922672",
    "2100495367",
    "2310919327",
    "2064675550",
    "2163922914",
    "2160815625",
    "2022508996",
    "1993882792",
    "2108069432"
  ]
}, {
  "id": "2108598243",
  "title": "ImageNet: A large-scale hierarchical image database | Paper | Microsoft Academic",
  "abstract": "The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called \u201cImageNet\u201d, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.",
  "date": " 2009 ",
  "authors": [
    "Jia Deng",
    "Wei Dong",
    "Richard Socher",
    "Li-Jia Li",
    "Kai Li",
    "Li Fei-Fei"
  ],
  "references": [
    "2151103935",
    "2038721957",
    "2128017662",
    "2110764733",
    "1782590233",
    "1576445103",
    "2145607950",
    "2141282920",
    "2115733720",
    "1528789833"
  ]
}, {
  "id": "3007497549",
  "title": "Correlation of Chest CT and RT-PCR Testing for Coronavirus Disease 2019 (COVID-19) in China: A Report of 1014 Cases. | Paper | Microsoft Academic",
  "abstract": "Chest CT had higher sensitivity for diagnosis of COVID-19 as compared with initial reverse-transcription polymerase chain reaction from swab samples in the epidemic area of China.",
  "date": " 2020 ",
  "authors": [
    "Tao Ai",
    "Zhenlu Yang",
    "Hongyan Hou",
    "Chenao Zhan",
    "Chong Chen",
    "Wenzhi Lv",
    "Qian Tao",
    "Ziyong Sun",
    "Liming Xia"
  ],
  "references": [
    "3001118548",
    "3008818676",
    "3004906315",
    "3006110666",
    "3006643024",
    "3006354146",
    "3003901880",
    "3005656138",
    "3004511262"
  ]
}, {
  "id": "3010604545",
  "title": "Detection of SARS-CoV-2 in Different Types of Clinical Specimens. | Paper | Microsoft Academic",
  "abstract": "This study describes results of PCR and viral RNA testing for SARS-CoV-2 in bronchoalveolar fluid, sputum, feces, blood, and urine specimens from patients with COVID-19 infection in China to identify possible means of non-respiratory transmission.",
  "date": " 2020 ",
  "authors": [
    "Wenling Wang",
    "Yanli Xu",
    "Ruqin Gao",
    "Roujian Lu",
    "Kai Han",
    "Guizhen Wu",
    "Wenjie Tan"
  ],
  "references": [
    "3005079553",
    "3008962515",
    "3008452791",
    "3033453353",
    "3034408674",
    "3035275617",
    "3034059415",
    "3033952286",
    "3035464429",
    "3036958556"
  ]
}, {
  "id": "3008985036",
  "title": "Sensitivity of Chest CT for COVID-19: Comparison to RT-PCR | Paper | Microsoft Academic",
  "abstract": "In a series of 51 patients with chest CT and real-time polymerase chain reaction assay (RT-PCR) performed within 3 days, the sensitivity of CT for 2019 novel coronavirus infection was 98% and that ...",
  "date": " 2020 ",
  "authors": [
    "Yicheng Fang",
    "Huangqi Zhang",
    "Jicheng Xie",
    "Minjie Lin",
    "Lingjun Ying",
    "Peipei Pang",
    "Wenbin Ji"
  ],
  "references": [
    "3002108456",
    "3006110666",
    "3006643024",
    "3028749392",
    "3032185657",
    "3042098369",
    "3037255629"
  ]
}, {
  "id": "2964121744",
  "title": "Adam: A Method for Stochastic Optimization | Paper | Microsoft Academic",
  "abstract": "Abstract: We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
  "date": " 2014 ",
  "authors": [
    "Diederik P. Kingma",
    "Jimmy Lei Ba"
  ],
  "references": [
    "2963403868",
    "2962739339",
    "2963073614",
    "2962793481",
    "2331128040",
    "2963470893",
    "2964015378",
    "1514535095",
    "2508457857"
  ]
}, {
  "id": "2117539524",
  "title": "ImageNet Large Scale Visual Recognition Challenge | Paper | Microsoft Academic",
  "abstract": "The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.",
  "date": " 2015 ",
  "authors": [
    "Olga Russakovsky",
    "Jia Deng",
    "Hao Su",
    "Jonathan Krause",
    "Sanjeev Satheesh",
    "Sean Ma",
    "Zhiheng Huang",
    "Andrej Karpathy",
    "Aditya Khosla",
    "Michael Bernstein",
    "Alexander C. Berg",
    "Li Fei-Fei"
  ],
  "references": [
    "2618530766",
    "2962835968",
    "2151103935",
    "2097117768",
    "2102605133",
    "1614298861",
    "2108598243",
    "2155893237",
    "2168356304",
    "1849277567"
  ]
}, {
  "id": "2099471712",
  "title": "Generative Adversarial Nets | Paper | Microsoft Academic",
  "abstract": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to \u00bd everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",
  "date": " 2014 ",
  "authors": [
    "Ian Goodfellow",
    "Jean Pouget-Abadie",
    "Mehdi Mirza",
    "Bing Xu",
    "David Warde-Farley",
    "Sherjil Ozair",
    "Aaron Courville",
    "Yoshua Bengio"
  ],
  "references": [
    "2618530766",
    "2136922672",
    "1959608418",
    "3118608800",
    "2310919327",
    "1904365287",
    "2964153729",
    "2072128103",
    "2546302380",
    "2294059674"
  ]
}, {
  "id": "2963073614",
  "title": "Image-to-Image Translation with Conditional Adversarial Networks | Paper | Microsoft Academic",
  "abstract": "We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Moreover, since the release of the pix2pix software associated with this paper, hundreds of twitter users have posted their own artistic experiments using our system. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without handengineering our loss functions either.",
  "date": " 2017 ",
  "authors": [
    "Phillip Isola",
    "Jun-Yan Zhu",
    "Tinghui Zhou",
    "Alexei A. Efros"
  ],
  "references": [
    "2964121744",
    "1836465849",
    "2117539524",
    "1901129140",
    "1903029394",
    "2099471712",
    "2133665775",
    "2100495367",
    "2963684088",
    "2340897893"
  ]
}, {
  "id": "2962793481",
  "title": "Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks | Paper | Microsoft Academic",
  "abstract": "Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X \u2192 Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y \u2192 X and introduce a cycle consistency loss to push F(G(X)) \u2248 X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.",
  "date": " 2017 ",
  "authors": [
    "Jun-Yan Zhu",
    "Taesung Park",
    "Phillip Isola",
    "Alexei A. Efros"
  ],
  "references": [
    "2194775991",
    "2962835968",
    "2117539524",
    "1903029394",
    "2099471712",
    "1959608418",
    "2100495367",
    "2963684088",
    "2340897893",
    "2963373786"
  ]
}, {
  "id": "2963684088",
  "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks | Paper | Microsoft Academic",
  "abstract": "Abstract: In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.",
  "date": " 2015 ",
  "authors": [
    "Alec Radford",
    "Luke Metz",
    "Soumith Chintala"
  ],
  "references": [
    "2962793481",
    "2331128040",
    "2963470893",
    "2963420272",
    "2405756170",
    "2963800363",
    "2963836885",
    "2738588019",
    "2962947361"
  ]
}, {
  "id": "2963373786",
  "title": "Improved techniques for training GANs | Paper | Microsoft Academic",
  "abstract": "We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.",
  "date": " 2016 ",
  "authors": [
    "Tim Salimans",
    "Ian Goodfellow",
    "Wojciech Zaremba",
    "Vicki Cheung",
    "Alec Radford",
    "Xi Chen"
  ],
  "references": [
    "1836465849",
    "2183341477",
    "2963684088",
    "2964153729",
    "2271840356",
    "648143168",
    "830076066",
    "2963685250",
    "2949416428",
    "1487641199"
  ]
}, {
  "id": "2963470893",
  "title": "Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network | Paper | Microsoft Academic",
  "abstract": "Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.",
  "date": " 2017 ",
  "authors": [
    "Christian Ledig",
    "Lucas Theis",
    "Ferenc Huszar",
    "Jose Caballero",
    "Andrew Cunningham",
    "Alejandro Acosta",
    "Andrew Aitken",
    "Alykhan Tejani",
    "Johannes Totz",
    "Zehan Wang",
    "Wenzhe Shi"
  ],
  "references": [
    "2194775991",
    "2618530766",
    "2962835968",
    "2964121744",
    "2097117768",
    "1836465849",
    "2117539524",
    "2099471712",
    "1677182931",
    "2133665775"
  ]
}, {
  "id": "2618530766",
  "title": "ImageNet classification with deep convolutional neural networks | Paper | Microsoft Academic",
  "abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.",
  "date": " 2017 ",
  "authors": [
    "Alex Krizhevsky",
    "Ilya Sutskever",
    "Geoffrey E. Hinton"
  ],
  "references": [
    "2194775991",
    "2097117768",
    "2108598243",
    "2911964244",
    "3118608800",
    "1904365287",
    "1665214252",
    "2546302380",
    "2110764733",
    "2130325614"
  ]
}, {
  "id": "2097117768",
  "title": "Going deeper with convolutions | Paper | Microsoft Academic",
  "abstract": "We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.",
  "date": " 2015 ",
  "authors": [
    "Christian Szegedy",
    "Wei Liu",
    "Yangqing Jia",
    "Pierre Sermanet",
    "Scott Reed",
    "Dragomir Anguelov",
    "Dumitru Erhan",
    "Vincent Vanhoucke",
    "Andrew Rabinovich"
  ],
  "references": [
    "2618530766",
    "2102605133",
    "1849277567",
    "2963542991",
    "2310919327",
    "1904365287",
    "2963911037",
    "2168231600",
    "2068730032",
    "104184427"
  ]
}, {
  "id": "639708223",
  "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks | Paper | Microsoft Academic",
  "abstract": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet  [1]  and Fast R-CNN  [2]  have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a  Region Proposal Network  (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features\u2014using the recently popular terminology of neural networks with \u2019attention\u2019 mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model  [3]  , our detection system has a frame rate of 5 fps ( including all steps ) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.",
  "date": " 2017 ",
  "authors": [
    "Shaoqing Ren",
    "Kaiming He",
    "Ross Girshick",
    "Jian Sun"
  ],
  "references": [
    "2194775991",
    "2618530766",
    "2962835968",
    "2097117768",
    "2102605133",
    "2117539524",
    "1903029394",
    "2155893237",
    "1536680647",
    "2168356304"
  ]
}, {
  "id": "2102605133",
  "title": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation | Paper | Microsoft Academic",
  "abstract": "Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.",
  "date": " 2014 ",
  "authors": [
    "Ross Girshick",
    "Jeff Donahue",
    "Trevor Darrell",
    "Jitendra Malik"
  ],
  "references": [
    "2618530766",
    "2151103935",
    "2161969291",
    "2108598243",
    "2168356304",
    "3118608800",
    "2310919327",
    "2031489346",
    "2088049833",
    "2155541015"
  ]
}, {
  "id": "1903029394",
  "title": "Fully convolutional networks for semantic segmentation | Paper | Microsoft Academic",
  "abstract": "Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build \u201cfully convolutional\u201d networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.",
  "date": " 2015 ",
  "authors": [
    "Jonathan Long",
    "Evan Shelhamer",
    "Trevor Darrell"
  ],
  "references": [
    "2618530766",
    "2962835968",
    "2097117768",
    "2102605133",
    "2155893237",
    "1663973292",
    "1849277567",
    "2963542991",
    "2109255472",
    "2155541015"
  ]
}, {
  "id": "2155893237",
  "title": "Caffe: Convolutional Architecture for Fast Feature Embedding | Paper | Microsoft Academic",
  "abstract": "Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (approx 2 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments.   Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.",
  "date": " 2014 ",
  "authors": [
    "Yangqing Jia",
    "Evan Shelhamer",
    "Jeff Donahue",
    "Sergey Karayev",
    "Jonathan Long",
    "Ross Girshick",
    "Sergio Guadarrama",
    "Trevor Darrell"
  ],
  "references": [
    "2618530766",
    "2102605133",
    "2963542991",
    "2088049833",
    "2155541015",
    "753012316",
    "1825604117",
    "2147414309",
    "1872489089",
    "2962883796"
  ]
}, {
  "id": "1536680647",
  "title": "Fast R-CNN | Paper | Microsoft Academic",
  "abstract": "This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.",
  "date": " 2015 ",
  "authors": [
    "Ross Girshick"
  ],
  "references": [
    "2618530766",
    "2962835968",
    "2102605133",
    "2108598243",
    "2155893237",
    "2168356304",
    "2164598857",
    "1861492603",
    "2963542991",
    "2109255472"
  ]
}, {
  "id": "2963341956",
  "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding | Paper | Microsoft Academic",
  "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
  "date": " 2018 ",
  "authors": [
    "Jacob Devlin",
    "Ming-Wei Chang",
    "Kenton Lee",
    "Kristina N. Toutanova"
  ],
  "references": [
    "2963403868",
    "2153579005",
    "2250539671",
    "2108598243",
    "2962739339",
    "2131744502",
    "2251939518",
    "2963748441",
    "2117130368",
    "2025768430"
  ]
}, {
  "id": "2965373594",
  "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach | Paper | Microsoft Academic",
  "abstract": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
  "date": " 2019 ",
  "authors": [
    "Yinhan Liu",
    "Myle Ott",
    "Naman Goyal",
    "Jingfei Du",
    "Mandar Joshi",
    "Danqi Chen",
    "Omer Levy",
    "Mike Lewis",
    "Luke Zettlemoyer",
    "Veselin Stoyanov"
  ],
  "references": [
    "2964121744",
    "2963403868",
    "2963341956",
    "2251939518",
    "2899771611",
    "2963748441",
    "2962784628",
    "1840435438",
    "2970597249",
    "2963026768"
  ]
}, {
  "id": "2970597249",
  "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding | Paper | Microsoft Academic",
  "abstract": "With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment setting, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.",
  "date": " 2019 ",
  "authors": [
    "Zhilin Yang",
    "Zihang Dai",
    "Yiming Yang",
    "Jaime G. Carbonell",
    "Ruslan Salakhutdinov",
    "Quoc V. Le"
  ],
  "references": [
    "2996035354",
    "2990704537",
    "3100307207",
    "3105966348",
    "3034238904",
    "3099342932",
    "2995015695"
  ]
}, {
  "id": "2963091558",
  "title": "Non-local Neural Networks | Paper | Microsoft Academic",
  "abstract": "Both convolutional and recurrent operations are building blocks that process one local neighborhood at a time. In this paper, we present non-local operations as a generic family of building blocks for capturing long-range dependencies. Inspired by the classical non-local means method [4] in computer vision, our non-local operation computes the response at a position as a weighted sum of the features at all positions. This building block can be plugged into many computer vision architectures. On the task of video classification, even without any bells and whistles, our nonlocal models can compete or outperform current competition winners on both Kinetics and Charades datasets. In static image recognition, our non-local models improve object detection/segmentation and pose estimation on the COCO suite of tasks. Code will be made available.",
  "date": " 2018 ",
  "authors": [
    "Xiaolong Wang",
    "Ross Girshick",
    "Abhinav Gupta",
    "Kaiming He"
  ],
  "references": [
    "2194775991",
    "2962835968",
    "639708223",
    "2963403868",
    "1836465849",
    "2117539524",
    "1677182931",
    "2806070179",
    "1861492603",
    "1904365287"
  ]
}, {
  "id": "2923014074",
  "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding | Paper | Microsoft Academic",
  "abstract": "Human ability to understand language is general, flexible, and robust. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.",
  "date": " 2018 ",
  "authors": [
    "Alex Wang",
    "Amanpreet Singh",
    "Julian Michael",
    "Felix Hill",
    "Omer Levy",
    "Samuel R. Bowman"
  ],
  "references": [
    "2962739339",
    "2251939518",
    "2963748441",
    "1486649854",
    "2963918774",
    "2963846996",
    "2525127255",
    "2962736243",
    "3104033643",
    "2130158090"
  ]
}, {
  "id": "2996428491",
  "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations | Paper | Microsoft Academic",
  "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations, longer training times, and unexpected model degradation. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large.",
  "date": " 2020 ",
  "authors": [
    "Zhenzhong Lan",
    "Mingda Chen",
    "Sebastian Goodman",
    "Kevin Gimpel",
    "Piyush Sharma",
    "Radu Soricut"
  ],
  "references": [
    "2963403868",
    "2963341956",
    "2153579005",
    "2250539671",
    "2131744502",
    "2251939518",
    "2963748441",
    "2964350391",
    "2965373594",
    "2970597249"
  ]
}, {
  "id": "2911489562",
  "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining. | Paper | Microsoft Academic",
  "abstract": "Motivation  Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora.  Results  We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62% F1 score improvement), biomedical relation extraction (2.80% F1 score improvement) and biomedical question answering (12.24% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts.  Availability and implementation  We make the pre-trained weights of BioBERT freely available at https://github.com/naver/biobert-pretrained, and the source code for fine-tuning BioBERT available at https://github.com/dmis-lab/biobert.",
  "date": " 2019 ",
  "authors": [
    "Jinhyuk Lee",
    "Wonjin Yoon",
    "Sungdong Kim",
    "Donghyeon Kim",
    "Sunkyu Kim",
    "Chan Ho So",
    "Jaewoo Kang"
  ],
  "references": [
    "2919115771",
    "2963403868",
    "2963341956",
    "2153579005",
    "2250539671",
    "2962739339",
    "2963748441",
    "2132339004",
    "2525778437",
    "2963756346"
  ]
}, {
  "id": "2964110616",
  "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context. | Paper | Microsoft Academic",
  "abstract": "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.",
  "date": " 2019 ",
  "authors": [
    "Zihang Dai",
    "Zhilin Yang",
    "Yiming Yang",
    "Jaime G. Carbonell",
    "Quoc Viet Le",
    "Ruslan Salakhutdinov"
  ],
  "references": [
    "2963403868",
    "2963341956",
    "2964308564",
    "2962739339",
    "2064675550",
    "179875071",
    "2132339004",
    "1810943226",
    "2963374479",
    "2584341106"
  ]
}, {
  "id": "2095705004",
  "title": "Dropout: a simple way to prevent neural networks from overfitting | Paper | Microsoft Academic",
  "abstract": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \"thinned\" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.",
  "date": " 2013 ",
  "authors": [
    "Nitish Srivastava",
    "Geoffrey Hinton",
    "Alex Krizhevsky",
    "Ilya Sutskever",
    "Ruslan Salakhutdinov"
  ],
  "references": [
    "2618530766",
    "2136922672",
    "3118608800",
    "2100495367",
    "2135046866",
    "2546302380",
    "2294059674",
    "2145094598",
    "2025768430",
    "2131241448"
  ]
}, {
  "id": "2146502635",
  "title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization | Paper | Microsoft Academic",
  "abstract": "We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.",
  "date": " 2011 ",
  "authors": [
    "John Duchi",
    "Elad Hazan",
    "Yoram Singer"
  ],
  "references": [
    "2296319761",
    "2108598243",
    "3120740533",
    "2798766386",
    "2610857016",
    "2167732364",
    "2150102617",
    "2124541940",
    "1992208280",
    "2160218441"
  ]
}, {
  "id": "1665214252",
  "title": "Rectified Linear Units Improve Restricted Boltzmann Machines | Paper | Microsoft Academic",
  "abstract": "Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these \"Stepped Sigmoid Units\" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.",
  "date": " 2010 ",
  "authors": [
    "Vinod Nair",
    "Geoffrey E. Hinton"
  ],
  "references": [
    "2136922672",
    "2100495367",
    "2116064496",
    "2546302380",
    "1782590233",
    "2134557905",
    "2099866409",
    "1994197834",
    "2536626143",
    "2157364932"
  ]
}, {
  "id": "2168231600",
  "title": "Large Scale Distributed Deep Networks | Paper | Microsoft Academic",
  "abstract": "Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.",
  "date": " 2012 ",
  "authors": [
    "Jeffrey Dean",
    "Greg Corrado",
    "Rajat Monga",
    "Kai Chen",
    "Matthieu Devin",
    "Mark Mao",
    "Marc'aurelio Ranzato",
    "Andrew Senior",
    "Paul Tucker",
    "Ke Yang",
    "Quoc V. Le",
    "Andrew Y. Ng"
  ],
  "references": [
    "2173213060",
    "2108598243",
    "2146502635",
    "3118608800",
    "2117130368",
    "2147768505",
    "2132339004",
    "2141125852",
    "2184045248",
    "2118858186"
  ]
}, {
  "id": "104184427",
  "title": "On the importance of initialization and momentum in deep learning | Paper | Microsoft Academic",
  "abstract": "Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned.\r\n\r\nOur success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.",
  "date": " 2013 ",
  "authors": [
    "Ilya Sutskever",
    "James Martens",
    "George Dahl",
    "Geoffrey Hinton"
  ],
  "references": [
    "2618530766",
    "2136922672",
    "2100495367",
    "2064675550",
    "1533861849",
    "2147768505",
    "2110798204",
    "1993882792",
    "2184045248",
    "196761320"
  ]
}, {
  "id": "2294059674",
  "title": "Maxout Networks | Paper | Microsoft Academic",
  "abstract": "We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.",
  "date": " 2013 ",
  "authors": [
    "Ian Goodfellow",
    "David Warde-Farley",
    "Mehdi Mirza",
    "Aaron Courville",
    "Yoshua Bengio"
  ],
  "references": [
    "2618530766",
    "3118608800",
    "2310919327",
    "1904365287",
    "2912934387",
    "2546302380",
    "2131241448",
    "2335728318",
    "2156387975",
    "189596042"
  ]
}, {
  "id": "2132339004",
  "title": "A neural probabilistic language model | Paper | Microsoft Academic",
  "abstract": "A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.",
  "date": " 2003 ",
  "authors": [
    "Yoshua Bengio",
    "R\u00e9jean Ducharme",
    "Pascal Vincent",
    "Christian Janvin"
  ],
  "references": [
    "2038721957",
    "2116064496",
    "2147152072",
    "1631260214",
    "2096175520",
    "2110485445",
    "1575350781",
    "2158195707",
    "2121227244",
    "2914484425"
  ]
}, {
  "id": "6908809",
  "title": "ADADELTA: An Adaptive Learning Rate Method | Paper | Microsoft Academic",
  "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.",
  "date": " 2012 ",
  "authors": [
    "Matthew D. Zeiler"
  ],
  "references": [
    "2168231600",
    "2147768505",
    "1498436455",
    "2120420045",
    "19621276",
    "1994616650"
  ]
}, {
  "id": "1753482797",
  "title": "Recurrent Continuous Translation Models | Paper | Microsoft Academic",
  "abstract": "We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.",
  "date": " 2013 ",
  "authors": [
    "Nal Kalchbrenner",
    "Phil Blunsom"
  ],
  "references": [
    "2146502635",
    "2117130368",
    "179875071",
    "2132339004",
    "1889268436",
    "2006969979",
    "2171928131",
    "2103305545",
    "2251222643",
    "196214544"
  ]
}, {
  "id": "1810943226",
  "title": "Generating Sequences With Recurrent Neural Networks | Paper | Microsoft Academic",
  "abstract": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.",
  "date": " 2013 ",
  "authors": [
    "Alex Graves"
  ],
  "references": [
    "2064675550",
    "1554663460",
    "2143612262",
    "44815768",
    "1632114991",
    "2131462252",
    "196214544",
    "2108677974",
    "2120861206",
    "3023071679"
  ]
}, {
  "id": "2964199361",
  "title": "On the Properties of Neural Machine Translation: Encoder--Decoder Approaches | Paper | Microsoft Academic",
  "abstract": "Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder\u2010Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.",
  "date": " 2014 ",
  "authors": [
    "Kyunghyun Cho",
    "Bart van Merrienboer",
    "Dzmitry Bahdanau",
    "Yoshua Bengio"
  ],
  "references": [
    "2130942839",
    "2157331557",
    "6908809",
    "1753482797",
    "1810943226",
    "2153653739",
    "2395935897",
    "1905522558",
    "1828163288",
    "2341457423"
  ]
}, {
  "id": "2153653739",
  "title": "Statistical phrase-based translation | Paper | Microsoft Academic",
  "abstract": "We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models out-perform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy word-level alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.",
  "date": " 2003 ",
  "authors": [
    "Philipp Koehn",
    "Franz Josef Och",
    "Daniel Marcu"
  ],
  "references": [
    "2101105183",
    "2006969979",
    "1508165687",
    "1973923101",
    "1986543644",
    "2116316001",
    "1517947178",
    "2161792612",
    "1549285799",
    "2158388102"
  ]
}, {
  "id": "1815076433",
  "title": "On the difficulty of training recurrent neural networks | Paper | Microsoft Academic",
  "abstract": "There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.",
  "date": " 2013 ",
  "authors": [
    "Razvan Pascanu",
    "Tomas Mikolov",
    "Yoshua Bengio"
  ],
  "references": [
    "2146502635",
    "2064675550",
    "1498436455",
    "1606347560",
    "196214544",
    "2110485445",
    "2171865010",
    "2118706537",
    "2122585011",
    "2107878631"
  ]
}, {
  "id": "2183341477",
  "title": "Rethinking the Inception Architecture for Computer Vision | Paper | Microsoft Academic",
  "abstract": "Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set.",
  "date": " 2016 ",
  "authors": [
    "Christian Szegedy",
    "Vincent Vanhoucke",
    "Sergey Ioffe",
    "Jon Shlens",
    "Zbigniew Wojna"
  ],
  "references": [
    "2618530766",
    "2962835968",
    "2097117768",
    "1836465849",
    "2102605133",
    "2117539524",
    "1903029394",
    "1677182931",
    "2096733369",
    "2016053056"
  ]
}, {
  "id": "2153579005",
  "title": "Distributed Representations of Words and Phrases and their Compositionality | Paper | Microsoft Academic",
  "abstract": "The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.\r\n\r\nAn inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
  "date": " 2013 ",
  "authors": [
    "Tomas Mikolov",
    "Ilya Sutskever",
    "Kai Chen",
    "Greg S Corrado",
    "Jeff Dean"
  ],
  "references": [
    "1614298861",
    "2141599568",
    "2117130368",
    "2132339004",
    "2158139315",
    "1423339008",
    "1498436455",
    "1662133657",
    "1889268436",
    "2131462252"
  ]
}, {
  "id": "2147768505",
  "title": "Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition | Paper | Microsoft Academic",
  "abstract": "We propose a novel context-dependent (CD) model for large-vocabulary speech recognition (LVSR) that leverages recent advances in using deep belief networks for phone recognition. We describe a pre-trained deep neural network hidden Markov model (DNN-HMM) hybrid architecture that trains the DNN to produce a distribution over senones (tied triphone states) as its output. The deep belief network pre-training algorithm is a robust and often helpful way to initialize deep neural networks generatively that can aid in optimization and reduce generalization error. We illustrate the key components of our model, describe the procedure for applying CD-DNN-HMMs to LVSR, and analyze the effects of various modeling choices on performance. Experiments on a challenging business search dataset demonstrate that CD-DNN-HMMs can significantly outperform the conventional context-dependent Gaussian mixture model (GMM)-HMMs, with an absolute sentence accuracy improvement of 5.8% and 9.2% (or relative error reduction of 16.0% and 23.2%) over the CD-GMM-HMMs trained using the minimum phone error rate (MPE) and maximum-likelihood (ML) criteria, respectively.",
  "date": " 2011 ",
  "authors": [
    "G. E. Dahl",
    "Dong Yu",
    "Li Deng",
    "A. Acero"
  ],
  "references": [
    "2136922672",
    "2100495367",
    "2072128103",
    "1533861849",
    "2116064496",
    "2117130368",
    "2546302380",
    "2025768430",
    "1993882792",
    "1498436455"
  ]
}, {
  "id": "2156387975",
  "title": "Deep Sparse Rectifier Neural Networks | Paper | Microsoft Academic",
  "abstract": "While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-dierentiabil ity",
  "date": " 2011 ",
  "authors": [
    "Xavier Glorot",
    "Antoine Bordes",
    "Yoshua Bengio"
  ],
  "references": [
    "2136922672",
    "3118608800",
    "2310919327",
    "1665214252",
    "2129131372",
    "2072128103",
    "1533861849",
    "2097726431",
    "2546302380",
    "2025768430"
  ]
}, {
  "id": "2963504252",
  "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks | Paper | Microsoft Academic",
  "abstract": "Abstract: Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.",
  "date": " 2013 ",
  "authors": [
    "Andrew M. Saxe",
    "James L. McClelland",
    "Surya Ganguli"
  ],
  "references": [
    "2618530766",
    "2100495367",
    "2072128103",
    "1533861849",
    "2117130368",
    "104184427",
    "2110798204",
    "1993882792",
    "2141125852",
    "1815076433"
  ]
}, {
  "id": "2147880316",
  "title": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data | Paper | Microsoft Academic",
  "abstract": "We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data.",
  "date": " 2001 ",
  "authors": [
    "John D. Lafferty",
    "Andrew McCallum",
    "Fernando C. N. Pereira"
  ],
  "references": [
    "2310919327",
    "1988790447",
    "1574901103",
    "2009570821",
    "2096175520",
    "1934019294",
    "1773803948",
    "2160842254",
    "2117400858",
    "3021452258"
  ]
}, {
  "id": "2156163116",
  "title": "Best practices for convolutional neural networks applied to visual document analysis | Paper | Microsoft Academic",
  "abstract": "Neural networks are a powerful technology forclassification of visual inputs arising from documents.However, there is a confusing plethora of different neuralnetwork methods that are used in the literature and inindustry. This paper describes a set of concrete bestpractices that document analysis researchers can use toget good results with neural networks. The mostimportant practice is getting a training set as large aspossible: we expand the training set by adding a newform of distorted data. The next most important practiceis that convolutional neural networks are better suited forvisual document tasks than fully connected networks. Wepropose that a simple \"do-it-yourself\" implementation ofconvolution with a flexible architecture is suitable formany visual document problems. This simpleconvolutional neural network does not require complexmethods, such as momentum, weight decay, structure-dependentlearning rates, averaging layers, tangent prop,or even finely-tuning the architecture. The end result is avery simple yet general architecture which can yieldstate-of-the-art performance for document analysis. Weillustrate our claims on the MNIST set of English digitimages.",
  "date": " 2003 ",
  "authors": [
    "P.Y. Simard",
    "D. Steinkraus",
    "J.C. Platt"
  ],
  "references": [
    "2310919327",
    "1554663460",
    "2159737176",
    "2027197837",
    "2068017609",
    "2147345686",
    "51975515",
    "2166469100"
  ]
}, {
  "id": "2963399829",
  "title": "mixup: Beyond Empirical Risk Minimization | Paper | Microsoft Academic",
  "abstract": "",
  "date": " 2017 ",
  "authors": [
    "Hongyi Zhang",
    "Moustapha Cisse",
    "Yann N. Dauphin",
    "David Lopez-Paz"
  ],
  "references": [
    "2964274690",
    "2966415767",
    "2963855133",
    "2970902013",
    "2987875759",
    "2971149989",
    "3098350627",
    "3034351824",
    "2963681653"
  ]
}, {
  "id": "2964311892",
  "title": "Spectral Networks and Locally Connected Networks on Graphs | Paper | Microsoft Academic",
  "abstract": "Abstract: Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.",
  "date": " 2013 ",
  "authors": [
    "Joan Bruna",
    "Wojciech Zaremba",
    "Arthur Szlam",
    "Yann LeCun"
  ],
  "references": [
    "2618530766",
    "1554944419",
    "2310919327",
    "2160815625",
    "2132914434",
    "1578099820",
    "2156718197",
    "2962820688",
    "1999192586",
    "2147860648"
  ]
}, {
  "id": "2157364932",
  "title": "Learning a similarity metric discriminatively, with application to face verification | Paper | Microsoft Academic",
  "abstract": "We present a method for training a similarity metric from data. The method can be used for recognition or verification applications where the number of categories is very large and not known during training, and where the number of training samples for a single category is very small. The idea is to learn a function that maps input patterns into a target space such that the L/sub 1/ norm in the target space approximates the \"semantic\" distance in the input space. The method is applied to a face verification task. The learning process minimizes a discriminative loss function that drives the similarity metric to be small for pairs of faces from the same person, and large for pairs from different persons. The mapping from raw to the target space is a convolutional network whose architecture is designed for robustness to geometric distortions. The system is tested on the Purdue/AR face database which has a very high degree of variability in the pose, lighting, expression, position, and artificial occlusions such as dark glasses and obscuring scarves.",
  "date": " 2005 ",
  "authors": [
    "S. Chopra",
    "R. Hadsell",
    "Y. LeCun"
  ],
  "references": [
    "2310919327",
    "2053186076",
    "2138451337",
    "2121647436",
    "2994340921",
    "2095757522",
    "2144354855",
    "2107369107",
    "1802356529",
    "10021998"
  ]
}, {
  "id": "1510526001",
  "title": "Probability Estimates for Multi-class Classification by Pairwise Coupling | Paper | Microsoft Academic",
  "abstract": "Pairwise coupling is a popular multi-class classification method that combines all comparisons for each pair of classes. This paper presents two approaches for obtaining class probabilities. Both methods can be reduced to linear systems and are easy to implement. We show conceptually and experimentally that the proposed approaches are more stable than the two existing popular methods: voting and the method by Hastie and Tibshirani (1998)",
  "date": " 2004 ",
  "authors": [
    "Ting-Fan Wu",
    "Chih-Jen Lin",
    "Ruby C. Weng"
  ],
  "references": [
    "2153635508",
    "2911964244",
    "2310919327",
    "2119821739",
    "2084812512",
    "2087347434",
    "1618905105",
    "2101276256",
    "2019575783",
    "1988195734"
  ]
}, {
  "id": "1944615693",
  "title": "Action recognition with trajectory-pooled deep-convolutional descriptors | Paper | Microsoft Academic",
  "abstract": "Visual features are of vital importance for human action understanding in videos. This paper presents a new video representation, called trajectory-pooled deep-convolutional descriptor (TDD), which shares the merits of both hand-crafted features [31] and deep-learned features [24]. Specifically, we utilize deep architectures to learn discriminative convolutional feature maps, and conduct trajectory-constrained pooling to aggregate these convolutional features into effective descriptors. To enhance the robustness of TDDs, we design two normalization methods to transform convolutional feature maps, namely spatiotemporal normalization and channel normalization. The advantages of our features come from (i) TDDs are automatically learned and contain high discriminative capacity compared with those hand-crafted features; (ii) TDDs take account of the intrinsic characteristics of temporal dimension and introduce the strategies of trajectory-constrained sampling and pooling for aggregating deep-learned features. We conduct experiments on two challenging datasets: HMD-B51 and UCF101. Experimental results show that TDDs outperform previous hand-crafted features [31] and deep-learned features [24]. Our method also achieves superior performance to the state of the art on these datasets.",
  "date": " 2015 ",
  "authors": [
    "Limin Wang",
    "Yu Qiao",
    "Xiaoou Tang"
  ],
  "references": [
    "2618530766",
    "2962835968",
    "2151103935",
    "2097117768",
    "2161969291",
    "2108598243",
    "2155893237",
    "1849277567",
    "2310919327",
    "1677409904"
  ]
}, {
  "id": "2158778629",
  "title": "Toward automatic phenotyping of developing embryos from videos | Paper | Microsoft Academic",
  "abstract": "We describe a trainable system for analyzing videos of developing C. elegans embryos. The system automatically detects, segments, and locates cells and nuclei in microscopic images. The system was designed as the central component of a fully automated phenotyping system. The system contains three modules 1) a convolutional network trained to classify each pixel into five categories: cell wall, cytoplasm, nucleus membrane, nucleus, outside medium; 2) an energy-based model, which cleans up the output of the convolutional network by learning local consistency constraints that must be satisfied by label images; 3) a set of elastic models of the embryo at various stages of development that are matched to the label images.",
  "date": " 2005 ",
  "authors": [
    "Feng Ning",
    "D. Delhomme",
    "Y. LeCun",
    "F. Piano",
    "L. Bottou",
    "P.E. Barbano"
  ],
  "references": [
    "2164598857",
    "2310919327",
    "2147880316",
    "2104095591",
    "1647075334",
    "2134557905",
    "2121927366",
    "2119823327",
    "1991848143",
    "2157364932"
  ]
}, {
  "id": "2107878631",
  "title": "Learning long-term dependencies with gradient descent is difficult | Paper | Microsoft Academic",
  "abstract": "Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered. >",
  "date": " 1994 ",
  "authors": [
    "Y. Bengio",
    "P. Simard",
    "P. Frasconi"
  ],
  "references": [
    "2581275558",
    "2154642048",
    "2016589492",
    "2128499899",
    "19621276",
    "2088978850",
    "2148099973",
    "1996741810",
    "2125329357",
    "1527772862"
  ]
}, {
  "id": "2128499899",
  "title": "Induction of Multiscale Temporal Structure | Paper | Microsoft Academic",
  "abstract": "Learning structure in temporally-extended sequences is a difficult computational problem because only a fraction of the relevant information is available at any instant. Although variants of back propagation can in principle be used to find structure in sequences, in practice they are not sufficiently powerful to discover arbitrary contingencies, especially those spanning long temporal intervals or involving high order statistics. For example, in designing a connectionist network for music composition, we have encountered the problem that the net is able to learn musical structure that occurs locally in time--e.g., relations among notes within a musical phrase--but not structure that occurs over longer time periods--e.g., relations among phrases. To address this problem, we require a means of constructing a reduced description of the sequence that makes global aspects more explicit or more readily detectable. I propose to achieve this using hidden units that operate with different time constants. Simulation experiments indicate that slower time-scale hidden units are able to pick up global structure, structure that simply can not be learned by standard back propagation.",
  "date": " 1991 ",
  "authors": [
    "Michael C Mozer"
  ],
  "references": [
    "2154642048",
    "2016589492",
    "2007431958",
    "2143503258",
    "1959983357",
    "2028629011",
    "2053127376",
    "2167607759"
  ]
}, {
  "id": "2007431958",
  "title": "Generalization of back-propagation to recurrent neural networks. | Paper | Microsoft Academic",
  "abstract": "An adaptive neural network with asymmetric connections is introduced. This network is related to the Hopfield network with graded neurons and uses a recurrent generalization of the \\ensuremath{\\delta} rule of Rumelhart, Hinton, and Williams to modify adaptively the synaptic weights. The new network bears a resemblance to the master/slave network of Lapedes and Farber but it is architecturally simpler.",
  "date": " 1987 ",
  "authors": [
    "Fernando J. Pineda"
  ],
  "references": [
    "1652505363",
    "2177721432",
    "2075510082"
  ]
}, {
  "id": "2123716044",
  "title": "Neurocontrol of nonlinear dynamical systems with Kalman filter trained recurrent networks | Paper | Microsoft Academic",
  "abstract": "Although the potential of the powerful mapping and representational capabilities of recurrent network architectures is generally recognized by the neural network research community, recurrent neural networks have not been widely used for the control of nonlinear dynamical systems, possibly due to the relative ineffectiveness of simple gradient descent training algorithms. Developments in the use of parameter-based extended Kalman filter algorithms for training recurrent networks may provide a mechanism by which these architectures will prove to be of practical value. This paper presents a decoupled extended Kalman filter (DEKF) algorithm for training of recurrent networks with special emphasis on application to control problems. We demonstrate in simulation the application of the DEKF algorithm to a series of example control problems ranging from the well-known cart-pole and bioreactor benchmark problems to an automotive subsystem, engine idle speed control. These simulations suggest that recurrent controller networks trained by Kalman filter methods can combine the traditional features of state-space controllers and observers in a homogeneous architecture for nonlinear dynamical systems, while simultaneously exhibiting less sensitivity than do purely feedforward controller networks to changes in plant parameters and measurement noise. >",
  "date": " 1994 ",
  "authors": [
    "G.V. Puskorius",
    "L.A. Feldkamp"
  ],
  "references": [
    "2138484437",
    "2016589492",
    "2150355110",
    "1583833196",
    "2143787696",
    "2057653135",
    "2132152975",
    "2112462566",
    "1529008516",
    "2090248140"
  ]
}, {
  "id": "2143503258",
  "title": "Learning state space trajectories in recurrent neural networks | Paper | Microsoft Academic",
  "abstract": "Many neural network learning procedures compute gradients of the errors on the output layer of units after they have settled to their final values. We describe a procedure for finding E/wij, where E is an error functional of the temporal trajectory of the states of a continuous recurrent network and wij are the weights of that network. Computing these quantities allows one to perform gradient descent in the weights to minimize E. Simulations in which networks are taught to move through limit cycles are shown. This type of recurrent network seems particularly suited for temporally continuous domains, such as signal processing, control, and speech.",
  "date": " 1989 ",
  "authors": [
    "Barak A. Pearlmutter"
  ],
  "references": [
    "2016589492",
    "2007431958",
    "1959983357",
    "1971129545",
    "2028629011",
    "1996647346"
  ]
}, {
  "id": "2154890045",
  "title": "Gradient calculations for dynamic recurrent neural networks: a survey | Paper | Microsoft Academic",
  "abstract": "Surveys learning algorithms for recurrent neural networks with hidden units and puts the various techniques into a common framework. The authors discuss fixed point learning algorithms, namely recurrent backpropagation and deterministic Boltzmann machines, and nonfixed point algorithms, namely backpropagation through time, Elman's history cutoff, and Jordan's output feedback architecture. Forward propagation, an on-line technique that uses adjoint equations, and variations thereof, are also discussed. In many cases, the unified presentation leads to generalizations of various sorts. The author discusses advantages and disadvantages of temporally continuous neural networks in contrast to clocked ones continues with some \"tricks of the trade\" for training, using, and simulating continuous time and recurrent neural networks. The author presents some simulations, and at the end, addresses issues of computational complexity and learning speed. >",
  "date": " 1995 ",
  "authors": [
    "B.A. Pearlmutter"
  ],
  "references": [
    "2581275558",
    "2154642048",
    "2138484437",
    "2110485445",
    "2895674046",
    "2147800946",
    "1597286183",
    "1535810436",
    "2173629880",
    "2016589492"
  ]
}, {
  "id": "194249466",
  "title": "Untersuchungen zu dynamischen neuronalen Netzen | Paper | Microsoft Academic",
  "abstract": "",
  "date": " 1990 ",
  "authors": [
    "Sepp Hochreiter"
  ],
  "references": [
    "2194775991",
    "2964308564",
    "2130942839",
    "2076063813",
    "2064675550",
    "2072128103",
    "1924770834",
    "1026270304"
  ]
}, {
  "id": "2048060899",
  "title": "A time-delay neural network architecture for isolated word recognition | Paper | Microsoft Academic",
  "abstract": "Abstract   A translation-invariant back-propagation network is described that performs better than a sophisticated continuous acoustic parameter hidden Markov model on a noisy, 100-speaker confusable vocabulary isolated word recognition task. The network's replicated architecture permits it to extract precise information from unaligned training patterns selected by a naive segmentation rule.",
  "date": " 1989 ",
  "authors": [
    "Kevin J. Lang",
    "Alex H. Waibel",
    "Geoffrey E. Hinton"
  ],
  "references": [
    "1498436455",
    "3017143921",
    "2173629880",
    "1966812932",
    "2176028050",
    "2101926813",
    "1959983357",
    "1991133427",
    "1573503290",
    "2048330959"
  ]
}, {
  "id": "2103452139",
  "title": "Learning long-term dependencies in NARX recurrent neural networks | Paper | Microsoft Academic",
  "abstract": "It has previously been shown that gradient-descent learning algorithms for recurrent neural networks can perform poorly on tasks that involve long-term dependencies, i.e. those problems for which the desired output depends on inputs presented at times far in the past. We show that the long-term dependencies problem is lessened for a class of architectures called nonlinear autoregressive models with exogenous (NARX) recurrent neural networks, which have powerful representational capabilities. We have previously reported that gradient descent learning can be more effective in NARX networks than in recurrent neural network architectures that have \"hidden states\" on problems including grammatical inference and nonlinear system identification. Typically, the network converges much faster and generalizes better than other networks. The results in this paper are consistent with this phenomenon. We present some experimental results which show that NARX networks can often retain information for two to three times as long as conventional recurrent neural networks. We show that although NARX networks do not circumvent the problem of long-term dependencies, they can greatly improve performance on long-term dependency problems. We also describe in detail some of the assumptions regarding what it means to latch information robustly and suggest possible ways to loosen these assumptions.",
  "date": " 1996 ",
  "authors": [
    "Tsungnan Lin",
    "B.G. Horne",
    "P. Tino",
    "C.L. Giles"
  ],
  "references": [
    "2064675550",
    "2154642048",
    "2138484437",
    "2110485445",
    "2107878631",
    "2798813531",
    "2128499899",
    "2123716044",
    "1674799117",
    "2098398123"
  ]
}, {
  "id": "1674799117",
  "title": "Gradient-based learning algorithms for recurrent networks and their computational complexity | Paper | Microsoft Academic",
  "abstract": "",
  "date": " 1994 ",
  "authors": [
    "Ronald J. Williams",
    "David Zipser"
  ],
  "references": [
    "2064675550",
    "1810943226",
    "2144499799",
    "2136848157",
    "1828163288",
    "1735317348",
    "2079735306",
    "2147568880",
    "3099873379"
  ]
}, {
  "id": "2136922672",
  "title": "A fast learning algorithm for deep belief nets | Paper | Microsoft Academic",
  "abstract": "We show how to use \"complementary priors\" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.",
  "date": " 2006 ",
  "authors": [
    "Geoffrey E. Hinton",
    "Simon Osindero",
    "Yee-Whye Teh"
  ],
  "references": [
    "2310919327",
    "2116064496",
    "2057175746",
    "2159080219",
    "2156163116",
    "2131686571",
    "2567948266",
    "2158778629",
    "2159737176",
    "2124914669"
  ]
}, {
  "id": "3118608800",
  "title": "Learning Multiple Layers of Features from Tiny Images | Paper | Microsoft Academic",
  "abstract": "In this work we describe how to train a multi-layer generative model of natural images. We use a dataset of millions of tiny colour images, described in the next section. This has been attempted by several groups but without success. The models on which we focus are RBMs (Restricted Boltzmann Machines) and DBNs (Deep Belief Networks). These models learn interesting-looking filters, which we show are more useful to a classifier than the raw pixels. We train the classifier on a labeled subset that we have collected and call the CIFAR-10 dataset.",
  "date": " 2008 ",
  "authors": [
    "Alex Krizhevsky"
  ],
  "references": [
    "2081580037",
    "2096192494",
    "2165225968"
  ]
}, {
  "id": "2072128103",
  "title": "Learning Deep Architectures for AI | Paper | Microsoft Academic",
  "abstract": "Can machine learning deliver AI? Theoretical results, inspiration from the brain and cognition, as well as machine learning experiments suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one would need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers, graphical models with many levels of latent variables, or in complicated propositional formulae re-using many sub-formulae. Each level of the architecture represents features at a different level of abstraction, defined as a composition of lower-level features. Searching the parameter space of deep architectures is a difficult task, but new algorithms have been discovered and a new sub-area has emerged in the machine learning community since 2006, following these discoveries. Learning algorithms such as those for Deep Belief Networks and other related unsupervised learning algorithms have recently been proposed to train deep architectures, yielding exciting results and beating the state-of-the-art in certain areas. Learning Deep Architectures for AI discusses the motivations for and principles of learning algorithms for deep architectures. By analyzing and comparing recent results with different learning algorithms for deep architectures, explanations for their success are proposed and discussed, highlighting challenges and suggesting avenues for future explorations in this area.",
  "date": " 2008 ",
  "authors": [
    "Yoshua Bengio"
  ],
  "references": [
    "2156909104",
    "2911964244",
    "2296616510",
    "2136922672",
    "2100495367",
    "2310919327",
    "2187089797",
    "2129131372",
    "2119821739",
    "2053186076"
  ]
}, {
  "id": "2117130368",
  "title": "A unified architecture for natural language processing: deep neural networks with multitask learning | Paper | Microsoft Academic",
  "abstract": "We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance.",
  "date": " 2008 ",
  "authors": [
    "Ronan Collobert",
    "Jason Weston"
  ],
  "references": [
    "2310919327",
    "2132339004",
    "2158847908",
    "2130903752",
    "2107008379",
    "2914746235",
    "2173629880",
    "2885050925",
    "2158823144",
    "2163568299"
  ]
}, {
  "id": "2025768430",
  "title": "Extracting and composing robust features with denoising autoencoders | Paper | Microsoft Academic",
  "abstract": "Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.",
  "date": " 2008 ",
  "authors": [
    "Pascal Vincent",
    "Hugo Larochelle",
    "Yoshua Bengio",
    "Pierre-Antoine Manzagol"
  ],
  "references": [
    "2136922672",
    "2100495367",
    "2072128103",
    "2110798204",
    "2153663612",
    "1652505363",
    "1498436455",
    "1994197834",
    "2293063825",
    "2172174689"
  ]
}, {
  "id": "2110798204",
  "title": "Greedy Layer-Wise Training of Deep Networks | Paper | Microsoft Academic",
  "abstract": "Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization.",
  "date": " 2006 ",
  "authors": [
    "Yoshua Bengio",
    "Pascal Lamblin",
    "Dan Popovici",
    "Hugo Larochelle"
  ],
  "references": [
    "2136922672",
    "2100495367",
    "2116064496",
    "2613634265",
    "2124914669",
    "1993845689",
    "2109779438",
    "2103626435",
    "2125569215",
    "2130313186"
  ]
}, {
  "id": "1498436455",
  "title": "Learning representations by back-propagating errors | Paper | Microsoft Academic",
  "abstract": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.",
  "date": " 1987 ",
  "authors": [
    "David E. Rumelhart",
    "Geoffrey E. Hinton",
    "Ronald J. Williams"
  ],
  "references": [
    "1652505363",
    "2322002063"
  ]
}, {
  "id": "1994197834",
  "title": "An empirical evaluation of deep architectures on problems with many factors of variation | Paper | Microsoft Academic",
  "abstract": "Recently, several learning algorithms relying on models with deep architectures have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment, for which many machine learning algorithms already report reasonable results. Here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established algorithms such as Support Vector Machines and single hidden-layer feed-forward neural networks.",
  "date": " 2007 ",
  "authors": [
    "Hugo Larochelle",
    "Dumitru Erhan",
    "Aaron Courville",
    "James Bergstra",
    "Yoshua Bengio"
  ],
  "references": [
    "2153635508",
    "2136922672",
    "2100495367",
    "2116064496",
    "2110798204",
    "2134557905",
    "2147800946",
    "2613634265",
    "2159737176",
    "2124914669"
  ]
}, {
  "id": "2131462252",
  "title": "A Scalable Hierarchical Distributed Language Model | Paper | Microsoft Academic",
  "abstract": "Neural probabilistic language models (NPLMs) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words, which was two orders of magnitude faster than the non-hierarchical model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical neural models as well as the best n-gram models.",
  "date": " 2008 ",
  "authors": [
    "Andriy Mnih",
    "Geoffrey E. Hinton"
  ],
  "references": [
    "2038721957",
    "2132339004",
    "36903255",
    "2158195707",
    "2121227244",
    "2091812280",
    "2127314673",
    "2111305191",
    "2056590938",
    "1558797106"
  ]
}, {
  "id": "2903899730",
  "title": "Origin and evolution of pathogenic coronaviruses | Paper | Microsoft Academic",
  "abstract": "Severe acute respiratory syndrome coronavirus (SARS-CoV) and Middle East respiratory syndrome coronavirus (MERS-CoV) are two highly transmissible and pathogenic viruses that emerged in humans at the beginning of the 21st century. Both viruses likely originated in bats, and genetically diverse coronaviruses that are related to SARS-CoV and MERS-CoV were discovered in bats worldwide. In this Review, we summarize the current knowledge on the origin and evolution of these two pathogenic coronaviruses and discuss their receptor usage; we also highlight the diversity and potential of spillover of bat-borne coronaviruses, as evidenced by the recent spillover of swine acute diarrhoea syndrome coronavirus (SADS-CoV) to pigs. Coronaviruses have a broad host range and distribution, and some highly pathogenic lineages have spilled over to humans and animals. Here, Cui, Li and Shi explore the viral factors that enabled the emergence of diseases such as severe acute respiratory syndrome and Middle East respiratory syndrome.",
  "date": " 2019 ",
  "authors": [
    "Jie Cui",
    "Fang Li",
    "Zheng Li Shi"
  ],
  "references": [
    "2144081223",
    "2166867592",
    "2111211467",
    "2470646526",
    "2132260239",
    "2104548316",
    "2306794997",
    "1993577573",
    "2775086803",
    "2119111857"
  ]
}, {
  "id": "2166867592",
  "title": "Isolation of a Novel Coronavirus from a Man with Pneumonia in Saudi Arabia | Paper | Microsoft Academic",
  "abstract": "A previously unknown coronavirus was isolated from the sputum of a 60-year-old man who presented with acute pneumonia and subsequent renal failure with a fatal outcome in Saudi Arabia. The virus (called HCoV-EMC) replicated readily in cell culture, producing cytopathic effects of rounding, detachment, and syncytium formation. The virus represents a novel betacoronavirus species. The closest known relatives are bat coronaviruses HKU4 and HKU5. Here, the clinical data, virus isolation, and molecular identification are presented. The clinical picture was remarkably similar to that of the severe acute respiratory syndrome (SARS) outbreak in 2003 and reminds us that animal coronaviruses can cause severe disease in humans.",
  "date": " 2012 ",
  "authors": [
    "Ali Moh Zaki",
    "Sander Van Boheemen",
    "Theo M. Bestebroer",
    "Albert D.M.E. Osterhaus",
    "Ron A.M. Fouchier"
  ],
  "references": [
    "2132260239",
    "2025170735",
    "2129542667",
    "1703839189",
    "2116586125",
    "1987783718",
    "1963953102",
    "2111412754",
    "2170933940",
    "2181908191"
  ]
}, {
  "id": "3000413850",
  "title": "Comparative therapeutic efficacy of remdesivir and combination lopinavir, ritonavir, and interferon beta against MERS-CoV. | Paper | Microsoft Academic",
  "abstract": "Middle East respiratory syndrome coronavirus (MERS-CoV) is the causative agent of a severe respiratory disease associated with more than 2468 human infections and over 851 deaths in 27 countries since 2012. There are no approved treatments for MERS-CoV infection although a combination of lopinavir, ritonavir and interferon beta (LPV/RTV-IFNb) is currently being evaluated in humans in the Kingdom of Saudi Arabia. Here, we show that remdesivir (RDV) and IFNb have superior antiviral activity to LPV and RTV in vitro. In mice, both prophylactic and therapeutic RDV improve pulmonary function and reduce lung viral loads and severe lung pathology. In contrast, prophylactic LPV/RTV-IFNb slightly reduces viral loads without impacting other disease parameters. Therapeutic LPV/RTV-IFNb improves pulmonary function but does not reduce virus replication or severe lung pathology. Thus, we provide in vivo evidence of the potential for RDV to treat MERS-CoV infections.",
  "date": " 2020 ",
  "authors": [
    "Timothy P. Sheahan",
    "Amy C. Sims",
    "Sarah R. Leist",
    "Alexandra Sch\u00e4fer",
    "John Won",
    "Ariane J. Brown",
    "Stephanie A. Montgomery",
    "Alison Hogg",
    "Darius Babusis",
    "Michael O. Clarke",
    "Jamie E. Spahn",
    "Laura Bauer",
    "Scott Sellers",
    "Danielle Porter",
    "Joy Y. Feng",
    "Tomas Cihlar",
    "Robert Jordan",
    "Mark R. Denison",
    "Ralph S. Baric"
  ],
  "references": [
    "2107277218",
    "2470646526",
    "2725497285",
    "1993577573",
    "2565805236",
    "2791599184",
    "2255243349",
    "2292021561",
    "2290466312",
    "2034462612"
  ]
}, {
  "id": "2026274122",
  "title": "KDIGO clinical practice guidelines for acute kidney injury. | Paper | Microsoft Academic",
  "abstract": "tion\u2019, implying that most patients \u2018should\u2019 receive a particular action. In contrast, level 2 guidelines are essentially \u2018suggestions\u2019 and are deemed to be \u2018weak\u2019 or discretionary, recognising that management decisions may vary in different clinical contexts. Each recommendation was further graded from A to D by the quality of evidence underpinning them, with grade A referring to a high quality of evidence whilst grade D recognised a \u2018very low\u2019 evidence base. The overall strength and quality of the supporting evidence is summarised in table 1 . The guidelines focused on 4 key domains: (1) AKI definition, (2) prevention and treatment of AKI, (3) contrastinduced AKI (CI-AKI) and (4) dialysis interventions for the treatment of AKI. The full summary of clinical practice statements is available at www.kdigo.org, but a few key recommendation statements will be highlighted here.",
  "date": " 2012 ",
  "authors": [
    "Arif Khwaja"
  ],
  "references": [
    "1967300023",
    "2131419242",
    "2143432233",
    "2117958746",
    "1531106656",
    "2157775267",
    "2028701043",
    "2111704803",
    "2135163018",
    "2042074736"
  ]
}, {
  "id": "2132260239",
  "title": "Identification of a novel coronavirus in patients with severe acute respiratory syndrome. | Paper | Microsoft Academic",
  "abstract": "BACKGROUND: The severe acute respiratory syndrome (SARS) has recently been identified as a new clinical entity. SARS is thought to be caused by an unknown infectious agent. METHODS: Clinical specimens from patients with SARS were searched for unknown viruses with the use of cell cultures and molecular techniques. RESULTS: A novel coronavirus was identified in patients with SARS. The virus was isolated in cell culture, and a sequence 300 nucleotides in length was obtained by a polymerase-chain-reaction (PCR)-based random-amplification procedure. Genetic characterization indicated that the virus is only distantly related to known coronaviruses (identical in 50 to 60 percent of the nucleotide sequence). On the basis of the obtained sequence, conventional and real-time PCR assays for specific and sensitive detection of the novel virus were established. Virus was detected in a variety of clinical specimens from patients with SARS but not in controls. High concentrations of viral RNA of up to 100 million molecules per milliliter were found in sputum. Viral RNA was also detected at extremely low concentrations in plasma during the acute phase and in feces during the late convalescent phase. Infected patients showed seroconversion on the Vero cells in which the virus was isolated. CONCLUSIONS: The novel coronavirus might have a role in causing SARS.",
  "date": " 2003 ",
  "authors": [
    "Christian Drosten",
    "Stephan G\u00fcnther",
    "Wolfgang Preiser",
    "Sylvie van der Werf",
    "Hans-Reinhard Brodt",
    "Stephan Becker",
    "Holger Rabenau",
    "Marcus Panning",
    "Larissa Kolesnikova",
    "Ron A.M. Fouchier",
    "Annemarie Berger",
    "Ana-Maria Burgui\u00e8re",
    "Jindrich Cinatl",
    "Markus Eickmann",
    "Nicolas Escriou",
    "Klaus Grywna",
    "Stefanie Kramme",
    "Jean-Claude Manuguerra",
    "Stefanie M\u00fcller",
    "Volker Rickerts",
    "Martin St\u00fcrmer",
    "Simon Vieth",
    "Hans-Dieter Klenk",
    "Albert D.M.E. Osterhaus",
    "Herbert Schmitz",
    "Hans Wilhelm Doerr"
  ],
  "references": [
    "2100820722",
    "2125251240",
    "2107922358",
    "2127062009",
    "2084994773",
    "2149579937",
    "2090060897",
    "2004869546",
    "2030133843"
  ]
}, {
  "id": "2104548316",
  "title": "A novel coronavirus associated with severe acute respiratory syndrome. | Paper | Microsoft Academic",
  "abstract": "background A worldwide outbreak of severe acute respiratory syndrome (SARS) has been associated with exposures originating from a single ill health care worker from Guangdong Province, China. We conducted studies to identify the etiologic agent of this outbreak. methods We received clinical specimens from patients in six countries and tested them, using virus isolation techniques, electron-microscopical and histologic studies, and molecular and serologic assays, in an attempt to identify a wide range of potential pathogens. results No classic respiratory or bacterial respiratory pathogen was consistently identified. However, a novel coronavirus was isolated from patients who met the case definition of SARS. Cytopathological features were noted microscopically in Vero E6 cells inoculated with a throat-swab specimen. Electron-microscopical examination of cultures revealed ultrastructural features characteristic of coronaviruses. Immunohistochemical and immunofluorescence staining revealed reactivity with group I coronavirus polyclonal antibodies. Consensus coronavirus primers designed to amplify a fragment of the polymerase gene by reverse transcription\u2013polymerase chain reaction (RT-PCR) were used to obtain a sequence that clearly identified the isolate as a unique coronavirus only distantly related to previously sequenced coronaviruses. With specific diagnostic RT-PCR primers we identified several identical nucleotide sequences in 12 patients from several locations, a finding consistent with a point source outbreak. Indirect fluorescent antibody tests and enzyme-linked immunosorbent assays made with the new coronavirus isolate have been used to demonstrate a virus-specific serologic response. Preliminary studies suggest that this virus may never before have infected the U.S. population. conclusions A novel coronavirus is associated with this outbreak, and the evidence indicates that this virus has an etiologic role in SARS. The name Urbani SARS-associated coronavirus is proposed for the virus.",
  "date": " 2003 ",
  "authors": [
    "Ksiazek Tg",
    "Erdman D",
    "Goldsmith Cs",
    "Zaki",
    "Peret T",
    "Emery S",
    "Tong S",
    "Urbani C",
    "Comer Ja",
    "Lim W",
    "Rollin Pe",
    "Dowell Sf",
    "Ling Ae",
    "Humphrey Cd",
    "Shieh Wj",
    "Guarner J",
    "Paddock Cd",
    "Rota P",
    "Fields B",
    "DeRisi J",
    "Yang Jy",
    "Cox N",
    "Hughes Jm",
    "LeDuc Jw",
    "Bellini Wj",
    "Anderson Lj"
  ],
  "references": [
    "2106882534",
    "2131262274",
    "2100820722",
    "2125251240",
    "2463755683",
    "2403756321",
    "2127949919",
    "1576737979",
    "2128788856",
    "2076620790"
  ]
}, {
  "id": "2131262274",
  "title": "A Major Outbreak of Severe Acute Respiratory Syndrome in Hong Kong | Paper | Microsoft Academic",
  "abstract": "background There has been an outbreak of the severe acute respiratory syndrome (SARS) worldwide. We report the clinical, laboratory, and radiologic features of 138 cases of suspected SARS during a hospital outbreak in Hong Kong. methods From March 11 to 25, 2003, all patients with suspected SARS after exposure to an index patient or ward were admitted to the isolation wards of the Prince of Wales Hospital. Their demographic, clinical, laboratory, and radiologic characteristics were analyzed. Clinical end points included the need for intensive care and death. Univariate and multivariate analyses were performed. results There were 66 male patients and 72 female patients in this cohort, 69 of whom were health care workers. The most common symptoms included fever (in 100 percent of the patients); chills, rigors, or both (73.2 percent); and myalgia (60.9 percent). Cough and headache were also reported in more than 50 percent of the patients. Other common findings were lymphopenia (in 69.6 percent), thrombocytopenia (44.8 percent), and elevated lactate dehydrogenase and creatine kinase levels (71.0 percent and 32.1 percent, respectively). Peripheral air-space consolidation was commonly observed on thoracic computed tomographic scanning. A total of 32 patients (23.2 percent) were admitted to the intensive care unit; 5 patients died, all of whom had coexisting conditions. In a multivariate analysis, the independent predictors of an adverse outcome were advanced age (odds ratio per decade of life, 1.80; 95 percent confidence interval, 1.16 to 2.81; P=0.009), a high peak lactate dehydrogenase level (odds ratio per 100 U per liter, 2.09; 95 percent confidence interval, 1.28 to 3.42; P=0.003), and an absolute neutrophil count that exceeded the upper limit of the normal range on presentation (odds ratio, 1.60; 95 percent confidence interval, 1.03 to 2.50; P=0.04). conclusions SARS is a serious respiratory illness that led to significant morbidity and mortality in our cohort.",
  "date": " 2003 ",
  "authors": [
    "Nelson Lee",
    "David Hui",
    "Alan Wu",
    "Paul Chan",
    "Peter Cameron",
    "Gavin M Joynt",
    "Anil Ahuja",
    "Man Yee Yung",
    "C B Leung",
    "K F To",
    "S F Lui",
    "C C Szeto",
    "Sydney Chung",
    "Joseph J Y Sung"
  ],
  "references": [
    "2123324969",
    "2130141864",
    "2463755683",
    "1991467275",
    "1982444609"
  ]
}, {
  "id": "2006434809",
  "title": "Epidemiological, demographic, and clinical characteristics of 47 cases of Middle East respiratory syndrome coronavirus disease from Saudi Arabia: a descriptive study | Paper | Microsoft Academic",
  "abstract": "Summary  Background  Middle East respiratory syndrome (MERS) is a new human disease caused by a novel coronavirus (CoV). Clinical data on MERS-CoV infections are scarce. We report epidemiological, demographic, clinical, and laboratory characteristics of 47 cases of MERS-CoV infections, identify knowledge gaps, and define research priorities.  Methods  We abstracted and analysed epidemiological, demographic, clinical, and laboratory data from confirmed cases of sporadic, household, community, and health-care-associated MERS-CoV infections reported from Saudi Arabia between Sept 1, 2012, and June 15, 2013. Cases were confirmed as having MERS-CoV by real-time RT-PCR.  Findings  47 individuals (46 adults, one child) with laboratory-confirmed MERS-CoV disease were identified; 36 (77%) were male (male:female ratio 3\u00b73:1). 28 patients died, a 60% case-fatality rate. The case-fatality rate rose with increasing age. Only two of the 47 cases were previously healthy; most patients (45 [96%]) had underlying comorbid medical disorders, including diabetes (32 [68%]), hypertension (16 [34%]), chronic cardiac disease (13 [28%]), and chronic renal disease (23 [49%]). Common symptoms at presentation were fever (46 [98%]), fever with chills or rigors (41 [87%]), cough (39 [83%]), shortness of breath (34 [72%]), and myalgia (15 [32%]). Gastrointestinal symptoms were also frequent, including diarrhoea (12 [26%]), vomiting (ten [21%]), and abdominal pain (eight [17%]). All patients had abnormal findings on chest radiography, ranging from subtle to extensive unilateral and bilateral abnormalities. Laboratory analyses showed raised concentrations of lactate dehydrogenase (23 [49%]) and aspartate aminotransferase (seven [15%]) and thrombocytopenia (17 [36%]) and lymphopenia (16 [34%]).  Interpretation  Disease caused by MERS-CoV presents with a wide range of clinical manifestations and is associated with substantial mortality in admitted patients who have medical comorbidities. Major gaps in our knowledge of the epidemiology, community prevalence, and clinical spectrum of infection and disease need urgent definition.  Funding  None.",
  "date": " 2013 ",
  "authors": [
    "Abdullah Assiri",
    "Jaffar A Al-Tawfiq",
    "Abdullah A Al-Rabeeah",
    "Fahad A Al-Rabiah",
    "Sami Al-Hajjar",
    "Ali Al-Barrak",
    "Hesham Flemban",
    "Wafa N Al-Nassir",
    "Hanan H Balkhy",
    "Rafat F Al-Hakeem",
    "Hatem Q Makhdoom",
    "Alimuddin I Zumla",
    "Ziad A Memish"
  ],
  "references": [
    "2166867592",
    "2107053896",
    "2131262274",
    "1703839189",
    "2112147913",
    "2045002682",
    "1852588318",
    "2163627712",
    "2140143765",
    "2119775949"
  ]
}, {
  "id": "2725497285",
  "title": "Broad-spectrum antiviral GS-5734 inhibits both epidemic and zoonotic coronaviruses. | Paper | Microsoft Academic",
  "abstract": "Emerging viral infections are difficult to control because heterogeneous members periodically cycle in and out of humans and zoonotic hosts, complicating the development of specific antiviral therapies and vaccines. Coronaviruses (CoVs) have a proclivity to spread rapidly into new host species causing severe disease. Severe acute respiratory syndrome CoV (SARS-CoV) and Middle East respiratory syndrome CoV (MERS-CoV) successively emerged, causing severe epidemic respiratory disease in immunologically naive human populations throughout the globe. Broad-spectrum therapies capable of inhibiting CoV infections would address an immediate unmet medical need and could be invaluable in the treatment of emerging and endemic CoV infections. We show that a nucleotide prodrug, GS-5734, currently in clinical development for treatment of Ebola virus disease, can inhibit SARS-CoV and MERS-CoV replication in multiple in vitro systems, including primary human airway epithelial cell cultures with submicromolar IC50 values. GS-5734 was also effective against bat CoVs, prepandemic bat CoVs, and circulating contemporary human CoV in primary human lung cells, thus demonstrating broad-spectrum anti-CoV activity. In a mouse model of SARS-CoV pathogenesis, prophylactic and early therapeutic administration of GS-5734 significantly reduced lung viral load and improved clinical signs of disease as well as respiratory function. These data provide substantive evidence that GS-5734 may prove effective against endemic MERS-CoV in the Middle East, circulating human CoV, and, possibly most importantly, emerging CoV of the future.",
  "date": " 2017 ",
  "authors": [
    "Timothy P. Sheahan",
    "Amy C. Sims",
    "Rachel L. Graham",
    "Vineet D. Menachery",
    "Lisa E. Gralinski",
    "James B. Case",
    "Sarah R. Leist",
    "Krzysztof Pyrc",
    "Joy Y. Feng",
    "Iva Trantcheva",
    "Roy Bannister",
    "Yeojin Park",
    "Darius Babusis",
    "Michael O. Clarke",
    "Richard L. Mackman",
    "Jamie E. Spahn",
    "Christopher A. Palmiotti",
    "Dustin Siegel",
    "Adrian S. Ray",
    "Tomas Cihlar",
    "Robert Jordan",
    "Mark R. Denison",
    "Ralph S. Baric"
  ],
  "references": [
    "2470646526",
    "2129542667",
    "2195009776",
    "2255243349",
    "2292021561",
    "2115555188",
    "2298153446",
    "2525468044",
    "1945961678",
    "2099941783"
  ]
}, {
  "id": "3011483298",
  "title": "SARS-CoV-2 entry factors are highly expressed in nasal epithelial cells together with innate immune genes. | Paper | Microsoft Academic",
  "abstract": "We investigated SARS-CoV-2 potential tropism by surveying expression of viral entry-associated genes in single-cell RNA-sequencing data from multiple tissues from healthy human donors. We co-detected these transcripts in specific respiratory, corneal and intestinal epithelial cells, potentially explaining the high efficiency of SARS-CoV-2 transmission. These genes are co-expressed in nasal epithelial cells with genes involved in innate immunity, highlighting the cells' potential role in initial viral infection, spread and clearance. The study offers a useful resource for further lines of inquiry with valuable clinical samples from COVID-19 patients and we provide our data in a comprehensive, open and user-friendly fashion at www.covid19cellatlas.org.",
  "date": " 2020 ",
  "authors": [
    "Waradon Sungnak",
    "Ni Huang",
    "Christophe B\u00e9cavin",
    "Marijn Berg",
    "Rachel Queen",
    "Monika Litvinukova",
    "Carlos Talavera-L\u00f3pez",
    "Henrike Maatz",
    "Daniel Reichart",
    "Fotios Sampaziotis",
    "Kaylee B. Worlock",
    "Masahiro Yoshida",
    "Josephine L. Barnes"
  ],
  "references": [
    "3001118548",
    "3001897055",
    "3008827533",
    "3003668884",
    "3002108456",
    "3002539152",
    "3004280078",
    "3004239190",
    "3009912996",
    "3007940623"
  ]
}, {
  "id": "1901129140",
  "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation | Paper | Microsoft Academic",
  "abstract": "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .",
  "date": " 2015 ",
  "authors": [
    "Olaf Ronneberger",
    "Philipp Fischer",
    "Thomas Brox"
  ],
  "references": [
    "2618530766",
    "2962835968",
    "1903029394",
    "2155893237",
    "1677182931",
    "1948751323",
    "2167510172",
    "1893585201",
    "2148349024",
    "2147800946"
  ]
}, {
  "id": "3106250896",
  "title": "SSD: Single Shot MultiBox Detector | Paper | Microsoft Academic",
  "abstract": "We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For \\(300 \\times 300\\) input, SSD achieves 74.3 % mAP on VOC2007 test at 59 FPS on a Nvidia Titan X and for \\(512 \\times 512\\) input, SSD achieves 76.9 % mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https://github.com/weiliu89/caffe/tree/ssd.",
  "date": " 2016 ",
  "authors": [
    "Wei Liu",
    "Dragomir Anguelov",
    "Dumitru Erhan",
    "Christian Szegedy",
    "Scott E. Reed",
    "Cheng-Yang Fu",
    "Alexander C. Berg"
  ],
  "references": [
    "2194775991",
    "2618530766",
    "2962835968",
    "2097117768",
    "639708223",
    "1836465849",
    "2102605133",
    "2117539524",
    "1903029394",
    "2155893237"
  ]
}, {
  "id": "3001897055",
  "title": "A Novel Coronavirus from Patients with Pneumonia in China, 2019. | Paper | Microsoft Academic",
  "abstract": "In December 2019, a cluster of patients with pneumonia of unknown cause was linked to a seafood wholesale market in Wuhan, China. A previously unknown betacoronavirus was discovered through the use of unbiased sequencing in samples from patients with pneumonia. Human airway epithelial cells were used to isolate a novel coronavirus, named 2019-nCoV, which formed a clade within the subgenus sarbecovirus, Orthocoronavirinae subfamily. Different from both MERS-CoV and SARS-CoV, 2019-nCoV is the seventh member of the family of coronaviruses that infect humans. Enhanced surveillance and further investigation are ongoing. (Funded by the National Key Research and Development Program of China and the National Major Project for Control and Prevention of Infectious Disease in China.).",
  "date": " 2020 ",
  "authors": [
    "Na Zhu",
    "Dingyu Zhang",
    "Wenling Wang",
    "Xingwang Li",
    "Bo Yang",
    "Jingdong Song",
    "Xiang Zhao",
    "Baoying Huang",
    "Weifeng Shi",
    "Roujian Lu",
    "Peihua Niu",
    "Faxian Zhan",
    "Xuejun Ma",
    "Dayan Wang",
    "Wenbo Xu",
    "Guizhen Wu",
    "George F. Gao",
    "Wenjie Tan"
  ],
  "references": [
    "2903899730",
    "2166867592",
    "2132260239",
    "2104548316",
    "2306794997",
    "1909499787",
    "3027518954",
    "2792024998",
    "2955025503",
    "2257005270"
  ]
}, {
  "id": "3005079553",
  "title": "Clinical Characteristics of 138 Hospitalized Patients With 2019 Novel Coronavirus-Infected Pneumonia in Wuhan, China. | Paper | Microsoft Academic",
  "abstract": "Importance  In December 2019, novel coronavirus (2019-nCoV)\u2013infected pneumonia (NCIP) occurred in Wuhan, China. The number of cases has increased rapidly but information on the clinical characteristics of affected patients is limited.  Objective  To describe the epidemiological and clinical characteristics of NCIP.  Design, Setting, and Participants  Retrospective, single-center case series of the 138 consecutive hospitalized patients with confirmed NCIP at Zhongnan Hospital of Wuhan University in Wuhan, China, from January 1 to January 28, 2020; final date of follow-up was February 3, 2020.  Exposures  Documented NCIP.  Main Outcomes and Measures  Epidemiological, demographic, clinical, laboratory, radiological, and treatment data were collected and analyzed. Outcomes of critically ill patients and noncritically ill patients were compared. Presumed hospital-related transmission was suspected if a cluster of health professionals or hospitalized patients in the same wards became infected and a possible source of infection could be tracked.  Results  Of 138 hospitalized patients with NCIP, the median age was 56 years (interquartile range, 42-68; range, 22-92 years) and 75 (54.3%) were men. Hospital-associated transmission was suspected as the presumed mechanism of infection for affected health professionals (40 [29%]) and hospitalized patients (17 [12.3%]). Common symptoms included fever (136 [98.6%]), fatigue (96 [69.6%]), and dry cough (82 [59.4%]). Lymphopenia (lymphocyte count, 0.8\u2009\u00d7\u2009109/L [interquartile range {IQR}, 0.6-1.1]) occurred in 97 patients (70.3%), prolonged prothrombin time (13.0 seconds [IQR, 12.3-13.7]) in 80 patients (58%), and elevated lactate dehydrogenase (261 U/L [IQR, 182-403]) in 55 patients (39.9%). Chest computed tomographic scans showed bilateral patchy shadows or ground glass opacity in the lungs of all patients. Most patients received antiviral therapy (oseltamivir, 124 [89.9%]), and many received antibacterial therapy (moxifloxacin, 89 [64.4%]; ceftriaxone, 34 [24.6%]; azithromycin, 25 [18.1%]) and glucocorticoid therapy (62 [44.9%]). Thirty-six patients (26.1%) were transferred to the intensive care unit (ICU) because of complications, including acute respiratory distress syndrome (22 [61.1%]), arrhythmia (16 [44.4%]), and shock (11 [30.6%]). The median time from first symptom to dyspnea was 5.0 days, to hospital admission was 7.0 days, and to ARDS was 8.0 days. Patients treated in the ICU (n\u2009=\u200936), compared with patients not treated in the ICU (n\u2009=\u2009102), were older (median age, 66 years vs 51 years), were more likely to have underlying comorbidities (26 [72.2%] vs 38 [37.3%]), and were more likely to have dyspnea (23 [63.9%] vs 20 [19.6%]), and anorexia (24 [66.7%] vs 31 [30.4%]). Of the 36 cases in the ICU, 4 (11.1%) received high-flow oxygen therapy, 15 (41.7%) received noninvasive ventilation, and 17 (47.2%) received invasive ventilation (4 were switched to extracorporeal membrane oxygenation). As of February 3, 47 patients (34.1%) were discharged and 6 died (overall mortality, 4.3%), but the remaining patients are still hospitalized. Among those discharged alive (n\u2009=\u200947), the median hospital stay was 10 days (IQR, 7.0-14.0).  Conclusions and Relevance  In this single-center case series of 138 hospitalized patients with confirmed NCIP in Wuhan, China, presumed hospital-related transmission of 2019-nCoV was suspected in 41% of patients, 26% of patients received ICU care, and mortality was 4.3%.",
  "date": " 2020 ",
  "authors": [
    "Dawei Wang",
    "Bo Hu",
    "Chang Hu",
    "Fangfang Zhu",
    "Xing Liu",
    "Jing Zhang",
    "Binbin Wang",
    "Hui Xiang",
    "Zhenshun Cheng",
    "Yong Xiong",
    "Yan Zhao",
    "Yirong Li",
    "Xinghuan Wang",
    "Zhiyong Peng"
  ],
  "references": [
    "3001118548",
    "3001897055",
    "3003668884",
    "3002108456",
    "3002539152",
    "3000834295",
    "3003951199",
    "2999409984",
    "1803784511",
    "2999318660"
  ]
}, {
  "id": "3003668884",
  "title": "Early Transmission Dynamics in Wuhan, China, of Novel Coronavirus-Infected Pneumonia. | Paper | Microsoft Academic",
  "abstract": "Abstract Background The initial cases of novel coronavirus (2019-nCoV)\u2013infected pneumonia (NCIP) occurred in Wuhan, Hubei Province, China, in December 2019 and January 2020. We analyzed data on the...",
  "date": " 2020 ",
  "authors": [
    "Qun Li",
    "Xuhua Guan",
    "Peng Wu",
    "Xiaoye Wang",
    "Lei Zhou",
    "Yeqing Tong",
    "Ruiqi Ren",
    "Kathy S.M. Leung",
    "Eric H.Y. Lau",
    "Jessica Y. Wong",
    "Xuesen Xing",
    "Nijuan Xiang",
    "Yang Wu",
    "Chao Li",
    "Qi Chen",
    "Dan Li",
    "Tian Liu",
    "Jing Zhao",
    "Man Liu",
    "Wenxiao Tu",
    "Chuding Chen",
    "Lianmei Jin",
    "Rui Yang",
    "Qi Wang",
    "Suhua Zhou",
    "Rui Wang",
    "Hui Liu",
    "Yingbo Luo",
    "Yuan Liu",
    "Ge Shao",
    "Huan Li",
    "Zhongfa Tao",
    "Yang Yang",
    "Zhiqiang Deng",
    "Boxi Liu",
    "Zhitao Ma",
    "Yanping Zhang",
    "Guoqing Shi",
    "Tommy T.Y. Lam",
    "Joseph T. Wu",
    "George F. Gao",
    "Benjamin J. Cowling",
    "Bo Yang",
    "Gabriel M. Leung",
    "Zijian Feng"
  ],
  "references": [
    "3001897055",
    "3002539152",
    "3000834295",
    "3002533507",
    "2470646526",
    "3002715510",
    "1909499787",
    "3001971765",
    "2147166346",
    "2149508011"
  ]
}, {
  "id": "3002108456",
  "title": "Epidemiological and clinical characteristics of 99 cases of 2019 novel coronavirus pneumonia in Wuhan, China: a descriptive study | Paper | Microsoft Academic",
  "abstract": "In December, 2019, a pneumonia associated with the 2019 novel coronavirus (2019-nCoV) emerged in Wuhan, China. We aimed to further clarify the epidemiological and clinical characteristics of 2019-nCoV pneumonia. In this retrospective, single-centre study, we included all confirmed cases of 2019-nCoV in Wuhan Jinyintan Hospital from Jan 1 to Jan 20, 2020. Cases were confirmed by real-time RT-PCR and were analysed for epidemiological, demographic, clinical, and radiological features and laboratory data. Outcomes were followed up until Jan 25, 2020.",
  "date": " 2020 ",
  "authors": [
    "Nanshan Chen",
    "Min Zhou",
    "Xuan Dong",
    "Jieming Qu",
    "Fengyun Gong",
    "Yang Han",
    "Yang Qiu",
    "Jingli Wang",
    "Ying Liu",
    "Yuan Wei",
    "Jia'an Xia",
    "Ting Yu",
    "Xinxin Zhang",
    "Li Zhang"
  ],
  "references": [
    "3001118548",
    "2903899730",
    "2166867592",
    "2999409984",
    "2999318660",
    "2132260239",
    "2999364275",
    "2991899552",
    "2909194930",
    "2775086803"
  ]
}, {
  "id": "3002539152",
  "title": "A familial cluster of pneumonia associated with the 2019 novel coronavirus indicating person-to-person transmission: a study of a family cluster. | Paper | Microsoft Academic",
  "abstract": "Summary  Background  An ongoing outbreak of pneumonia associated with a novel coronavirus was reported in Wuhan city, Hubei province, China. Affected patients were geographically linked with a local wet market as a potential source. No data on person-to-person or nosocomial transmission have been published to date.  Methods  In this study, we report the epidemiological, clinical, laboratory, radiological, and microbiological findings of five patients in a family cluster who presented with unexplained pneumonia after returning to Shenzhen, Guangdong province, China, after a visit to Wuhan, and an additional family member who did not travel to Wuhan. Phylogenetic analysis of genetic sequences from these patients were done.  Findings  From Jan 10, 2020, we enrolled a family of six patients who travelled to Wuhan from Shenzhen between Dec 29, 2019 and Jan 4, 2020. Of six family members who travelled to Wuhan, five were identified as infected with the novel coronavirus. Additionally, one family member, who did not travel to Wuhan, became infected with the virus after several days of contact with four of the family members. None of the family members had contacts with Wuhan markets or animals, although two had visited a Wuhan hospital. Five family members (aged 36\u201366 years) presented with fever, upper or lower respiratory tract symptoms, or diarrhoea, or a combination of these 3\u20136 days after exposure. They presented to our hospital (The University of Hong Kong-Shenzhen Hospital, Shenzhen) 6\u201310 days after symptom onset. They and one asymptomatic child (aged 10 years) had radiological ground-glass lung opacities. Older patients (aged >60 years) had more systemic symptoms, extensive radiological ground-glass lung changes, lymphopenia, thrombocytopenia, and increased C-reactive protein and lactate dehydrogenase levels. The nasopharyngeal or throat swabs of these six patients were negative for known respiratory microbes by point-of-care multiplex RT-PCR, but five patients (four adults and the child) were RT-PCR positive for genes encoding the internal RNA-dependent RNA polymerase and surface Spike protein of this novel coronavirus, which were confirmed by Sanger sequencing. Phylogenetic analysis of these five patients' RT-PCR amplicons and two full genomes by next-generation sequencing showed that this is a novel coronavirus, which is closest to the bat severe acute respiatory syndrome (SARS)-related coronaviruses found in Chinese horseshoe bats.  Interpretation  Our findings are consistent with person-to-person transmission of this novel coronavirus in hospital and family settings, and the reports of infected travellers in other geographical regions.  Funding  The Shaw Foundation Hong Kong, Michael Seak-Kan Tong, Respiratory Viral Research Foundation Limited, Hui Ming, Hui Hoy and Chow Sin Lan Charity Fund Limited, Marina Man-Wai Lee, the Hong Kong Hainan Commercial Association South China Microbiology Research Fund, Sanming Project of Medicine (Shenzhen), and High Level-Hospital Program (Guangdong Health Commission).",
  "date": " 2020 ",
  "authors": [
    "Jasper Fuk Woo Chan",
    "Shuofeng Yuan",
    "Kin Hang Kok",
    "Kelvin Kai Wang To",
    "Hin Chu",
    "Jin Yang",
    "Fanfan Xing",
    "Jieling Liu",
    "Cyril Chik Yan Yip",
    "Rosana Wing Shan Poon",
    "Hoi Wah Tsoi",
    "Simon Kam Fai Lo",
    "Kwok Hung Chan",
    "Vincent Kwok Man Poon",
    "Wan Mui Chan",
    "Jonathan Daniel Ip",
    "Jian Piao Cai",
    "Vincent Chi Chung Cheng",
    "Honglin Chen",
    "Christopher Kim Ming Hui",
    "Kwok Yung Yuen"
  ],
  "references": [
    "2025170735",
    "2129542667",
    "2103503670",
    "2115555188",
    "2105637133",
    "2807736175",
    "2889758689",
    "2769543984",
    "2140338292",
    "2170933940"
  ]
}, {
  "id": "3004318991",
  "title": "Genomic characterisation and epidemiology of 2019 novel coronavirus: implications for virus origins and receptor binding. | Paper | Microsoft Academic",
  "abstract": "Summary  Background  In late December, 2019, patients presenting with viral pneumonia due to an unidentified microbial agent were reported in Wuhan, China. A novel coronavirus was subsequently identified as the causative pathogen, provisionally named 2019 novel coronavirus (2019-nCoV). As of Jan 26, 2020, more than 2000 cases of 2019-nCoV infection have been confirmed, most of which involved people living in or visiting Wuhan, and human-to-human transmission has been confirmed.  Methods  We did next-generation sequencing of samples from bronchoalveolar lavage fluid and cultured isolates from nine inpatients, eight of whom had visited the Huanan seafood market in Wuhan. Complete and partial 2019-nCoV genome sequences were obtained from these individuals. Viral contigs were connected using Sanger sequencing to obtain the full-length genomes, with the terminal regions determined by rapid amplification of cDNA ends. Phylogenetic analysis of these 2019-nCoV genomes and those of other coronaviruses was used to determine the evolutionary history of the virus and help infer its likely origin. Homology modelling was done to explore the likely receptor-binding properties of the virus.  Findings  The ten genome sequences of 2019-nCoV obtained from the nine patients were extremely similar, exhibiting more than 99\u00b798% sequence identity. Notably, 2019-nCoV was closely related (with 88% identity) to two bat-derived severe acute respiratory syndrome (SARS)-like coronaviruses, bat-SL-CoVZC45 and bat-SL-CoVZXC21, collected in 2018 in Zhoushan, eastern China, but were more distant from SARS-CoV (about 79%) and MERS-CoV (about 50%). Phylogenetic analysis revealed that 2019-nCoV fell within the subgenus Sarbecovirus of the genus Betacoronavirus, with a relatively long branch length to its closest relatives bat-SL-CoVZC45 and bat-SL-CoVZXC21, and was genetically distinct from SARS-CoV. Notably, homology modelling revealed that 2019-nCoV had a similar receptor-binding domain structure to that of SARS-CoV, despite amino acid variation at some key residues.  Interpretation  2019-nCoV is sufficiently divergent from SARS-CoV to be considered a new human-infecting betacoronavirus. Although our phylogenetic analysis suggests that bats might be the original host of this virus, an animal sold at the seafood market in Wuhan might represent an intermediate host facilitating the emergence of the virus in humans. Importantly, structural analysis suggests that 2019-nCoV might be able to bind to the angiotensin-converting enzyme 2 receptor in humans. The future evolution, adaptation, and spread of this virus warrant urgent investigation.  Funding  National Key Research and Development Program of China, National Major Project for Control and Prevention of Infectious Disease in China, Chinese Academy of Sciences, Shandong First Medical University.",
  "date": " 2020 ",
  "authors": [
    "Roujian Lu",
    "Xiang Zhao",
    "Juan Li",
    "Peihua Niu",
    "Bo Yang",
    "Honglong Wu",
    "Wenling Wang",
    "Hao Song",
    "Baoying Huang",
    "Na Zhu",
    "Yuhai Bi",
    "Xuejun Ma",
    "Faxian Zhan",
    "Liang Wang",
    "Tao Hu",
    "Hong Zhou",
    "Zhenhong Hu",
    "Weimin Zhou",
    "Li Zhao",
    "Jing Chen",
    "Yao Meng",
    "Ji Wang",
    "Yang Lin",
    "Jianying Yuan",
    "Zhihao Xie",
    "Jinmin Ma",
    "William J Liu",
    "Dayan Wang",
    "Wenbo Xu",
    "Edward C Holmes",
    "George F Gao",
    "Guizhen Wu",
    "Weijun Chen",
    "Weifeng Shi",
    "Wenjie Tan"
  ],
  "references": [
    "3001118548",
    "3001897055",
    "3002539152",
    "3004280078",
    "2103441770",
    "2141052558",
    "2166867592",
    "2306794997",
    "2804822363",
    "3017468735"
  ]
}, {
  "id": "3003465021",
  "title": "First Case of 2019 Novel Coronavirus in the United States. | Paper | Microsoft Academic",
  "abstract": "An outbreak of novel coronavirus (2019-nCoV) that began in Wuhan, China, has spread rapidly, with cases now confirmed in multiple countries. We report the first case of 2019-nCoV infection confirmed in the United States and describe the identification, diagnosis, clinical course, and management of the case, including the patient's initial mild symptoms at presentation with progression to pneumonia on day 9 of illness. This case highlights the importance of close coordination between clinicians and public health authorities at the local, state, and federal levels, as well as the need for rapid dissemination of clinical information related to the care of patients with this emerging infection.",
  "date": " 2020 ",
  "authors": [
    "Michelle L Holshue",
    "Chas DeBolt",
    "Scott Lindquist",
    "Kathy H Lofy",
    "John Wiesman",
    "Hollianne Bruce",
    "Christopher Spitters",
    "Keith Ericson",
    "Sara Wilkerson",
    "Ahmet Tural",
    "George Diaz",
    "Amanda Cohn",
    "LeAnne Fox",
    "Anita Patel",
    "Susan I Gerber",
    "Lindsay Kim",
    "Suxiang Tong",
    "Xiaoyan Lu",
    "Steve Lindstrom",
    "Mark A Pallansch",
    "William C Weldon",
    "Holly M Biggs",
    "Timothy M Uyeki",
    "Satish K Pillai"
  ],
  "references": [
    "3001118548",
    "3001897055",
    "3002539152",
    "3003951199",
    "3000413850",
    "2991491848",
    "2605343262"
  ]
}, {
  "id": "3004239190",
  "title": "Transmission of 2019-nCoV Infection from an Asymptomatic Contact in Germany. | Paper | Microsoft Academic",
  "abstract": "2019-nCoV Transmission from Asymptomatic Patient In this report, investigators in Germany detected the spread of the novel coronavirus (2019-nCoV) from a person who had recently traveled from China...",
  "date": " 2020 ",
  "authors": [
    "Camilla Rothe",
    "Mirjam Schunk",
    "Peter Sothmann",
    "Gisela Bretzel",
    "Guenter Froeschl",
    "Claudia Wallrauch",
    "Thorbj\u00f6rn Zimmer",
    "Verena Thiel",
    "Christian Janke",
    "Wolfgang Guggemos",
    "Michael Seilmaier",
    "Christian Drosten",
    "Patrick Vollmar",
    "Katrin Zwirglmaier",
    "Sabine Zange",
    "Roman W\u00f6lfel",
    "Michael Hoelscher"
  ],
  "references": [
    "3001897055",
    "3001388158"
  ]
}, {
  "id": "3003573988",
  "title": "Nowcasting and forecasting the potential domestic and international spread of the 2019-nCoV outbreak originating in Wuhan, China: a modelling study. | Paper | Microsoft Academic",
  "abstract": "Summary  Background  Since Dec 31, 2019, the Chinese city of Wuhan has reported an outbreak of atypical pneumonia caused by the 2019 novel coronavirus (2019-nCoV). Cases have been exported to other Chinese cities, as well as internationally, threatening to trigger a global outbreak. Here, we provide an estimate of the size of the epidemic in Wuhan on the basis of the number of cases exported from Wuhan to cities outside mainland China and forecast the extent of the domestic and global public health risks of epidemics, accounting for social and non-pharmaceutical prevention interventions.  Methods  We used data from Dec 31, 2019, to Jan 28, 2020, on the number of cases exported from Wuhan internationally (known days of symptom onset from Dec 25, 2019, to Jan 19, 2020) to infer the number of infections in Wuhan from Dec 1, 2019, to Jan 25, 2020. Cases exported domestically were then estimated. We forecasted the national and global spread of 2019-nCoV, accounting for the effect of the metropolitan-wide quarantine of Wuhan and surrounding cities, which began Jan 23\u201324, 2020. We used data on monthly flight bookings from the Official Aviation Guide and data on human mobility across more than 300 prefecture-level cities in mainland China from the Tencent database. Data on confirmed cases were obtained from the reports published by the Chinese Center for Disease Control and Prevention. Serial interval estimates were based on previous studies of severe acute respiratory syndrome coronavirus (SARS-CoV). A susceptible-exposed-infectious-recovered metapopulation model was used to simulate the epidemics across all major cities in China. The basic reproductive number was estimated using Markov Chain Monte Carlo methods and presented using the resulting posterior mean and 95% credibile interval (CrI).  Findings  In our baseline scenario, we estimated that the basic reproductive number for 2019-nCoV was 2\u00b768 (95% CrI 2\u00b747\u20132\u00b786) and that 75\u2008815 individuals (95% CrI 37\u2008304\u2013130\u2008330) have been infected in Wuhan as of Jan 25, 2020. The epidemic doubling time was 6\u00b74 days (95% CrI 5\u00b78\u20137\u00b71). We estimated that in the baseline scenario, Chongqing, Beijing, Shanghai, Guangzhou, and Shenzhen had imported 461 (95% CrI 227\u2013805), 113 (57\u2013193), 98 (49\u2013168), 111 (56\u2013191), and 80 (40\u2013139) infections from Wuhan, respectively. If the transmissibility of 2019-nCoV were similar everywhere domestically and over time, we inferred that epidemics are already growing exponentially in multiple major cities of China with a lag time behind the Wuhan outbreak of about 1\u20132 weeks.  Interpretation  Given that 2019-nCoV is no longer contained within Wuhan, other major Chinese cities are probably sustaining localised outbreaks. Large cities overseas with close transport links to China could also become outbreak epicentres, unless substantial public health interventions at both the population and personal levels are implemented immediately. Independent self-sustaining outbreaks in major cities globally could become inevitable because of substantial exportation of presymptomatic cases and in the absence of large-scale public health interventions. Preparedness plans and mitigation interventions should be readied for quick deployment globally.  Funding  Health and Medical Research Fund (Hong Kong, China).",
  "date": " 2020 ",
  "authors": [
    "Joseph T Wu",
    "Kathy Leung",
    "Gabriel M Leung"
  ],
  "references": [
    "3003668884",
    "3004397688",
    "3002764620",
    "2147166346",
    "3002533591",
    "2069251911",
    "2096145431",
    "1815575713",
    "2104595316",
    "1998725525"
  ]
}, {
  "id": "2145339207",
  "title": "Human-level control through deep reinforcement learning | Paper | Microsoft Academic",
  "abstract": "The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.",
  "date": " 2015 ",
  "authors": [
    "Volodymyr Mnih",
    "Koray Kavukcuoglu",
    "David Silver",
    "Andrei A. Rusu",
    "Joel Veness",
    "Marc G. Bellemare",
    "Alex Graves",
    "Martin Riedmiller",
    "Andreas K. Fidjeland",
    "Georg Ostrovski",
    "Stig Petersen",
    "Charles Beattie",
    "Amir Sadik",
    "Ioannis Antonoglou",
    "Helen King",
    "Dharshan Kumaran",
    "Daan Wierstra",
    "Shane Legg",
    "Demis Hassabis"
  ],
  "references": [
    "2618530766",
    "2100495367",
    "2310919327",
    "2187089797",
    "1665214252",
    "2072128103",
    "2546302380",
    "1652505363",
    "2121863487",
    "2952509347"
  ]
}, {
  "id": "2100495367",
  "title": "Reducing the Dimensionality of Data with Neural Networks | Paper | Microsoft Academic",
  "abstract": "High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such \"autoencoder\" networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.",
  "date": " 2006 ",
  "authors": [
    "G. E. Hinton",
    "R. R. Salakhutdinov"
  ],
  "references": [
    "2136922672",
    "2053186076",
    "2001141328",
    "2293063825",
    "2121122425",
    "2032647857",
    "2021774695"
  ]
}, {
  "id": "2163922914",
  "title": "Representation Learning: A Review and New Perspectives | Paper | Microsoft Academic",
  "abstract": "The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.",
  "date": " 2013 ",
  "authors": [
    "Y. Bengio",
    "A. Courville",
    "P. Vincent"
  ],
  "references": [
    "2618530766",
    "2136922672",
    "3118608800",
    "2100495367",
    "2310919327",
    "2158899491",
    "2162915993",
    "2187089797",
    "1665214252",
    "2160815625"
  ]
}, {
  "id": "2160815625",
  "title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups | Paper | Microsoft Academic",
  "abstract": "Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks (DNNs) that have many hidden layers and are trained using new methods have been shown to outperform GMMs on a variety of speech recognition benchmarks, sometimes by a large margin. This article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using DNNs for acoustic modeling in speech recognition.",
  "date": " 2012 ",
  "authors": [
    "G. Hinton",
    "Li Deng",
    "Dong Yu",
    "G. E. Dahl",
    "A. Mohamed",
    "N. Jaitly",
    "Andrew Senior",
    "V. Vanhoucke",
    "P. Nguyen",
    "T. N. Sainath",
    "B. Kingsbury"
  ],
  "references": [
    "2136922672",
    "2100495367",
    "1533861849",
    "2116064496",
    "2147768505",
    "2145094598",
    "1993882792",
    "44815768",
    "1498436455",
    "1994197834"
  ]
}, {
  "id": "2022508996",
  "title": "Learning Hierarchical Features for Scene Labeling | Paper | Microsoft Academic",
  "abstract": "Scene labeling consists of labeling each pixel in an image with the category of the object it belongs to. We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features, and produces a powerful representation that captures texture, shape, and contextual information. We report results using multiple postprocessing methods to produce the final labeling. Among those, we propose a technique to automatically retrieve, from a pool of segmentation components, an optimal set of components that best explain the scene; these components are arbitrary, for example, they can be taken from a segmentation tree or from any family of oversegmentations. The system yields record accuracies on the SIFT Flow dataset (33 classes) and the Barcelona dataset (170 classes) and near-record accuracy on Stanford background dataset (eight classes), while being an order of magnitude faster than competing approaches, producing a 320\u00d7240 image labeling in less than a second, including feature extraction.",
  "date": " 2013 ",
  "authors": [
    "C. Farabet",
    "C. Couprie",
    "L. Najman",
    "Y. LeCun"
  ],
  "references": [
    "2310919327",
    "2110158442",
    "2546302380",
    "2130325614",
    "1999478155",
    "2143516773",
    "1423339008",
    "2169551590",
    "2156163116",
    "2113137767"
  ]
}, {
  "id": "1993882792",
  "title": "Acoustic Modeling Using Deep Belief Networks | Paper | Microsoft Academic",
  "abstract": "Gaussian mixture models are currently the dominant technique for modeling the emission distribution of hidden Markov models for speech recognition. We show that better phone recognition on the TIMIT dataset can be achieved by replacing Gaussian mixture models by deep neural networks that contain many layers of features and a very large number of parameters. These networks are first pre-trained as a multi-layer generative model of a window of spectral feature vectors without making use of any discriminative information. Once the generative pre-training has designed the features, we perform discriminative fine-tuning using backpropagation to adjust the features slightly to make them better at predicting a probability distribution over the states of monophone hidden Markov models.",
  "date": " 2011 ",
  "authors": [
    "A. Mohamed",
    "G. E. Dahl",
    "G. Hinton"
  ],
  "references": [
    "2136922672",
    "3118608800",
    "2100495367",
    "2116064496",
    "2147768505",
    "2159080219",
    "44815768",
    "1994197834",
    "2913932916",
    "2103359087"
  ]
}, {
  "id": "2108069432",
  "title": "2012 Special Issue: Multi-column deep neural network for traffic sign classification | Paper | Microsoft Academic",
  "abstract": "We describe the approach that won the final phase of the German traffic sign recognition benchmark. Our method is the only one that achieved a better-than-human recognition rate of 99.46%. We use a fast, fully parameterizable GPU implementation of a Deep Neural Network (DNN) that does not require careful design of pre-wired feature extractors, which are rather learned in a supervised way. Combining various DNNs trained on differently preprocessed data into a Multi-Column DNN (MCDNN) further boosts recognition performance, making the system insensitive also to variations in contrast and illumination.",
  "date": " 2012 ",
  "authors": [
    "Dan Cire\u015fAn",
    "Ueli Meier",
    "Jonathan Masci",
    "J\u00fcRgen Schmidhuber"
  ],
  "references": [
    "1663973292",
    "2310919327",
    "2156163116",
    "2148461049",
    "1624854622",
    "2132424367",
    "2105464873",
    "2149194912",
    "1523493493",
    "2125085157"
  ]
}, {
  "id": "2151103935",
  "title": "Distinctive Image Features from Scale-Invariant Keypoints | Paper | Microsoft Academic",
  "abstract": "This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.",
  "date": " 2004 ",
  "authors": [
    "David G. Lowe"
  ],
  "references": [
    "2033819227",
    "2124386111",
    "2154422044",
    "2012778485",
    "2124404372",
    "1676552347",
    "2124087378",
    "2111308925",
    "2165497495",
    "1949116567"
  ]
}, {
  "id": "2038721957",
  "title": "WordNet : an electronic lexical database | Paper | Microsoft Academic",
  "abstract": "Part 1 The lexical database: nouns in WordNet, George A. Miller modifiers in WordNet, Katherine J. Miller a semantic network of English verbs, Christiane Fellbaum design and implementation of the WordNet lexical database and searching software, Randee I. Tengi. Part 2: automated discovery of WordNet relations, Marti A. Hearst representing verb alterations in WordNet, Karen T. Kohl et al the formalization of WordNet by methods of relational concept analysis, Uta E. Priss. Part 3 Applications of WordNet: building semantic concordances, Shari Landes et al performance and confidence in a semantic annotation task, Christiane Fellbaum et al WordNet and class-based probabilities, Philip Resnik combining local context and WordNet similarity for word sense identification, Claudia Leacock and Martin Chodorow using WordNet for text retrieval, Ellen M. Voorhees lexical chains as representations of context for the detection and correction of malapropisms, Graeme Hirst and David St-Onge temporal indexing through lexical chaining, Reem Al-Halimi and Rick Kazman COLOR-X - using knowledge from WordNet for conceptual modelling, J.F.M. Burg and R.P. van de Riet knowledge processing on an extended WordNet, Sanda M. Harabagiu and Dan I Moldovan appendix - obtaining and using WordNet.",
  "date": " 2000 ",
  "authors": [
    "Christiane Fellbaum"
  ],
  "references": [
    "2108598243",
    "1861492603",
    "2031489346",
    "2097726431",
    "2110764733",
    "2160660844",
    "2132339004",
    "2145607950",
    "2952122856",
    "2017814585"
  ]
}, {
  "id": "2128017662",
  "title": "Scalable Recognition with a Vocabulary Tree | Paper | Microsoft Academic",
  "abstract": "A recognition scheme that scales efficiently to a large number of objects is presented. The efficiency and quality is exhibited in a live demonstration that recognizes CD-covers from a database of 40000 images of popular music CD\u0092s. The scheme builds upon popular techniques of indexing descriptors extracted from local regions, and is robust to background clutter and occlusion. The local region descriptors are hierarchically quantized in a vocabulary tree. The vocabulary tree allows a larger and more discriminatory vocabulary to be used efficiently, which we show experimentally leads to a dramatic improvement in retrieval quality. The most significant property of the scheme is that the tree directly defines the quantization. The quantization and the indexing are therefore fully integrated, essentially being one and the same. The recognition quality is evaluated through retrieval on a database with ground truth, showing the power of the vocabulary tree approach, going as high as 1 million images.",
  "date": " 2006 ",
  "authors": [
    "D. Nister",
    "H. Stewenius"
  ],
  "references": [
    "2151103935",
    "2177274842",
    "2131846894",
    "1980911747",
    "2104978738",
    "2172188317",
    "2124404372",
    "2147717514",
    "2162006472",
    "2165497495"
  ]
}, {
  "id": "2110764733",
  "title": "LabelMe: A Database and Web-Based Tool for Image Annotation | Paper | Microsoft Academic",
  "abstract": "We seek to build a large collection of images with ground truth labels to be used for object detection and recognition research. Such data is useful for supervised learning and quantitative evaluation. To achieve this, we developed a web-based tool that allows easy image annotation and instant sharing of such annotations. Using this annotation tool, we have collected a large dataset that spans many object categories, often containing multiple instances over a wide variety of images. We quantify the contents of the dataset and compare against existing state of the art datasets used for object recognition and detection. Also, we show how to extend the dataset to automatically enhance object labels with WordNet, discover object parts, recover a depth ordering of objects in a scene, and increase the number of labels using minimal user supervision and images from the web.",
  "date": " 2008 ",
  "authors": [
    "Bryan C. Russell",
    "Antonio Torralba",
    "Kevin P. Murphy",
    "William T. Freeman"
  ],
  "references": [
    "2156909104",
    "2164598857",
    "2038721957",
    "2138451337",
    "2154422044",
    "2107034620",
    "1566135517",
    "2166049352",
    "2156598602",
    "2134557905"
  ]
}, {
  "id": "1782590233",
  "title": "Labeled Faces in the Wild: A Database forStudying Face Recognition in Unconstrained Environments | Paper | Microsoft Academic",
  "abstract": "Most face databases have been created under controlled conditions to facilitate the study of specific parameters on the face recognition problem. These parameters include such variables as position, pose, lighting, background, camera quality, and gender. While there are many applications for face recognition technology in which one can control the parameters of image acquisition, there are also many applications in which the practitioner has little or no control over such parameters. This database, Labeled Faces in the Wild, is provided as an aid in studying the latter, unconstrained, recognition problem. The database contains labeled face photographs spanning the range of conditions typically encountered in everyday life. The database exhibits \u201cnatural\u201d variability in factors such as pose, lighting, race, accessories, occlusions, and background. In addition to describing the details of the database, we provide specific experimental paradigms for which the database is suitable. This is done in an effort to make research performed with the database as consistent and comparable as possible. We provide baseline results, including results of a state of the art face recognition system combined with a face alignment system. To facilitate experimentation on the database, we provide several parallel databases, including an aligned version.",
  "date": " 2008 ",
  "authors": [
    "Gary B. Huang",
    "Marwan Mattar",
    "Tamara Berg",
    "Eric Learned-Miller"
  ],
  "references": [
    "3097096317",
    "1999478155",
    "2121647436",
    "2033419168",
    "2137659841",
    "3111480503",
    "2098693229",
    "2125310925",
    "2994340921",
    "2006793117"
  ]
}, {
  "id": "1576445103",
  "title": "Caltech-256 Object Category Dataset | Paper | Microsoft Academic",
  "abstract": "We introduce a challenging set of 256 object categories containing a total of 30607 images. The original Caltech-101 [1] was collected by choosing a set of object categories, downloading examples from Google Images and then manually screening out all images that did not fit the category. Caltech-256 is collected in a similar manner with several improvements: a) the number of categories is more than doubled, b) the minimum number of images in any category is increased from 31 to 80, c) artifacts due to image rotation are avoided and d) a new and larger clutter category is introduced for testing background rejection. We suggest several testing paradigms to measure classification performance, then benchmark the dataset using two simple metrics as well as a state-of-the-art spatial pyramid matching [2] algorithm. Finally we use the clutter category to train an interest detector which rejects uninformative background regions.",
  "date": " 2007 ",
  "authors": [
    "Gregory Griffin",
    "Alex Holub",
    "Pietro Perona"
  ],
  "references": [
    "2618530766",
    "2962835968",
    "2117539524",
    "2108598243",
    "1861492603",
    "2031489346",
    "2963173190",
    "2295107390"
  ]
}, {
  "id": "2145607950",
  "title": "80 Million Tiny Images: A Large Data Set for Nonparametric Object and Scene Recognition | Paper | Microsoft Academic",
  "abstract": "With the advent of the Internet, billions of images are now freely available online and constitute a dense sampling of the visual world. Using a variety of non-parametric methods, we explore this world with the aid of a large dataset of 79,302,017 images collected from the Internet. Motivated by psychophysical results showing the remarkable tolerance of the human visual system to degradations in image resolution, the images in the dataset are stored as 32 x 32 color images. Each image is loosely labeled with one of the 75,062 non-abstract nouns in English, as listed in the Wordnet lexical database. Hence the image database gives a comprehensive coverage of all object categories and scenes. The semantic information from Wordnet can be used in conjunction with nearest-neighbor methods to perform object classification over a range of semantic levels minimizing the effects of labeling noise. For certain classes that are particularly prevalent in the dataset, such as people, we are able to demonstrate a recognition performance comparable to class-specific Viola-Jones style detectors.",
  "date": " 2008 ",
  "authors": [
    "A. Torralba",
    "R. Fergus",
    "W.T. Freeman"
  ],
  "references": [
    "2164598857",
    "2162915993",
    "2124386111",
    "2038721957",
    "2128017662",
    "2110764733",
    "1576445103",
    "2107034620",
    "1566135517",
    "2111993661"
  ]
}, {
  "id": "2141282920",
  "title": "Labeling images with a computer game | Paper | Microsoft Academic",
  "abstract": "We introduce a new interactive system: a game that is fun and can be used to create valuable output. When people play the game they help determine the contents of images by providing meaningful labels for them. If the game is played as much as popular online games, we estimate that most images on the Web can be labeled in a few months. Having proper labels associated with each image on the Web would allow for more accurate image search, improve the accessibility of sites (by providing descriptions of images to visually impaired individuals), and help users block inappropriate images. Our system makes a significant contribution because of its valuable output and because of the way it addresses the image-labeling problem. Rather than using computer vision techniques, which don't work well enough, we encourage people to do the work by taking advantage of their desire to be entertained.",
  "date": " 2004 ",
  "authors": [
    "Luis von Ahn",
    "Laura Dabbish"
  ],
  "references": [
    "1666447063",
    "1934863104",
    "2166770390",
    "1587328194",
    "2293605478",
    "2055225264",
    "2050457084",
    "181417509",
    "2612148268"
  ]
}, {
  "id": "2115733720",
  "title": "One-shot learning of object categories | Paper | Microsoft Academic",
  "abstract": "Learning visual models of object categories notoriously requires hundreds or thousands of training examples. We show that it is possible to learn much information about a category from just one, or a handful, of images. The key insight is that, rather than learning from scratch, one can take advantage of knowledge coming from previously learned categories, no matter how different these categories might be. We explore a Bayesian implementation of this idea. Object categories are represented by probabilistic models. Prior knowledge is represented as a probability density function on the parameters of these models. The posterior model for an object category is obtained by updating the prior in the light of one or more observations. We test a simple implementation of our algorithm on a database of 101 diverse object categories. We compare category models learned by an implementation of our Bayesian approach to models learned from by maximum likelihood (ML) and maximum a posteriori (MAP) methods. We find that on a database of more than 100 categories, the Bayesian approach produces informative models when the number of training examples is too small for other methods to operate successfully.",
  "date": " 2006 ",
  "authors": [
    "Li Fei-Fei",
    "R. Fergus",
    "P. Perona"
  ],
  "references": [
    "2164598857",
    "2310919327",
    "2124386111",
    "2217896605",
    "2154422044",
    "2045656233",
    "2166049352",
    "2134557905",
    "2130416410",
    "2049633694"
  ]
}, {
  "id": "1528789833",
  "title": "TextonBoost : joint appearance, shape and context modeling for multi-class object recognition and segmentation | Paper | Microsoft Academic",
  "abstract": "This paper proposes a new approach to learning a discriminative model of object classes, incorporating appearance, shape and context information efficiently. The learned model is used for automatic visual recognition and semantic segmentation of photographs. Our discriminative model exploits novel features, based on textons, which jointly model shape and texture. Unary classification and feature selection is achieved using shared boosting to give an efficient classifier which can be applied to a large number of classes. Accurate image segmentation is achieved by incorporating these classifiers in a conditional random field. Efficient training of the model on very large datasets is achieved by exploiting both random feature selection and piecewise training methods.\r\n\r\nHigh classification and segmentation accuracy are demonstrated on three different databases: i) our own 21-object class database of photographs of real objects viewed under general lighting conditions, poses and viewpoints, ii) the 7-class Corel subset and iii) the 7-class Sowerby database used in [1]. The proposed algorithm gives competitive results both for highly textured (e.g. grass, trees), highly structured (e.g. cars, faces, bikes, aeroplanes) and articulated objects (e.g. body, cow).",
  "date": " 2006 ",
  "authors": [
    "Jamie Shotton",
    "John Winn",
    "Carsten Rother",
    "Antonio Criminisi"
  ],
  "references": [
    "2164598857",
    "2147880316",
    "2057175746",
    "2154422044",
    "2124351162",
    "2024046085",
    "2169551590",
    "1666447063",
    "2168002178",
    "1484228140"
  ]
}, {
  "id": "3008818676",
  "title": "The epidemiological characteristics of an outbreak of 2019 novel coronavirus diseases (COVID-19) in China | Paper | Microsoft Academic",
  "abstract": "Objective\r\nAn outbreak of 2019 novel coronavirus diseases (COVID-19) in Wuhan, China has spread quickly nationwide. Here, we report results of a descriptive, exploratory analysis of all cases diagnosed as of February 11, 2020.\r\n\r\n\r\nMethods\r\nAll COVID-19 cases reported through February 11, 2020 were extracted from China\u2019s Infectious Disease Information System. Analyses included: 1) summary of patient characteristics; 2) examination of age distributions and sex ratios; 3) calculation of case fatality and mortality rates; 4) geo-temporal analysis of viral spread; 5) epidemiological curve construction; and 6) subgroup analysis.\r\n\r\n\r\nResults\r\nA total of 72 314 patient records-44 672 (61.8%) confirmed cases, 16 186 (22.4%) suspected cases, 10567 (14.6%) clinical diagnosed cases (Hubei only), and 889 asymptomatic cases (1.2%)-contributed data for the analysis. Among confirmed cases, most were aged 30-79 years (86.6%), diagnosed in Hubei (74.7%), and considered mild/mild pneumonia (80.9%). A total of 1 023 deaths occurred among confirmed cases for an overall case-fatality rate of 2.3%. The COVID-19 spread outward from Hubei sometime after December 2019 and by February 11, 2020, 1 386 counties across all 31 provinces were affected. The epidemic curve of onset of symptoms peaked in January 23-26, then began to decline leading up to February 11. A total of 1 716 health workers have become infected and 5 have died (0.3%).\r\n\r\n\r\nConclusions\r\nThe COVID-19 epidemic has spread very quickly. It only took 30 days to expand from Hubei to the rest of Mainland China. With many people returning from a long holiday, China needs to prepare for the possible rebound of the epidemic.\r\n\r\n\r\nKey words: \r\n2019 Novel Coronavirus;\u00a0Outbreak;\u00a0Epidemiological characteristics",
  "date": " 2020 ",
  "authors": [
    "Novel Coronavirus Pneumonia Emergency Response Epidemiology Team"
  ],
  "references": [
    "3033453353",
    "3034593359",
    "3035018050",
    "3033301213",
    "3021916232",
    "3037552531",
    "3031029566",
    "3037451072",
    "3037851904"
  ]
}, {
  "id": "3004906315",
  "title": "CT Imaging Features of 2019 Novel Coronavirus (2019-nCoV). | Paper | Microsoft Academic",
  "abstract": "In this retrospective case series, chest CT scans of 21 symptomatic patients from China infected with the 2019 novel coronavirus (2019-nCoV) were reviewed, with emphasis on identifying and characterizing the most common findings. Typical CT findings included bilateral pulmonary parenchymal ground-glass and consolidative pulmonary opacities, sometimes with a rounded morphology and a peripheral lung distribution. Notably, lung cavitation, discrete pulmonary nodules, pleural effusions, and lymphadenopathy were absent. Follow-up imaging in a subset of patients during the study time window often demonstrated mild or moderate progression of disease, as manifested by increasing extent and density of lung opacities.",
  "date": " 2020 ",
  "authors": [
    "Michael Chung",
    "Adam Bernheim",
    "Xueyan Mei",
    "Ning Zhang",
    "Mingqian Huang",
    "Xianjun Zeng",
    "Jiufa Cui",
    "Wenjian Xu",
    "Yang Yang",
    "Zahi A. Fayad",
    "Adam Jacobi",
    "Kunwei Li",
    "Shaolin Li",
    "Hong Shan"
  ],
  "references": [
    "2102634410",
    "2800783955",
    "2112136274",
    "2056155046",
    "2279340859",
    "2080286891",
    "2162899218"
  ]
}, {
  "id": "3006110666",
  "title": "Chest CT for Typical 2019-nCoV Pneumonia: Relationship to Negative RT-PCR Testing. | Paper | Microsoft Academic",
  "abstract": "Some patients with positive chest CT findings may present with negative results of real-time reverse-transcription polymerase chain reaction (RT-PCR) tests for coronavirus disease 2019 (COVID-19). In this study, the authors present chest CT findings from five patients with COVID-19 infection who had initial negative RT-PCR results. All five patients had typical imaging findings, including ground-glass opacity (five patients) and/or mixed ground-glass opacity and mixed consolidation (two patients). After isolation for presumed COVID-19 pneumonia, all patients were eventually confirmed to have COVID-19 infection by means of repeated swab tests. A combination of repeated swab tests and CT scanning may be helpful for individuals with a high clinical suspicion of COVID-19 infection but negative findings at RT-PCR screening.",
  "date": " 2020 ",
  "authors": [
    "Xingzhi Xie",
    "Zheng Zhong",
    "Wei Zhao",
    "Chao Zheng",
    "Fei Wang",
    "Jun Liu"
  ],
  "references": [
    "3004906315",
    "3005272159",
    "2112136274",
    "2056155046"
  ]
}, {
  "id": "3006643024",
  "title": "Time Course of Lung Changes at Chest CT during Recovery from Coronavirus Disease 2019 (COVID-19). | Paper | Microsoft Academic",
  "abstract": "Background Chest CT is used to assess the severity of lung involvement in coronavirus disease 2019 (COVID-19). Purpose To determine the changes in chest CT findings associated with COVID-19 from initial diagnosis until patient recovery. Materials and Methods This retrospective review included patients with real-time polymerase chain reaction-confirmed COVID-19 who presented between January 12, 2020, and February 6, 2020. Patients with severe respiratory distress and/or oxygen requirement at any time during the disease course were excluded. Repeat chest CT was performed at approximately 4-day intervals. Each of the five lung lobes was visually scored on a scale of 0 to 5, with 0 indicating no involvement and 5 indicating more than 75% involvement. The total CT score was determined as the sum of lung involvement, ranging from 0 (no involvement) to 25 (maximum involvement). Results Twenty-one patients (six men and 15 women aged 25-63 years) with confirmed COVID-19 were evaluated. A total of 82 chest CT scans were obtained in these patients, with a mean interval (\u00b1standard deviation) of 4 days \u00b1 1 (range, 1-8 days). All patients were discharged after a mean hospitalization period of 17 days \u00b1 4 (range, 11-26 days). Maximum lung involved peaked at approximately 10 days (with a calculated total CT score of 6) from the onset of initial symptoms (R2 = 0.25, P < .001). Based on quartiles of chest CT scans from day 0 to day 26 involvement, four stages of lung CT findings were defined. CT scans obtained in stage 1 (0-4 days) showed ground-glass opacities (18 of 24 scans [75%]), with a mean total CT score of 2 \u00b1 2; scans obtained in stage 2 (5-8 days) showed an increase in both the crazy-paving pattern (nine of 17 scans [53%]) and total CT score (mean, 6 \u00b1 4; P = .002); scans obtained in stage 3 (9-13 days) showed consolidation (19 of 21 scans [91%]) and a peak in the total CT score (mean, 7 \u00b1 4); and scans obtained in stage 4 (\u226514 days) showed gradual resolution of consolidation (15 of 20 scans [75%]) and a decrease in the total CT score (mean, 6 \u00b1 4) without crazy-paving pattern. Conclusion In patients recovering from coronavirus disease 2019 (without severe respiratory distress during the disease course), lung abnormalities on chest CT scans showed greatest severity approximately 10 days after initial onset of symptoms. \u00a9 RSNA, 2020.",
  "date": " 2020 ",
  "authors": [
    "Feng Pan",
    "Tianhe Ye",
    "Peng Sun",
    "Shan Gui",
    "Bo Liang",
    "Lingli Li",
    "Dandan Zheng",
    "Jiazheng Wang",
    "Richard L Hesketh",
    "Lian Yang",
    "Chuansheng Zheng"
  ],
  "references": [
    "3001118548",
    "3001897055",
    "3005079553",
    "3003668884",
    "3004906315",
    "2102634410",
    "2800783955",
    "3004802901",
    "2092969802",
    "2056155046"
  ]
}, {
  "id": "3006354146",
  "title": "Initial CT findings and temporal changes in patients with the novel coronavirus pneumonia (2019-nCoV): a study of 63 patients in Wuhan, China. | Paper | Microsoft Academic",
  "abstract": "The purpose of this study was to observe the imaging characteristics of the novel coronavirus pneumonia. Sixty-three confirmed patients were enrolled from December 30, 2019 to January 31, 2020. High-resolution CT (HRCT) of the chest was performed. The number of affected lobes, ground glass nodules (GGO), patchy/punctate ground glass opacities, patchy consolidation, fibrous stripes and irregular solid nodules in each patient's chest CT image were recorded. Additionally, we performed imaging follow-up of these patients. CT images of 63 confirmed patients were collected. M/F ratio: 33/30. The mean age was 44.9 \u00b1 15.2 years. The mean number of affected lobes was 3.3 \u00b1 1.8. Nineteen (30.2%) patients had one affected lobe, five (7.9%) patients had two affected lobes, four (6.3%) patients had three affected lobes, seven (11.1%) patients had four affected lobes while 28 (44.4%) patients had 5 affected lobes. Fifty-four (85.7%) patients had patchy/punctate ground glass opacities, 14 (22.2%) patients had GGO, 12 (19.0%) patients had patchy consolidation, 11 (17.5%) patients had fibrous stripes and 8 (12.7%) patients had irregular solid nodules. Fifty-four (85.7%) patients progressed, including single GGO increased, enlarged and consolidated; fibrous stripe enlarged, while solid nodules increased and enlarged. Imaging changes in novel viral pneumonia are rapid. The manifestations of the novel coronavirus pneumonia are diverse. Imaging changes of typical viral pneumonia and some specific imaging features were observed. Therefore, we need to strengthen the recognition of image changes to help clinicians to diagnose quickly and accurately. \u2022 High-resolution CT (HRCT) of the chest is critical for early detection, evaluation of disease severity and follow-up of patients with the novel coronavirus pneumonia. \u2022 The manifestations of the novel coronavirus pneumonia are diverse and change rapidly. \u2022 Radiologists should be aware of the various features of the disease and temporal changes.",
  "date": " 2020 ",
  "authors": [
    "Yueying Pan",
    "Hanxiong Guan",
    "Shuchang Zhou",
    "Yujin Wang",
    "Qian Li",
    "Tingting Zhu",
    "Qiongjie Hu",
    "Liming Xia"
  ],
  "references": [
    "3001118548",
    "3002539152",
    "3001465255",
    "3001456238"
  ]
}, {
  "id": "3003901880",
  "title": "CT Imaging of the 2019 Novel Coronavirus (2019-nCoV) Pneumonia. | Paper | Microsoft Academic",
  "abstract": "",
  "date": " 2020 ",
  "authors": [
    "Junqiang Lei",
    "Junfeng Li",
    "Xun Li",
    "Xiaolong Qi"
  ],
  "references": [
    "3002533507"
  ]
}, {
  "id": "3005656138",
  "title": "Use of Chest CT in Combination with Negative RT-PCR Assay for the 2019 Novel Coronavirus but High Clinical Suspicion. | Paper | Microsoft Academic",
  "abstract": "",
  "date": " 2020 ",
  "authors": [
    "Peikai Huang",
    "Tianzhu Liu",
    "Lesheng Huang",
    "Hailong Liu",
    "Ming Lei",
    "Wangdong Xu",
    "Xiaolu Hu",
    "Jun Chen",
    "Bo Liu"
  ],
  "references": [
    "3001897055",
    "3004668429"
  ]
}, {
  "id": "3004511262",
  "title": "Evolution of CT Manifestations in a Patient Recovered from 2019 Novel Coronavirus (2019-nCoV) Pneumonia in Wuhan, China. | Paper | Microsoft Academic",
  "abstract": "",
  "date": " 2020 ",
  "authors": [
    "Heshui Shi",
    "Xiaoyu Han",
    "Chuansheng Zheng"
  ],
  "references": [
    "3007497549",
    "3008627141",
    "3025334942",
    "3009992310",
    "3008928918",
    "3008801544",
    "3034593359",
    "3011414603",
    "3013468450"
  ]
}, {
  "id": "3008962515",
  "title": "Molecular and serological investigation of 2019-nCoV infected patients: implication of multiple shedding routes. | Paper | Microsoft Academic",
  "abstract": "In December 2019, a novel coronavirus (2019-nCoV) caused an outbreak in Wuhan, China, and soon spread to other parts of the world. It was believed that 2019-nCoV was transmitted through respiratory tract and then induced pneumonia, thus molecular diagnosis based on oral swabs was used for confirmation of this disease. Likewise, patient will be released upon two times of negative detection from oral swabs. However, many coronaviruses can also be transmitted through oral-fecal route by infecting intestines. Whether 2019-nCoV infected patients also carry virus in other organs like intestine need to be tested. We conducted investigation on patients in a local hospital who were infected with this virus. We found the presence of 2019-nCoV in anal swabs and blood as well, and more anal swab positives than oral swab positives in a later stage of infection, suggesting shedding and thereby transmitted through oral-fecal route. We also showed serology test can improve detection positive rate thus should be used in future epidemiology. Our report provides a cautionary warning that 2019-nCoV may be shed through multiple routes.",
  "date": " 2020 ",
  "authors": [
    "Wei Zhang",
    "Rong-Hui Du",
    "Bei Li",
    "Xiao-Shuang Zheng",
    "Xing-Lou Yang",
    "Ben Hu",
    "Yan-Yi Wang",
    "Geng-Fu Xiao",
    "Bing Yan",
    "Zheng-Li Shi",
    "Peng Zhou"
  ],
  "references": [
    "3001118548",
    "3001897055",
    "3003668884",
    "3004280078",
    "2786098272",
    "2769543984",
    "2021442163",
    "3025232310",
    "3028321619",
    "3027541845"
  ]
}, {
  "id": "3008452791",
  "title": "Viral load of SARS-CoV-2 in clinical samples. | Paper | Microsoft Academic",
  "abstract": "",
  "date": " 2020 ",
  "authors": [
    "Yang Pan",
    "Daitao Zhang",
    "Peng Yang",
    "Leo L M Poon",
    "Quanyi Wang"
  ],
  "references": [
    "3004318991",
    "2129542667",
    "3003637715"
  ]
}, {
  "id": "3033453353",
  "title": "Recent Understandings Toward Coronavirus Disease 2019 (COVID-19): From Bench to Bedside | Paper | Microsoft Academic",
  "abstract": "In late December 2019, an unprecedented outbreak of coronavirus disease 2019 (COVID-19) caused by SARS coronavirus 2 (SARS-CoV-2) (previously named 2019-nCoV) in Wuhan became the most challenging health emergency. Since its rapid spread in China and many other countries, the World Health Organization (WHO) declared COVID-19 a public health emergency of international concern (PHEIC) on 30th January 2020 and a pandemic on 11th March 2020. Thousands of people have died, and there are currently no vaccines or specific antiviral drugs for COVID-19. Therefore, it is critical to have a comprehensive understanding of the virus. In this review, we highlight the etiology, epidemiology, pathogenesis and pathology, clinical characteristics, diagnosis, clinical management, prognosis, infection control and prevention of COVID-19 based on recent studies.",
  "date": " 2020 ",
  "authors": [
    "Jie Yu",
    "Peiwei Chai",
    "Shengfang Ge",
    "Xianqun Fan"
  ],
  "references": [
    "3001118548",
    "3001897055",
    "3005079553",
    "3003668884",
    "3002108456",
    "3009885589",
    "3004280078",
    "3004318991",
    "3003465021",
    "3004239190"
  ]
}, {
  "id": "3034408674",
  "title": "Risk assessment of mixed and displacement ventilation (LAF) during orthopedic and trauma surgery on COVID-19 patients with increased release of infectious aerosols | Paper | Microsoft Academic",
  "abstract": "No abstract available\r\nKeywords: Displacement ventilation; SARS-CoV-2 spread; laminar air flow; mixed ventilation; negative pressure; no ventilation.",
  "date": " 2020 ",
  "authors": [
    "Axel Kramer",
    "R\u00fcdiger K\u00fclpmann",
    "Arnold Brunner",
    "Michael M\u00fcller",
    "Georgi Wassilew"
  ],
  "references": [
    "3006961006",
    "3010604545"
  ]
}, {
  "id": "3035275617",
  "title": "Recreational waters - A potential transmission route for SARS-CoV-2 to humans? | Paper | Microsoft Academic",
  "abstract": "Coronavirus disease 2019 (COVID-19), the respiratory illness caused by the novel virus, severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), which has lead to high morbidity and mortality rates worldwide, has been causing major public health concerns since first detected in late 2019. Following identification of novel pathogens, questions in relation to dissemination of the pathogen and transmission routes begin to emerge. This rapidly spreading SARS-CoV-2 virus has been detected in both faecal and wastewater samples across the globe, highlighting the potential for faecal-oral transmission of the virus. As a result, concerns regarding the transmission of the virus in the environment and the risk associated with contracting the virus in recreational waters, particularly where inadequately treated wastewater is discharged, have been emerging in recent weeks. This paper highlights the need for further research to be carried out to investigate the presence, infectivity and viability of this newly identified SARS-CoV-2 virus in wastewater effluent and receiving recreational waters.",
  "date": " 2020 ",
  "authors": [
    "Niamh Cahill",
    "Dearbh\u00e1ile Morris"
  ],
  "references": [
    "3004280078",
    "3003465021",
    "3010604545",
    "3013893137",
    "3004824173",
    "3003464757",
    "3009834387",
    "3011863580",
    "3006846061",
    "3010096538"
  ]
}, {
  "id": "3034059415",
  "title": "Impact of Lockdown on the Epidemic Dynamics of COVID-19 in France | Paper | Microsoft Academic",
  "abstract": "The COVID-19 epidemic was reported in the Hubei province in China in December 2019 and then spread around the world reaching the pandemic stage at the beginning of March 2020. Since then, several countries went into lockdown. Using a mechanistic-statistical formalism, we estimate the effect of the lockdown in France on the contact rate and the effective reproduction number R e of the COVID-19. We obtain a reduction by a factor 7 (R e = 0.47, 95%-CI: 0.45-0.50), compared to the estimates carried out in France at the early stage of the epidemic. We also estimate the fraction of the population that would be infected by the beginning of May, at the official date at which the lockdown should be relaxed. We find a fraction of 3.7% (95%-CI: 3.0-4.8%) of the total French population, without taking into account the number of recovered individuals before April 1st, which is not known. This proportion is seemingly too low to reach herd immunity. Thus, even if the lockdown strongly mitigated the first epidemic wave, keeping a low value of R e is crucial to avoid an uncontrolled second wave (initiated with much more infectious cases than the first wave) and to hence avoid the saturation of hospital facilities.",
  "date": " 2020 ",
  "authors": [
    "Lionel Roques",
    "Etienne K. Klein",
    "Julien Papa\u00efx",
    "Antoine Sar",
    "Samuel Soubeyrand"
  ],
  "references": [
    "3003668884",
    "3009885589",
    "3008443627",
    "3010604545",
    "3013967887",
    "3015571324",
    "3006642361",
    "3004397688",
    "3012789146",
    "3013594674"
  ]
}, {
  "id": "3033952286",
  "title": "Real-time reverse transcription loop-mediated isothermal amplification for rapid detection of SARS-CoV-2 | Paper | Microsoft Academic",
  "abstract": "Background  Highly sensitive real-time reverse transcription polymerase chain reaction (RT-qPCR) methods have been developed for the detection of SARS-CoV-2. However, they are costly. Loop-mediated isothermal amplification (LAMP) assay has emerged as a novel alternative isothermal amplification method for the detection of nucleic acid.  Methods  A rapid, sensitive and specific real-time reverse transcription LAMP (RT-LAMP) assay was developed for SARS-CoV-2 detection.  Results  This assay detected one copy/reaction of SARS-CoV-2 RNA in 30 min. Both the clinical sensitivity and specificity of this assay were 100%. The RT-LAMP showed comparable performance with RT-qPCR. Combining simplicity and cost-effectiveness, this assay is therefore recommended for use in resource resource-limited settings.",
  "date": " 2020 ",
  "authors": [
    "Yee Ling Lau",
    "Ilyiana Ismail",
    "Nur Izati Mustapa",
    "Meng Yee Lai",
    "Tuan Suhaila Tuan Soh",
    "Afifah Hassan",
    "Kalaiarasu M Peariasamy",
    "Yee Leng Lee",
    "Yoong Min Chong",
    "I-Ching Sam",
    "Pik Pin Goh"
  ],
  "references": [
    "3001195213",
    "3010604545",
    "2105275554",
    "3011969828",
    "2770752141",
    "2263084061",
    "2175815746",
    "1991420168",
    "2207764089",
    "2073600962"
  ]
}, {
  "id": "3035464429",
  "title": "Sampling and detection of corona viruses in air: A mini review. | Paper | Microsoft Academic",
  "abstract": "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is a strain of coronaviruses that causes coronavirus disease 2019 (COVID-19). In these days, the spread of the SARS-CoV-2 virus through the air has become a controversial topic among scientists. Various organizations provide standard methods for monitoring biological agents in the air. Nevertheless, there has been no standard recommended method for sampling and determination of viruses in air. This manuscript aimed at reviewing published papers for sampling and detection of corona viruses, especially SARS-Cov-2 as a global health concern. It was found that SARS-Cov 2 was present in some air samples that were collected from patient's rooms in hospitals. This result warrants its airborne transmission potential. However, due to the fact that in the most reviewed studies, sampling was performed in the patient's room, it seems difficult to discriminate whether it is airborne or is transmitted through respiratory droplets. Moreover, some other disrupting factors such as patient distance from the sampler, using protective or oxygen masks by patients, patient activities, coughing and sneezing during sampling time, air movement, air conditioning, sampler type, sampling conditions, storage and transferring conditions, can affect the results. About the sampling methods, most of the used samplers such as PTFE filters, gelatin filers and cyclones showed suitable performance for trapping SARS-Co and MERS-Cov viruses followed by PCR analysis.",
  "date": " 2020 ",
  "authors": [
    "Ali Reza Rahmani",
    "Mostafa Leili",
    "Ghasem Azarian",
    "Ali Poormohammadi"
  ],
  "references": [
    "3010604545",
    "2132260239",
    "3010449299",
    "1981646999",
    "2103503670",
    "3018334611",
    "3015704123",
    "2158121945",
    "3030968929",
    "2337456675"
  ]
}, {
  "id": "3036958556",
  "title": "Involvement of digestive system in COVID-19: manifestations, pathology, management and challenges | Paper | Microsoft Academic",
  "abstract": "The pandemic of novel coronavirus disease (COVID-19) has developed as a tremendous threat to global health. Although most COVID-19 patients present with respiratory symptoms, some present with gastrointestinal (GI) symptoms like diarrhoea, loss of appetite, nausea/vomiting and abdominal pain as the major complaints. These features may be attributable to the following facts: (a) COVID-19 is caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), and its receptor angiotensin converting enzyme 2 (ACE2) was found to be highly expressed in GI epithelial cells, providing a prerequisite for SARS-CoV-2 infection; (b) SARS-CoV-2 viral RNA has been found in stool specimens of infected patients, and 20% of patients showed prolonged presence of SARS-CoV-2 RNA in faecal samples after the virus converting to negative in the respiratory system. These findings suggest that SARS-CoV-2 may be able to actively infect and replicate in the GI tract. Moreover, GI infection could be the first manifestation antedating respiratory symptoms; patients suffering only digestive symptoms but no respiratory symptoms as clinical manifestation have also been reported. Thus, the implications of digestive symptoms in patients with COVID-19 is of great importance. In this review, we summarise recent findings on the epidemiology of GI tract involvement, potential mechanisms of faecal-oral transmission, GI and liver manifestation, pathological/histological features in patients with COVID-19 and the diagnosis, management of patients with pre-existing GI and liver diseases as well as precautions for preventing SARS-CoV-2 infection during GI endoscopy procedures.",
  "date": " 2020 ",
  "authors": [
    "Song Su",
    "Jun Shen",
    "Liangru Zhu",
    "Yun Qiu",
    "Jin-Shen He",
    "Jin-Yu Tan",
    "Marietta Iacucci",
    "Siew C Ng",
    "Subrata Ghosh",
    "Ren Mao",
    "Jie Liang"
  ],
  "references": [
    "3001118548",
    "3008827533",
    "3005079553",
    "3002108456",
    "3002539152",
    "3003465021",
    "3008090866",
    "3007940623",
    "3010604545",
    "3011242477"
  ]
}, {
  "id": "3028749392",
  "title": "A Collaborative Multidisciplinary Approach to the Management of Coronavirus Disease 2019 in the Hospital Setting. | Paper | Microsoft Academic",
  "abstract": "The novel severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) causes coronavirus disease 2019 (COVID-19), which presents an unprecedented challenge to medical providers worldwide. Although most SARS-CoV-2-infected individuals manifest with a self-limited mild disease that resolves with supportive care in the outpatient setting, patients with moderate to severe COVID-19 will require a multidisciplinary collaborative management approach for optimal care in the hospital setting. Laboratory and radiologic studies provide critical information on disease severity, management options, and overall prognosis. Medical management is mostly supportive with antipyretics, hydration, oxygen supplementation, and other measures as dictated by clinical need. Among its medical complications is a characteristic proinflammatory cytokine storm often associated with end-organ dysfunction, including respiratory failure, liver and renal insufficiency, cardiac injury, and coagulopathy. Specific recommendations for the management of these medical complications are discussed. Despite the issuance of emergency use authorization for remdesivir, there are still no proven effective antiviral and immunomodulatory therapies, and their use in COVID-19 management should be guided by clinical trial protocols or treatment registries. The medical care of patients with COVID-19 extends beyond their hospitalization. Postdischarge follow-up and monitoring should be performed, preferably using telemedicine, until the patients have fully recovered from their illness and are released from home quarantine protocols.",
  "date": " 2020 ",
  "authors": [
    "Raymund R. Razonable",
    "Kelly M. Pennington",
    "Anne M. Meehan",
    "John W. Wilson",
    "Adam T. Froemming",
    "Courtney E. Bennett",
    "Ariela L. Marshall",
    "Abinash Virk",
    "Eva M. Carmona"
  ],
  "references": [
    "3001118548",
    "3001897055",
    "3008827533",
    "3005079553",
    "3003668884",
    "3002108456",
    "3009885589",
    "3002539152",
    "3008028633",
    "3004318991"
  ]
}, {
  "id": "3032185657",
  "title": "Trajectory of the COVID-19 pandemic: chasing a moving target | Paper | Microsoft Academic",
  "abstract": "The spread of COVID-19 has already taken a pandemic form, affecting over 180 countries in a matter of three months. The full continuum of disease ranges from mild, self-limiting illness to severe progressive COVID-19 pneumonia, multiorgan failure, cytokine storm and death. Younger and healthy population is now getting affected than before. Possibilities of airborne and fecal oral routes of transmission has increased the concern. In the absence of any specific therapeutic agent for coronavirus infections, the most effective manner to contain this pandemic is probably the non-pharmacological interventions (NPIs). The damage due to the pandemic disease is multifaceted and crippling to economy, trade, and health of the citizens of the countries. The extent of damage in such scenarios is something that is beyond calculation by Gross Domestic Product rate or currency value of the country. Unfortunately, unlike many other diseases, we are still away from the target antiviral drug and vaccine for severe acute respiratory syndrome (SARS-CoV-2) infection. The prime importance of NPIs like social distancing, staying in home, work from home, self-monitoring, public awareness, self-quarantine, etc. are constantly being emphasized by CDC, WHO, health ministries of all countries and social media houses. This is time of introspection and learning from our mistakes. Countries like China and South Korea who were initially the most hit countries could contain the disease spread by liberal testing of their population, stringent quarantine of people under investigation and isolation of the positive cases. Rest of the countries need to act urgently as well to bring an immediate halt in the community transmission.",
  "date": " 2020 ",
  "authors": [
    "Kamal Kant Sahu",
    "Ajay Kumar Mishra",
    "Amos Lal"
  ],
  "references": [
    "3001118548",
    "3008827533",
    "3005079553",
    "3003668884",
    "3009885589",
    "3008028633",
    "3008090866",
    "3007497549",
    "3010930696",
    "3011242477"
  ]
}, {
  "id": "3042098369",
  "title": "Laboratory Tests for COVID-19: A Review of Peer-Reviewed Publications and Implications for Clinical UIse | Paper | Microsoft Academic",
  "abstract": "Diagnostic tests for the coronavirus infection 2019 (COVID-19) are critical for prompt diagnosis, treatment and isolation to break the cycle of transmission. A positive real-time reverse-transcriptase polymerase chain reaction (RT-PCR), in conjunction with clinical and epidemiologic data, is the current standard for diagnosis, but several challenges still exist. Serological assays help to understand epidemiology better and to evaluate vaccine responses but they are unreliable for diagnosis in the acute phase of illness or assuming protective immunity. Serology is gaining attention, mainly because of convalescent plasma gaining importance as treatment for clinically worsening COVID-19 patients. We provide a narrative review of peer-reviewed research studies on RT-PCR, serology and antigen immune-assays for COVID-19, briefly describe their lab methods and discuss their limitations for clinical practice.",
  "date": " 2020 ",
  "authors": [
    "Daniel Shyu",
    "James Dorroh",
    "Caleb Holtmeyer",
    "Detlef Ritter",
    "Anandhi Upendran",
    "Raghuraman Kannan",
    "Dima Dandachi",
    "Christian Rojas-Moreno",
    "Stevan P Whitt",
    "Hariharan Regunath"
  ],
  "references": [
    "3001118548",
    "3005079553",
    "3003668884",
    "3009885589",
    "3002539152",
    "3004280078",
    "3001195213",
    "3007497549",
    "3010604545",
    "3013893137"
  ]
}, {
  "id": "3037255629",
  "title": "COVID-19 in children: An ample review | Paper | Microsoft Academic",
  "abstract": "The aim of this review was to describe the current knowledge about coronavirus disease 2019 (COVID-19, which is caused by severe acute respiratory syndrome coronavirus 2 [SARS-CoV-2]) in children, from epidemiological, clinical, and laboratory perspectives, including knowledge on the disease course, treatment, and prognosis. An extensive literature search was performed to identify papers on COVID-19 (SARS-CoV-2 infection) in children, published between January 1, 2020 and April 1, 2020. There were 44 relevant papers on COVID-19 in children. The results showed that COVID-19 occurs in 0.39-12.3% of children. Clinical signs and symptoms are comparable to those in adults, but milder forms and a large percentage of asymptomatic carriers are found among children. Elevated inflammatory markers are associated with complications and linked to various co-infections. Chest computed tomography (CT) scans in children revealed structural changes similar to those found in adults, with consolidations surrounded by halos being somewhat specific for children with COVID-19. The recommended treatment includes providing symptomatic therapy, with no specific drug recommendations for children. The prognosis is much better for children compared to adults. This review highlights that COVID-19 in children is similar to the disease in the adult population, but with particularities regarding clinical manifestations, laboratory test results, chest imaging, and treatment. The prognosis is much better for children compared to adults, but with the progression of the pandemic; the cases in children might change in the future.",
  "date": " 2020 ",
  "authors": [
    "Ioana M Ciuca"
  ],
  "references": [
    "3001118548",
    "3001897055",
    "3008827533",
    "3005079553",
    "3002108456",
    "3008028633",
    "3007940623",
    "3007497549",
    "3010604545",
    "3010930696"
  ]
}, {
  "id": "2962739339",
  "title": "Deep contextualized word representations | Paper | Microsoft Academic",
  "abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
  "date": " 2018 ",
  "authors": [
    "Matthew E. Peters",
    "Mark Neumann",
    "Mohit Iyyer",
    "Matt Gardner",
    "Christopher Clark",
    "Kenton Lee",
    "Luke Zettlemoyer"
  ],
  "references": [
    "2964121744",
    "2153579005",
    "2095705004",
    "2250539671",
    "2158899491",
    "2064675550",
    "2147880316",
    "2251939518",
    "2493916176",
    "2963748441"
  ]
}, {
  "id": "2331128040",
  "title": "Perceptual Losses for Real-Time Style Transfer and Super-Resolution | Paper | Microsoft Academic",
  "abstract": "We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a per-pixel loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing perceptual loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al. in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.",
  "date": " 2016 ",
  "authors": [
    "Justin Johnson",
    "Alexandre Alahi",
    "Li Fei-Fei"
  ],
  "references": [
    "2194775991",
    "2962835968",
    "2964121744",
    "1836465849",
    "2117539524",
    "1903029394",
    "2133665775",
    "1861492603",
    "2963684088",
    "2964153729"
  ]
}, {
  "id": "2964015378",
  "title": "Semi-Supervised Classification with Graph Convolutional Networks | Paper | Microsoft Academic",
  "abstract": "",
  "date": " 2016 ",
  "authors": [
    "Thomas N. Kipf",
    "Max Welling"
  ],
  "references": [
    "2962711740",
    "2907492528",
    "3100848837",
    "2963224980",
    "2962883549",
    "2963184176",
    "2796426482",
    "3100278010",
    "2786016794",
    "2966149470"
  ]
}, {
  "id": "1514535095",
  "title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention | Paper | Microsoft Academic",
  "abstract": "Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr9k, Flickr30k and MS COCO.",
  "date": " 2015 ",
  "authors": [
    "Kelvin Xu",
    "Jimmy Ba",
    "Ryan Kiros",
    "Kyunghyun Cho",
    "Aaron Courville",
    "Ruslan Salakhudinov",
    "Rich Zemel",
    "Yoshua Bengio"
  ],
  "references": [
    "2618530766",
    "2962835968",
    "2964121744",
    "2097117768",
    "2117539524",
    "2964308564",
    "2095705004",
    "2130942839",
    "2157331557",
    "1861492603"
  ]
}, {
  "id": "2508457857",
  "title": "Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising | Paper | Microsoft Academic",
  "abstract": "The discriminative model learning for image denoising has been recently attracting considerable attentions due to its favorable denoising performance. In this paper, we take one step forward by investigating the construction of feed-forward denoising convolutional neural networks (DnCNNs) to embrace the progress in very deep architecture, learning algorithm, and regularization method into image denoising. Specifically, residual learning and batch normalization are utilized to speed up the training process as well as boost the denoising performance. Different from the existing discriminative denoising models which usually train a specific model for additive white Gaussian noise at a certain noise level, our DnCNN model is able to handle Gaussian denoising with unknown noise level (i.e., blind Gaussian denoising). With the residual learning strategy, DnCNN implicitly removes the latent clean image in the hidden layers. This property motivates us to train a single DnCNN model to tackle with several general image denoising tasks, such as Gaussian denoising, single image super-resolution, and JPEG image deblocking. Our extensive experiments demonstrate that our DnCNN model can not only exhibit high effectiveness in several general image denoising tasks, but also be efficiently implemented by benefiting from GPU computing.",
  "date": " 2017 ",
  "authors": [
    "Kai Zhang",
    "Wangmeng Zuo",
    "Yunjin Chen",
    "Deyu Meng",
    "Lei Zhang"
  ],
  "references": [
    "2194775991",
    "2618530766",
    "2962835968",
    "2964121744",
    "2097117768",
    "1836465849",
    "1677182931",
    "2146502635",
    "2121058967",
    "2056370875"
  ]
}, {
  "id": "1614298861",
  "title": "Efficient Estimation of Word Representations in Vector Space | Paper | Microsoft Academic",
  "abstract": "We propose two novel model architectures for computing continuous vector\nrepresentations of words from very large data sets. The quality of these\nrepresentations is measured in a word similarity task, and the results are\ncompared to the previously best performing techniques based on different types\nof neural networks. We observe large improvements in accuracy at much lower\ncomputational cost, i.e. it takes less than a day to learn high quality word\nvectors from a 1.6 billion words data set. Furthermore, we show that these\nvectors provide state-of-the-art performance on our test set for measuring\nsyntactic and semantic word similarities.",
  "date": " 2013 ",
  "authors": [
    "Tomas Mikolov",
    "Kai Chen",
    "Greg S. Corrado",
    "Jeffrey Dean"
  ],
  "references": [
    "2153579005",
    "2250539671",
    "2271840356",
    "1895577753",
    "3104097132",
    "1888005072",
    "1486649854",
    "2964321699",
    "2100664567",
    "2123024445"
  ]
}, {
  "id": "2168356304",
  "title": "Object Detection with Discriminatively Trained Part-Based Models | Paper | Microsoft Academic",
  "abstract": "We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL data sets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin-sensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI--SVM in terms of latent variables. A latent SVM is semiconvex, and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and optimizing the latent SVM objective function.",
  "date": " 2010 ",
  "authors": [
    "P F Felzenszwalb",
    "R B Girshick",
    "D McAllester",
    "D Ramanan"
  ],
  "references": [
    "2151103935",
    "2161969291",
    "3097096317",
    "2154422044",
    "2120419212",
    "3111950349",
    "3021469268",
    "2145072179",
    "2030536784",
    "2115763357"
  ]
}, {
  "id": "1849277567",
  "title": "Visualizing and Understanding Convolutional Networks | Paper | Microsoft Academic",
  "abstract": "Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.",
  "date": " 2014 ",
  "authors": [
    "Matthew D. Zeiler",
    "Rob Fergus"
  ],
  "references": [
    "2618530766",
    "2102605133",
    "2108598243",
    "2136922672",
    "1904365287",
    "2155541015",
    "2546302380",
    "2025768430",
    "2110798204",
    "2097018403"
  ]
}, {
  "id": "1959608418",
  "title": "Auto-Encoding Variational Bayes | Paper | Microsoft Academic",
  "abstract": "Abstract: How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.",
  "date": " 2013 ",
  "authors": [
    "Diederik P Kingma",
    "Max Welling"
  ],
  "references": [
    "2146502635",
    "2163922914",
    "2145094598",
    "2166851633",
    "2097268041",
    "2963173382",
    "2951493172",
    "2171490498",
    "2119196781",
    "3104819538"
  ]
}, {
  "id": "1904365287",
  "title": "Improving neural networks by preventing co-adaptation of feature detectors | Paper | Microsoft Academic",
  "abstract": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.",
  "date": " 2012 ",
  "authors": [
    "Geoffrey E. Hinton",
    "Nitish Srivastava",
    "Alex Krizhevsky",
    "Ilya Sutskever",
    "Ruslan R. Salakhutdinov"
  ],
  "references": [
    "2108598243",
    "2911964244",
    "3118608800",
    "2100495367",
    "2310919327",
    "2912934387",
    "2116064496",
    "2147768505",
    "1993882792",
    "4919037"
  ]
}, {
  "id": "2964153729",
  "title": "Intriguing properties of neural networks | Paper | Microsoft Academic",
  "abstract": "Abstract: Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. \r\nFirst, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. \r\nSecond, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
  "date": " 2013 ",
  "authors": [
    "Christian Szegedy",
    "Wojciech Zaremba",
    "Ilya Sutskever",
    "Joan Bruna",
    "Dumitru Erhan",
    "Ian Goodfellow",
    "Rob Fergus"
  ],
  "references": [
    "2618530766",
    "2102605133",
    "1614298861",
    "2108598243",
    "2160815625",
    "2072128103",
    "2120419212",
    "2206858481",
    "2120480077",
    "2150165932"
  ]
}, {
  "id": "2546302380",
  "title": "What is the best multi-stage architecture for object recognition? | Paper | Microsoft Academic",
  "abstract": "In many recent object recognition systems, feature extraction stages are generally composed of a filter bank, a non-linear transformation, and some sort of feature pooling layer. Most systems use only one stage of feature extraction in which the filters are hard-wired, or two stages where the filters in one or both stages are learned in supervised or unsupervised mode. This paper addresses three questions: 1. How does the non-linearities that follow the filter banks influence the recognition accuracy? 2. does learning the filter banks in an unsupervised or supervised manner improve the performance over random filters or hardwired filters? 3. Is there any advantage to using an architecture with two stages of feature extraction, rather than one? We show that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks. We show that two stages of feature extraction yield better accuracy than one. Most surprisingly, we show that a two-stage system with random filters can yield almost 63% recognition rate on Caltech-101, provided that the proper non-linearities and pooling layers are used. Finally, we show that with supervised refinement, the system achieves state-of-the-art performance on NORB dataset (5.6%) and unsupervised pre-training followed by supervised refinement produces good accuracy on Caltech-101 (\u226b 65%), and the lowest known error rate on the undistorted, unprocessed MNIST dataset (0.53%).",
  "date": " 2009 ",
  "authors": [
    "Kevin Jarrett",
    "Koray Kavukcuoglu",
    "Marc'Aurelio Ranzato",
    "Yann LeCun"
  ],
  "references": [
    "2151103935",
    "2161969291",
    "2100495367",
    "2310919327",
    "2162915993",
    "2110798204",
    "2130325614",
    "2097018403",
    "2166049352",
    "2134557905"
  ]
}, {
  "id": "2133665775",
  "title": "Image quality assessment: from error visibility to structural similarity | Paper | Microsoft Academic",
  "abstract": "Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu//spl sim/lcv/ssim/.",
  "date": " 2004 ",
  "authors": [
    "Zhou Wang",
    "A.C. Bovik",
    "H.R. Sheikh",
    "E.P. Simoncelli"
  ],
  "references": [
    "2159269332",
    "2142276208",
    "2053691921",
    "2118217749",
    "2153777140",
    "2912116903",
    "2107790757",
    "2158564760",
    "2124731682",
    "2115838129"
  ]
}, {
  "id": "2340897893",
  "title": "The Cityscapes Dataset for Semantic Urban Scene Understanding | Paper | Microsoft Academic",
  "abstract": "Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations, 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.",
  "date": " 2016 ",
  "authors": [
    "Marius Cordts",
    "Mohamed Omran",
    "Sebastian Ramos",
    "Timo Rehfeld",
    "Markus Enzweiler",
    "Rodrigo Benenson",
    "Uwe Franke",
    "Stefan Roth",
    "Bernt Schiele"
  ],
  "references": [
    "2618530766",
    "2962835968",
    "639708223",
    "2919115771",
    "2102605133",
    "2117539524",
    "1903029394",
    "1536680647",
    "2168356304",
    "1861492603"
  ]
}, {
  "id": "2963420272",
  "title": "Context Encoders: Feature Learning by Inpainting | Paper | Microsoft Academic",
  "abstract": "We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders \u2013 a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.",
  "date": " 2016 ",
  "authors": [
    "Deepak Pathak",
    "Philipp Krahenbuhl",
    "Jeff Donahue",
    "Trevor Darrell",
    "Alexei A. Efros"
  ],
  "references": [
    "2618530766",
    "2964121744",
    "2102605133",
    "2117539524",
    "2153579005",
    "1903029394",
    "2099471712",
    "2155893237",
    "1536680647",
    "1849277567"
  ]
}, {
  "id": "2405756170",
  "title": "Generative adversarial text to image synthesis | Paper | Microsoft Academic",
  "abstract": "Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to generate highly compelling images of specific categories, such as faces, album covers, and room interiors. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image modeling, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions.",
  "date": " 2016 ",
  "authors": [
    "Scott Reed",
    "Zeynep Akata",
    "Xinchen Yan",
    "Lajanugen Logeswaran",
    "Bernt Schiele",
    "Honglak Lee"
  ],
  "references": [
    "2964121744",
    "2097117768",
    "1836465849",
    "2099471712",
    "2963684088",
    "2064675550",
    "1895577753",
    "1514535095",
    "2481240925",
    "1947481528"
  ]
}, {
  "id": "2963800363",
  "title": "High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs | Paper | Microsoft Academic",
  "abstract": "We present a new method for synthesizing high-resolution photo-realistic images from semantic label maps using conditional generative adversarial networks (conditional GANs). Conditional GANs have enabled a variety of applications, but the results are often limited to low-resolution and still far from realistic. In this work, we generate 2048 A\u2014 1024 visually appealing results with a novel adversarial loss, as well as new multi-scale generator and discriminator architectures. Furthermore, we extend our framework to interactive visual manipulation with two additional features. First, we incorporate object instance segmentation information, which enables object manipulations such as removing/adding objects and changing the object category. Second, we propose a method to generate diverse results given the same input, allowing users to edit the object appearance interactively. Human opinion studies demonstrate that our method significantly outperforms existing methods, advancing both the quality and the resolution of deep image synthesis and editing.",
  "date": " 2018 ",
  "authors": [
    "Ting-Chun Wang",
    "Ming-Yu Liu",
    "Jun-Yan Zhu",
    "Andrew Tao",
    "Jan Kautz",
    "Bryan Catanzaro"
  ],
  "references": [
    "2194775991",
    "2962835968",
    "1901129140",
    "1903029394",
    "1959608418",
    "2962793481",
    "2963684088",
    "2340897893",
    "2963373786",
    "2331128040"
  ]
}, {
  "id": "2963836885",
  "title": "Spectral Normalization for Generative Adversarial Networks | Paper | Microsoft Academic",
  "abstract": "",
  "date": " 2018 ",
  "authors": [
    "Takeru Miyato",
    "Toshiki Kataoka",
    "Masanori Koyama",
    "Yuichi Yoshida"
  ],
  "references": [
    "3003301247",
    "2893749619",
    "2962974533",
    "2804078698",
    "3035574324",
    "2982763192",
    "2962754210",
    "2963841322",
    "2933374552"
  ]
}, {
  "id": "2738588019",
  "title": "Globally and locally consistent image completion | Paper | Microsoft Academic",
  "abstract": "We present a novel approach for image completion that results in images that are both locally and globally consistent. With a fully-convolutional neural network, we can complete images of arbitrary resolutions by filling-in missing regions of any shape. To train this image completion network to be consistent, we use global and local context discriminators that are trained to distinguish real images from completed ones. The global discriminator looks at the entire image to assess if it is coherent as a whole, while the local discriminator looks only at a small area centered at the completed region to ensure the local consistency of the generated patches. The image completion network is then trained to fool the both context discriminator networks, which requires it to generate images that are indistinguishable from real ones with regard to overall consistency as well as in details. We show that our approach can be used to complete a wide variety of scenes. Furthermore, in contrast with the patch-based approaches such as PatchMatch, our approach can generate fragments that do not appear elsewhere in the image, which allows us to naturally complete the images of objects with familiar and highly specific structures, such as faces.",
  "date": " 2017 ",
  "authors": [
    "Satoshi Iizuka",
    "Edgar Simo-Serra",
    "Hiroshi Ishikawa"
  ],
  "references": [
    "1836465849",
    "1903029394",
    "2099471712",
    "2108598243",
    "2963073614",
    "2963684088",
    "1665214252",
    "2963373786",
    "2963840672",
    "6908809"
  ]
}, {
  "id": "2962947361",
  "title": "Unsupervised Image-to-Image Translation Networks | Paper | Microsoft Academic",
  "abstract": "Unsupervised image-to-image translation aims at learning a joint distribution of images in different domains by using images from the marginal distributions in individual domains. Since there exists an infinite set of joint distributions that can arrive the given marginal distributions, one could infer nothing about the joint distribution from the marginal distributions without additional assumptions. To address the problem, we make a shared-latent space assumption and propose an unsupervised image-to-image translation framework based on Coupled GANs. We compare the proposed framework with competing approaches and present high quality image translation results on various challenging unsupervised image translation tasks, including street scene image translation, animal image translation, and face image translation. We also apply the proposed framework to domain adaptation and achieve state-of-the-art performance on benchmark datasets. Code and additional results are available in https://github.com/mingyuliutw/unit.",
  "date": " 2017 ",
  "authors": [
    "Ming-Yu Liu",
    "Thomas Breuel",
    "Jan Kautz"
  ],
  "references": [
    "2962793481",
    "2795155917",
    "2984529706",
    "2963626105",
    "3098418424",
    "2989855043",
    "2970902013",
    "2981988113"
  ]
}, {
  "id": "2271840356",
  "title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems | Paper | Microsoft Academic",
  "abstract": "TensorFlow is an interface for expressing machine learning algorithms, and an\nimplementation for executing such algorithms. A computation expressed using\nTensorFlow can be executed with little or no change on a wide variety of\nheterogeneous systems, ranging from mobile devices such as phones and tablets\nup to large-scale distributed systems of hundreds of machines and thousands of\ncomputational devices such as GPU cards. The system is flexible and can be used\nto express a wide variety of algorithms, including training and inference\nalgorithms for deep neural network models, and it has been used for conducting\nresearch and for deploying machine learning systems into production across more\nthan a dozen areas of computer science and other fields, including speech\nrecognition, computer vision, robotics, information retrieval, natural language\nprocessing, geographic information extraction, and computational drug\ndiscovery. This paper describes the TensorFlow interface and an implementation\nof that interface that we have built at Google. The TensorFlow API and a\nreference implementation were released as an open-source package under the\nApache 2.0 license in November, 2015 and are available at www.tensorflow.org.",
  "date": " 2014 ",
  "authors": [
    "Mart\u00edn Abadi",
    "Ashish Agarwal",
    "Paul Barham",
    "Eugene Brevdo",
    "Zhifeng Chen",
    "Craig Citro",
    "Gregory S. Corrado",
    "Andy Davis",
    "Jeffrey Dean",
    "Matthieu Devin",
    "Sanjay Ghemawat",
    "Ian J. Goodfellow",
    "Andrew Harp",
    "Geoffrey Irving",
    "Michael Isard",
    "Yangqing Jia",
    "Rafal J\u00f3zefowicz",
    "Lukasz Kaiser",
    "Manjunath Kudlur",
    "Josh Levenberg",
    "Dan Man\u00e9",
    "Rajat Monga",
    "Sherry Moore",
    "Derek Gordon Murray",
    "Chris Olah",
    "Mike Schuster",
    "Jonathon Shlens",
    "Benoit Steiner",
    "Ilya Sutskever",
    "Kunal Talwar",
    "Paul A. Tucker",
    "Vincent Vanhoucke",
    "Vijay Vasudevan",
    "Fernanda B. Vi\u00e9gas",
    "Oriol Vinyals",
    "Pete Warden",
    "Martin Wattenberg",
    "Martin Wicke",
    "Yuan Yu",
    "Xiaoqiang Zheng"
  ],
  "references": [
    "2097117768",
    "1836465849",
    "1614298861",
    "2130942839",
    "2155893237",
    "2064675550",
    "2160815625",
    "2168231600",
    "2016053056",
    "2131975293"
  ]
}, {
  "id": "648143168",
  "title": "Deep generative image models using a Laplacian pyramid of adversarial networks | Paper | Microsoft Academic",
  "abstract": "In this paper we introduce a generative parametric model capable of producing high quality samples of natural images. Our approach uses a cascade of convolutional networks within a Laplacian pyramid framework to generate images in a coarse-to-fine fashion. At each level of the pyramid, a separate generative convnet model is trained using the Generative Adversarial Nets (GAN) approach [11]. Samples drawn from our model are of significantly higher quality than alternate approaches. In a quantitative assessment by human evaluators, our CIFAR10 samples were mistaken for real images around 40% of the time, compared to 10% for samples drawn from a GAN baseline model. We also show samples from models trained on the higher resolution images of the LSUN scene dataset.",
  "date": " 2015 ",
  "authors": [
    "Emily Denton",
    "Soumith Chintala",
    "Arthur Szlam",
    "Rob Fergus"
  ],
  "references": [
    "1836465849",
    "2099471712",
    "2108598243",
    "1959608418",
    "3118608800",
    "2100495367",
    "2025768430",
    "2125389028",
    "2118858186",
    "189596042"
  ]
}, {
  "id": "830076066",
  "title": "Semi-supervised learning with Ladder networks | Paper | Microsoft Academic",
  "abstract": "We combine supervised learning with unsupervised learning in deep neural networks. The proposed model is trained to simultaneously minimize the sum of supervised and unsupervised cost functions by backpropagation, avoiding the need for layer-wise pre-training. Our work builds on top of the Ladder network proposed by Valpola [1] which we extend by combining the model with supervision. We show that the resulting model reaches state-of-the-art performance in semi-supervised MNIST and CIFAR-10 classification in addition to permutation-invariant MNIST classification with all labels.",
  "date": " 2015 ",
  "authors": [
    "Antti Rasmus",
    "Harri Valpola",
    "Mikko Honkala",
    "Mathias Berglund",
    "Tapani Raiko"
  ],
  "references": [
    "2964121744",
    "1836465849",
    "2095705004",
    "2100495367",
    "2963207607",
    "2294059674",
    "2145094598",
    "2963382180",
    "1606347560",
    "2048679005"
  ]
}, {
  "id": "2963685250",
  "title": "Weight normalization: a simple reparameterization to accelerate training of deep neural networks | Paper | Microsoft Academic",
  "abstract": "We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.",
  "date": " 2016 ",
  "authors": [
    "Tim Salimans",
    "Diederik P. Kingma"
  ],
  "references": [
    "2194775991",
    "1836465849",
    "2145339207",
    "1959608418",
    "3118608800",
    "2064675550",
    "2963911037",
    "1533861849",
    "2294059674",
    "104184427"
  ]
}, {
  "id": "2949416428",
  "title": "Semi-Supervised Learning with Deep Generative Models | Paper | Microsoft Academic",
  "abstract": "The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.",
  "date": " 2014 ",
  "authors": [
    "Diederik P. Kingma",
    "Danilo J. Rezende",
    "Shakir Mohamed",
    "Max Welling"
  ],
  "references": [
    "1959608418",
    "2146502635",
    "2962897886",
    "2335728318",
    "2136504847",
    "2107008379",
    "1676820704",
    "2407712691",
    "2158049734",
    "2122457239"
  ]
}, {
  "id": "1487641199",
  "title": "Generative Moment Matching Networks | Paper | Microsoft Academic",
  "abstract": "We consider the problem of learning deep generative models from data. We formulate a method that generates an independent sample via a single feedforward pass through a multilayer perceptron, as in the recently proposed generative adversarial networks (Goodfellow et al., 2014). Training a generative adversarial network, however, requires careful optimization of a difficult minimax program. Instead, we utilize a technique from statistical hypothesis testing known as maximum mean discrepancy (MMD), which leads to a simple objective that can be interpreted as matching all orders of statistics between a dataset and samples from the model, and can be trained by backpropagation. We further boost the performance of this approach by combining our generative network with an auto-encoder network, using MMD to learn to generate codes that can then be decoded to produce samples. We show that the combination of these techniques yields excellent generative models compared to baseline approaches as measured on MNIST and the Toronto Face Database.",
  "date": " 2015 ",
  "authors": [
    "Yujia Li",
    "Kevin Swersky",
    "Rich Zemel"
  ],
  "references": [
    "2618530766",
    "2097117768",
    "2099471712",
    "2130942839",
    "2157331557",
    "2963542991",
    "1959608418",
    "2310919327",
    "1904365287",
    "1665214252"
  ]
}, {
  "id": "2911964244",
  "title": "Random Forests | Paper | Microsoft Academic",
  "abstract": "Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, aaa, 148\u2013156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.",
  "date": " 2001 ",
  "authors": [
    "Leo Breiman"
  ],
  "references": [
    "2912934387",
    "2112076978",
    "1975846642",
    "2152761983",
    "2113242816",
    "1605688901",
    "2120240539",
    "2099968818",
    "2067885219",
    "1580948147"
  ]
}, {
  "id": "2130325614",
  "title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations | Paper | Microsoft Academic",
  "abstract": "There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks. Scaling such models to full-sized, high-dimensional images remains a difficult problem. To address this problem, we present the convolutional deep belief network, a hierarchical generative model which scales to realistic image sizes. This model is translation-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling, a novel technique which shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images.",
  "date": " 2009 ",
  "authors": [
    "Honglak Lee",
    "Roger Grosse",
    "Rajesh Ranganath",
    "Andrew Y. Ng"
  ],
  "references": [
    "2136922672",
    "2100495367",
    "2162915993",
    "2116064496",
    "2110798204",
    "2166049352",
    "2145889472",
    "2122922389",
    "2147800946",
    "2168002178"
  ]
}, {
  "id": "2963542991",
  "title": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks | Paper | Microsoft Academic",
  "abstract": "Abstract: We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.",
  "date": " 2014 ",
  "authors": [
    "Pierre Sermanet",
    "David Eigen",
    "Xiang Zhang",
    "Michael Mathieu",
    "Rob Fergus",
    "Yann LeCun"
  ],
  "references": [
    "2194775991",
    "2962835968",
    "2097117768",
    "639708223",
    "1903029394",
    "2155893237"
  ]
}, {
  "id": "2963911037",
  "title": "Network In Network | Paper | Microsoft Academic",
  "abstract": "Abstract: We propose a novel deep network structure called \"Network In Network\" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.",
  "date": " 2014 ",
  "authors": [
    "Min Lin",
    "Qiang Chen",
    "Shuicheng Yan"
  ],
  "references": [
    "2962835968",
    "2097117768",
    "2117539524",
    "1536680647",
    "2963446712",
    "2963037989",
    "2109255472",
    "2096733369",
    "2302255633"
  ]
}, {
  "id": "2068730032",
  "title": "Scalable Object Detection Using Deep Neural Networks | Paper | Microsoft Academic",
  "abstract": "Deep convolutional neural networks have recently achieved state-of-the-art performance on a number of image recognition benchmarks, including the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model on the localization sub-task was a network that predicts a single bounding box and a confidence score for each object category in the image. Such a model captures the whole-image context around the objects but cannot handle multiple instances of the same object in the image without naively replicating the number of outputs for each instance. In this work, we propose a saliency-inspired neural network model for detection, which predicts a set of class-agnostic bounding boxes along with a single score for each box, corresponding to its likelihood of containing any object of interest. The model naturally handles a variable number of instances for each class and allows for cross-class generalization at the highest levels of the network. We are able to obtain competitive recognition performance on VOC2007 and ILSVRC2012, while using only the top few predicted locations in each image and a small number of neural network evaluations.",
  "date": " 2014 ",
  "authors": [
    "Dumitru Erhan",
    "Christian Szegedy",
    "Alexander Toshev",
    "Dragomir Anguelov"
  ],
  "references": [
    "2618530766",
    "2102605133",
    "2168356304",
    "2031489346",
    "2088049833",
    "2130306094",
    "2129305389",
    "2017691720",
    "2128715914",
    "2113201641"
  ]
}, {
  "id": "2161969291",
  "title": "Histograms of oriented gradients for human detection | Paper | Microsoft Academic",
  "abstract": "We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.",
  "date": " 2005 ",
  "authors": [
    "N. Dalal",
    "B. Triggs"
  ],
  "references": [
    "2151103935",
    "2177274842",
    "3021469268",
    "2145072179",
    "2172188317",
    "2152473410",
    "1576520375",
    "1608462934",
    "1992825118",
    "2295106276"
  ]
}, {
  "id": "2031489346",
  "title": "The Pascal Visual Object Classes (VOC) Challenge | Paper | Microsoft Academic",
  "abstract": "The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.\r\n\r\nThis paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension.",
  "date": " 2010 ",
  "authors": [
    "Mark Everingham",
    "Luc Gool",
    "Christopher K. Williams",
    "John Winn",
    "Andrew Zisserman"
  ],
  "references": [
    "2151103935",
    "2161969291",
    "3097096317",
    "2162915993",
    "2038721957",
    "2131846894",
    "2104974755",
    "2110764733",
    "1576445103",
    "1565746575"
  ]
}, {
  "id": "2088049833",
  "title": "Selective Search for Object Recognition | Paper | Microsoft Academic",
  "abstract": "This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99 % recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software: http://disi.unitn.it/~uijlings/SelectiveSearch.html ).",
  "date": " 2013 ",
  "authors": [
    "J. R. Uijlings",
    "K. E. Sande",
    "T. Gevers",
    "A. W. Smeulders"
  ],
  "references": [
    "2151103935",
    "2161969291",
    "2168356304",
    "2164598857",
    "2031489346",
    "3097096317",
    "2162915993",
    "2163352848",
    "2067191022",
    "2121947440"
  ]
}, {
  "id": "2155541015",
  "title": "DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition | Paper | Microsoft Academic",
  "abstract": "We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be repurposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms.",
  "date": " 2014 ",
  "authors": [
    "Jeff Donahue",
    "Yangqing Jia",
    "Oriol Vinyals",
    "Judy Hoffman",
    "Ning Zhang",
    "Eric Tzeng",
    "Trevor Darrell"
  ],
  "references": [
    "2618530766",
    "2161969291",
    "2108598243",
    "2168356304",
    "2100495367",
    "2310919327",
    "1677409904",
    "1904365287",
    "2187089797",
    "2546302380"
  ]
}, {
  "id": "1663973292",
  "title": "Pattern Recognition and Machine Learning | Paper | Microsoft Academic",
  "abstract": "Probability Distributions.- Linear Models for Regression.- Linear Models for Classification.- Neural Networks.- Kernel Methods.- Sparse Kernel Machines.- Graphical Models.- Mixture Models and EM.- Approximate Inference.- Sampling Methods.- Continuous Latent Variables.- Sequential Data.- Combining Models.",
  "date": " 2006 ",
  "authors": [
    "Christopher M. Bishop"
  ],
  "references": [
    "2117812871",
    "1496357020"
  ]
}, {
  "id": "2109255472",
  "title": "Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition | Paper | Microsoft Academic",
  "abstract": "Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224  $\\times$      224) input image. This requirement is \u201cartificial\u201d and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, \u201cspatial pyramid pooling\u201d, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image representation and no fine-tuning. The power of SPP-net is also significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102   $\\times$       faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007. In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank #2 in object detection and #3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition.",
  "date": " 2015 ",
  "authors": [
    "Kaiming He",
    "Xiangyu Zhang",
    "Shaoqing Ren",
    "Jian Sun"
  ],
  "references": [
    "2618530766",
    "2962835968",
    "2151103935",
    "2097117768",
    "2153635508",
    "2102605133",
    "2117539524",
    "2161969291",
    "2108598243",
    "2168356304"
  ]
}, {
  "id": "753012316",
  "title": "Torch7: A Matlab-like Environment for Machine Learning | Paper | Microsoft Academic",
  "abstract": "Torch7 is a versatile numeric computing framework and machine learning library that extends Lua. Its goal is to provide a flexible environment to design and train learning machines. Flexibility is obtained via Lua, an extremely lightweight scripting language. High performance is obtained via efficient OpenMP/SSE and CUDA implementations of low-level numeric routines. Torch7 can easily be interfaced to third-party software thanks to Lua\u2019s light interface.",
  "date": " 2010 ",
  "authors": [
    "Ronan Collobert",
    "Koray Kavukcuoglu",
    "Cl\u00e9ment Farabet"
  ],
  "references": [
    "2606594511"
  ]
}, {
  "id": "1825604117",
  "title": "Open-vocabulary Object Retrieval | Paper | Microsoft Academic",
  "abstract": "",
  "date": " 2014 ",
  "authors": [
    "Sergio Guadarrama",
    "Erik Rodner",
    "Kate Saenko",
    "Ning Zhang",
    "Ryan Farrell",
    "Jeff Donahue",
    "Trevor Darrell"
  ],
  "references": [
    "2088049833",
    "2131846894",
    "2128017662",
    "2141362318",
    "2094728533",
    "1889268436",
    "1618905105",
    "21006490",
    "1897761818",
    "2066134726"
  ]
}, {
  "id": "2147414309",
  "title": "PANDA: Pose Aligned Networks for Deep Attribute Modeling | Paper | Microsoft Academic",
  "abstract": "We propose a method for inferring human attributes (such as gender, hair style, clothes style, expression, action) from images of people under large variation of viewpoint, pose, appearance, articulation and occlusion. Convolutional Neural Nets (CNN) have been shown to perform very well on large scale object recognition problems. In the context of attribute classification, however, the signal is often subtle and it may cover only a small part of the image, while the image is dominated by the effects of pose and viewpoint. Discounting for pose variation would require training on very large labeled datasets which are not presently available. Part-based models, such as poselets [4] and DPM [12] have been shown to perform well for this problem but they are limited by shallow low-level features. We propose a new method which combines part-based models and deep learning by training pose-normalized CNNs. We show substantial improvement vs. state-of-the-art methods on challenging attribute classification tasks in unconstrained settings. Experiments confirm that our method outperforms both the best part-based methods on this problem and conventional CNNs trained on the full bounding box of the person.",
  "date": " 2014 ",
  "authors": [
    "Ning Zhang",
    "Manohar Paluri",
    "Marc'Aurelio Ranzato",
    "Trevor Darrell",
    "Lubomir Bourdev"
  ],
  "references": [
    "2618530766",
    "2168356304",
    "2310919327",
    "2155541015",
    "2162915993",
    "2546302380",
    "1498436455",
    "2536626143",
    "2134270519",
    "2098411764"
  ]
}, {
  "id": "1872489089",
  "title": "Pylearn2: a machine learning research library | Paper | Microsoft Academic",
  "abstract": "Pylearn2 is a machine learning research library. This does not just mean that it is a collection of machine learning algorithms that share a common API; it means that it has been designed for flexibility and extensibility in order to facilitate research projects that involve new or unusual use cases. In this paper we give a brief history of the library, an overview of its basic philosophy, a summary of the library's architecture, and a description of how the Pylearn2 community functions socially.",
  "date": " 2013 ",
  "authors": [
    "Ian J. Goodfellow",
    "David Warde-Farley",
    "Pascal Lamblin",
    "Vincent Dumoulin",
    "Mehdi Mirza",
    "Razvan Pascanu",
    "James Bergstra",
    "Fr\u00e9d\u00e9ric Bastien",
    "Yoshua Bengio"
  ],
  "references": [
    "2618530766",
    "2101234009",
    "3118608800",
    "2310919327",
    "1904365287",
    "2168231600",
    "2119821739",
    "2116064496",
    "2294059674",
    "2025768430"
  ]
}, {
  "id": "2962883796",
  "title": "Recognizing Image Style. | Paper | Microsoft Academic",
  "abstract": "The style of an image plays a significant role in how it is viewed, but style has received little attention in computer vision research. We describe an approach to predicting style of images, and perform a thorough evaluation of different image features for these tasks. We find that features learned in a multi-layer network generally perform best \u2013 even when trained with object class (not style) labels. Our large-scale learning methods results in the best published performance on an existing dataset of aesthetic ratings and photographic style annotations. We present two novel datasets: 80K Flickr photographs annotated with 20 curated style labels, and 85K paintings annotated with 25 style/genre labels. Our approach shows excellent classification performance on both datasets. We use the learned classifiers to extend traditional tag-based image search to consider stylistic constraints, and demonstrate cross-dataset understanding of style.",
  "date": " 2013 ",
  "authors": [
    "Sergey Karayev",
    "Matthew Trentacoste",
    "Helen Han",
    "Aseem Agarwala",
    "Trevor Darrell",
    "Aaron Hertzmann",
    "Holger Winnemoeller"
  ],
  "references": [
    "2618530766",
    "2108598243",
    "2146502635",
    "2155541015",
    "1566135517",
    "2135957164",
    "1511924373",
    "2075456404",
    "2078807908",
    "2157462866"
  ]
}, {
  "id": "2164598857",
  "title": "Rapid object detection using a boosted cascade of simple features | Paper | Microsoft Academic",
  "abstract": "This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the \"integral image\" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a \"cascade\" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.",
  "date": " 2001 ",
  "authors": [
    "P. Viola",
    "M. Jones"
  ],
  "references": [
    "1988790447",
    "2128272608",
    "2217896605",
    "2115763357",
    "1975846642",
    "2124351082",
    "2159686933",
    "2155511848",
    "2101522199",
    "1588351438"
  ]
}, {
  "id": "1861492603",
  "title": "Microsoft COCO: Common Objects in Context | Paper | Microsoft Academic",
  "abstract": "We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.",
  "date": " 2014 ",
  "authors": [
    "Tsung-Yi Lin",
    "Michael Maire",
    "Serge J. Belongie",
    "James Hays",
    "Pietro Perona",
    "Deva Ramanan",
    "Piotr Doll\u00e1r",
    "C. Lawrence Zitnick"
  ],
  "references": [
    "2618530766",
    "2102605133",
    "2161969291",
    "2108598243",
    "2168356304",
    "2963542991",
    "3118608800",
    "2031489346",
    "2038721957",
    "2110158442"
  ]
}, {
  "id": "2250539671",
  "title": "Glove: Global Vectors for Word Representation | Paper | Microsoft Academic",
  "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
  "date": " 2014 ",
  "authors": [
    "Jeffrey Pennington",
    "Richard Socher",
    "Christopher Manning"
  ],
  "references": [
    "2153579005",
    "1614298861",
    "2146502635",
    "2158899491",
    "2072128103",
    "2141599568",
    "2117130368",
    "2118020653",
    "2132339004",
    "2158139315"
  ]
}, {
  "id": "2131744502",
  "title": "Distributed Representations of Sentences and Documents | Paper | Microsoft Academic",
  "abstract": "Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, \"powerful,\" \"strong\" and \"Paris\" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperforms bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.",
  "date": " 2014 ",
  "authors": [
    "Quoc Le",
    "Tomas Mikolov"
  ],
  "references": [
    "2153579005",
    "1614298861",
    "2158899491",
    "2131744502",
    "2251939518",
    "2141599568",
    "2117130368",
    "2132339004",
    "2158139315",
    "2113459411"
  ]
}, {
  "id": "2251939518",
  "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank | Paper | Microsoft Academic",
  "abstract": "Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.",
  "date": " 2013 ",
  "authors": [
    "Richard Socher",
    "Alex Perelygin",
    "Jean Wu",
    "Jason Chuang",
    "Christopher D. Manning",
    "Andrew Ng",
    "Christopher Potts"
  ],
  "references": [
    "2146502635",
    "2097726431",
    "2117130368",
    "2132339004",
    "1423339008",
    "71795751",
    "1662133657",
    "1889268436",
    "2164019165",
    "2097606805"
  ]
}, {
  "id": "2963748441",
  "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text | Paper | Microsoft Academic",
  "abstract": "",
  "date": " 2016 ",
  "authors": [
    "Pranav Rajpurkar",
    "Jian Zhang",
    "Konstantin Lopyrev",
    "Percy Liang"
  ],
  "references": [
    "2108598243",
    "1544827683",
    "1632114991",
    "2125436846",
    "2964267515",
    "2962809918",
    "2171278097",
    "2962790689",
    "2251818205",
    "2251349042"
  ]
}, {
  "id": "2899771611",
  "title": "Automatic differentiation in PyTorch | Paper | Microsoft Academic",
  "abstract": "",
  "date": " 2017 ",
  "authors": [
    "Adam Paszke",
    "Sam Gross",
    "Soumith Chintala",
    "Gregory Chanan",
    "Edward Yang",
    "Zachary DeVito",
    "Zeming Lin",
    "Alban Desmaison",
    "Luca Antiga",
    "Adam Lerer"
  ],
  "references": [
    "2754835109",
    "1585773866",
    "1579027943"
  ]
}, {
  "id": "2962784628",
  "title": "Neural Machine Translation of Rare Words with Subword Units | Paper | Microsoft Academic",
  "abstract": "Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English!German and English!Russian by up to 1.1 and 1.3 BLEU, respectively.",
  "date": " 2016 ",
  "authors": [
    "Rico Sennrich",
    "Barry Haddow",
    "Alexandra Birch"
  ],
  "references": [
    "2964308564",
    "2130942839",
    "2157331557",
    "1902237438",
    "6908809",
    "1753482797",
    "2124807415",
    "2251012068",
    "1815076433",
    "2100664567"
  ]
}, {
  "id": "1840435438",
  "title": "A large annotated corpus for learning natural language inference | Paper | Microsoft Academic",
  "abstract": "Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.",
  "date": " 2015 ",
  "authors": [
    "Samuel R. Bowman",
    "Gabor Angeli",
    "Christopher Potts",
    "Christopher D. Manning"
  ],
  "references": [
    "2095705004",
    "2250539671",
    "2064675550",
    "2251939518",
    "6908809",
    "2081580037",
    "2097606805",
    "2584341106",
    "2185175083",
    "2154359981"
  ]
}, {
  "id": "2963026768",
  "title": "Universal Language Model Fine-tuning for Text Classification | Paper | Microsoft Academic",
  "abstract": "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.",
  "date": " 2018 ",
  "authors": [
    "Jeremy Howard",
    "Sebastian Ruder"
  ],
  "references": [
    "2194775991",
    "1836465849",
    "2153579005",
    "1903029394",
    "2963446712",
    "2962739339",
    "2155541015",
    "3112605745",
    "2062118960",
    "2963012544"
  ]
}, {
  "id": "2996035354",
  "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators | Paper | Microsoft Academic",
  "abstract": "While masked language modeling (MLM) pre-training methods such as BERT produce excellent results on downstream NLP tasks, they require large amounts of compute to be effective. These approaches corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some input tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the model learns from all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by methods such as BERT and XLNet given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where we match the performance of RoBERTa, the current state-of-the-art pre-trained transformer, while using less than 1/4 of the compute.",
  "date": " 2020 ",
  "authors": [
    "Kevin Clark",
    "Minh-Thang Luong",
    "Quoc V. Le",
    "Christopher D. Manning"
  ],
  "references": [
    "2964121744",
    "2963403868",
    "2963341956",
    "2099471712",
    "1614298861",
    "2250539671",
    "2962739339",
    "2963684088",
    "2158899491",
    "2251939518"
  ]
}, {
  "id": "2990704537",
  "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems | Paper | Microsoft Academic",
  "abstract": "In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at https://super.gluebenchmark.com.",
  "date": " 2019 ",
  "authors": [
    "Alex Wang",
    "Yada Pruksachatkun",
    "Nikita Nangia",
    "Amanpreet Singh",
    "Julian Michael",
    "Felix Hill",
    "Omer Levy",
    "Samuel R. Bowman"
  ],
  "references": [
    "2965373594",
    "3098903812",
    "2980282514",
    "3100307207",
    "3034238904",
    "2970986510",
    "3007332492",
    "3035503910"
  ]
}, {
  "id": "3100307207",
  "title": "Experience Grounds Language | Paper | Microsoft Academic",
  "abstract": "Language understanding research is held back by a failure to relate language to the physical world it describes and to the social interactions it facilitates. Despite the incredible effectiveness of language processing models to tackle tasks after being trained on text alone, successful linguistic communication relies on a shared experience of the world. It is this shared experience that makes utterances meaningful. Natural language processing is a diverse field, and progress throughout its development has come from new representational theories, modeling techniques, data collection paradigms, and tasks. We posit that the present success of representation learning approaches trained on large, text-only corpora requires the parallel tradition of research on the broader physical and social context of language to address the deeper questions of communication.",
  "date": " 2020 ",
  "authors": [
    "Yonatan Bisk",
    "Ari Holtzman",
    "Jesse Thomason",
    "Jacob Andreas",
    "Yoshua Bengio",
    "Joyce Chai",
    "Mirella Lapata",
    "Angeliki Lazaridou",
    "Jonathan May",
    "Aleksandr Nisnevich",
    "Nicolas Pinto",
    "Joseph P. Turian"
  ],
  "references": [
    "2618530766",
    "2963403868",
    "2963341956",
    "2117539524",
    "2153579005",
    "2250539671",
    "1880262756",
    "2962739339",
    "2117130368",
    "2132339004"
  ]
}, {
  "id": "3105966348",
  "title": "TinyBERT: Distilling BERT for Natural Language Understanding | Paper | Microsoft Academic",
  "abstract": "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large \u201cteacher\u201d BERT can be effectively transferred to a small \u201cstudent\u201d TinyBERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general-domain as well as the task-specific knowledge in BERT. TinyBERT4 with 4 layers is empirically effective and achieves more than 96.8% the performance of its teacher BERT-Base on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT4 is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only ~28% parameters and ~31% inference time of them. Moreover, TinyBERT6 with 6 layers performs on-par with its teacher BERT-Base.",
  "date": " 2020 ",
  "authors": [
    "Xiaoqi Jiao",
    "Yichun Yin",
    "Lifeng Shang",
    "Xin Jiang",
    "Xiao Chen",
    "Linlin Li",
    "Fang Wang",
    "Qun Liu"
  ],
  "references": [
    "3101045333",
    "3100985894",
    "3022810465",
    "3117450517",
    "3015609966",
    "3103473439",
    "3115789797",
    "3110662498",
    "3106429660"
  ]
}, 